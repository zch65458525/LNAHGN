The paper proposes and theoretically justifies two statistical tests to distinguish social influence from other sources of correlation (e.g., homophily) in social networks, validating them through simulations and applying them to Flickr data to show that observed tagging correlations are not due to influence.
The paper proposes two efficient semi-streaming algorithms for approximating local triangle counts in massive graphs using min-wise independent permutations, demonstrating their practical utility in detecting spam and assessing content quality.
The paper proposes a joint probabilistic generative model that leverages mutual benefits between structured entity identification and document categorization, improving both tasks through unsupervised learning via an EM algorithm.
The paper proposes an adaptive approach for mining frequent closed unlabeled rooted trees in evolving data streams, introducing three algorithms (IncTreeNat, WinTreeNat, AdaTreeNat) based on Galois Lattice Theory and relaxed closed trees.
The paper proposes a label acquisition method for collective classification that identifies and corrects mistakes made by the algorithm, significantly outperforming existing approaches in accuracy and scalability on both real and synthetic datasets.
The paper introduces the problem of query decomposition—breaking a query into coherent, distinct subqueries—and presents two algorithmic approaches (a greedy set-cover method and a constrained clustering method) to solve it effectively, validated by experiments on a web-scale search engine.
The paper presents a novel two-stage algorithm for the Column Subset Selection Problem (CSSP) that improves theoretical bounds for small to moderate *k* and demonstrates its effectiveness as an unsupervised feature selection method in finance, document-term data, and genetics by identifying representative features comparable to PCA's top eigenfeatures.
The paper demonstrates that anonymization methods like *k*-anonymity and *l*-diversity severely degrade data-mining utility while offering minimal privacy benefits compared to trivial sanitization.
The paper proposes a statistical framework for generating optimal, context-aware link-titles for URLs by aggregating and selecting from diverse candidate sources, achieving at least 20% improvement over baselines.
The paper proposes efficient max-margin structured learning algorithms to optimize ranking criteria (MRR and NDCG) and introduces a multi-criteria optimization framework that improves ranking performance across diverse query types.
The paper proposes **partitioned logistic regression**, a hybrid model combining logistic regression on disjoint feature groups with naive Bayes prediction fusion, which outperforms both methods theoretically and empirically, achieving significant improvements in spam filtering.
The paper proposes an efficient subspace kernel learning method using HSIC, introduces joint optimization for kernel learning and classification, and extends the approach to multiple kernels with efficient SILP formulations, demonstrating improved performance on benchmark datasets.
The paper proposes **Combinational Collaborative Filtering (CCF)**, a hybrid method integrating semantic and user data with Gibbs sampling and EM for scalable, personalized community recommendations, validated on the Orkut dataset.
The paper proposes FAST, a ROC-based feature selection method for imbalanced and small-sample data, which outperforms correlation and RELIEF methods in skewed datasets and performs comparably in balanced cases while significantly improving classification with fewer features.
The paper proposes a semi-supervised learning framework using Hidden Markov Model Regression and covariance alignment to improve long-term time series forecasting by leveraging both historical data and simulation-generated scenarios.
The paper presents a method combining numerical sensitivity analysis and probabilistic graphical models to reconstruct chemical reaction networks from concentration time-series data, identifying network topology and reaction properties, with applications in systems biology.
The paper presents an automatic two-step record linkage method using seeded nearest-neighbour and iterative SVM classification, outperforming unsupervised approaches by generating high-quality training data without manual intervention.
The paper investigates the feedback loop between social influence and selection in online communities, showing that similarity between individuals predicts future interactions and continues to grow afterward, while also comparing the predictive power of social ties versus similarity in modeling behavior.
The paper proposes a two-step method for detecting anomalous patterns in categorical datasets by first identifying local anomalies and then searching for significant subsets with unexpectedly high anomaly concentrations, validated on real-world datasets.
The paper introduces a method to reduce query abandonment by analyzing bypassed documents in search results and optimizing for sets of results less likely to be collectively skipped, showing improved performance over maximal marginal relevance in metrics like mean average precision and mean reciprocal rank.
The paper proposes a method to deduplicate URLs by learning rewrite rules from equivalence classes of URLs with similar content, enabling canonicalization without fetching content, and demonstrates its effectiveness through large-scale experiments.
The paper presents a scalable metric learning framework using the log-determinant matrix divergence to efficiently optimize structured Mahalanobis distances for high-dimensional problems, demonstrating improved accuracy and recall in applications like text analysis and collaborative filtering.
The paper explores the integration of constraint programming with itemset mining, demonstrating its flexibility and effectiveness in handling diverse mining constraints through empirical evaluation.
The paper proposes two methods for learning binary classifiers from only positive and unlabeled data, demonstrating their effectiveness over existing approaches in identifying incomplete protein records for molecular biology databases.
The paper introduces a new class of locality-sensitive hash functions for cosine similarity based on concomitant rank order statistics, offering improved collision rates and practical approximations for efficient high-dimensional nearest-neighbor search.
The paper proposes a model-based search tree method that directly mines discriminative and essential frequent patterns by partitioning data at each node, enabling discovery of highly compact and discriminative patterns with extremely low support, outperforming traditional two-step approaches in accuracy and efficiency.
The paper presents a scalable two-phase text classifier combining information retrieval speed with machine learning accuracy, achieving a 27x runtime reduction and improved F-measure on the Reuters RCV1 corpus.
SPIRAL is an efficient and exact model identification method for hidden Markov models that significantly speeds up search by pruning candidates using successive likelihood approximations, achieving over 500x faster performance than naive approaches.
The paper proposes a novel method for classification in sparsely labeled networks by introducing "ghost edges" to enhance information flow between labeled and unlabeled nodes, improving performance over existing approaches while maintaining computational efficiency.
The paper examines how composition attacks exploit independently released anonymized datasets to breach privacy, showing vulnerabilities in techniques like *k*-anonymity while demonstrating that differential privacy and its relaxations resist such attacks by design.
The paper proposes improved entity categorization methods by analyzing cross-document contexts and leveraging large entity lists, enhancing accuracy and scalability over traditional local-context approaches.
The paper proposes a locally weighted ensemble framework for transfer learning that dynamically assigns model weights based on predictive power and structural consistency with test examples, achieving significant accuracy improvements across various domains.
The paper formally defines and studies the banded structure in binary matrices, presents algorithms for measuring deviation from bandedness and finding approximately banded submatrices, and demonstrates the concept's practical utility through experiments on real-world datasets.
The paper proposes a systematic evaluation framework to compare and optimize approximate frequent pattern mining algorithms, introduces improved algorithm variations, and tests them on synthetic and real datasets with noise.
The paper introduces an unsupervised generative model for deduplication that explicitly models cross-field dependencies between attributes, such as titles and venues in citations, achieving a 58% error reduction compared to standard methods.
The paper proposes the **Permu-pattern** algorithm to discover frequent mutable permutation patterns in noisy sequence data, addressing mutation noise and symbol order variations, and demonstrates its effectiveness in gene cluster discovery and efficiency on synthetic datasets.
The paper demonstrates that High Order SVD (HOSVD) simultaneously performs tensor subspace selection (data compression) and K-means clustering, enabling its use for unsupervised learning tasks, and validates this finding on real-world datasets.
The paper introduces *bridging centrality*, a selective metric for identifying critical bridges in networks, demonstrates their disruptive impact when removed, and proposes a *bridge cut* clustering method that outperforms existing approaches in module detection.
The paper proposes two interpretable nonnegative matrix decomposition methods (nonnegative CX and CUR) that represent a matrix as a linear combination of its columns or columns and rows, providing intuitive and accurate results with lower reconstruction errors than existing methods.
The paper introduces an efficient logistic regression method for text categorization that automatically learns variable-length n-gram tokenization using gradient ascent and branch-and-bound optimization, outperforming traditional classifiers like SVMs and coordinate descent logistic regression.
The paper proposes a probabilistic latent semantic visualization method that maps documents and topics into a low-dimensional space based on topic modeling, enabling documents with similar topics to be visualized close together.
The paper presents the first automated system for identifying quasi-experimental designs (QEDs) in databases using first-order logic and theorem-proving to discover causal knowledge without manual intervention.
The paper proposes a framework for multi-label classification by extracting a shared subspace among labels, solving it via a generalized eigenvalue problem, and demonstrates its effectiveness on multi-topic web page categorization.
The paper proposes a greedy method to mine user preferences on unknown categorical attributes from superior and inferior examples, demonstrating its practicality using real and synthetic datasets.
The paper proposes two regression-based methods, **k-regression** and **tree-regression**, to efficiently summarize frequent itemset patterns by minimizing restoration error through probabilistic modeling and partitioning techniques.
The paper presents a fast sequential dual method for training large-scale multi-class linear SVMs, using per-example optimization and heuristics to significantly outperform existing solvers like bundle, cutting plane, and exponentiated gradient methods.
The paper proposes a method to construct concise summaries of large event sequences by balancing summary brevity and descriptive accuracy, using dynamic-programming and greedy algorithms to efficiently reveal local event associations.
The paper introduces an improved collaborative filtering model that combines latent factor and neighborhood approaches while leveraging both explicit and implicit user feedback, achieving higher accuracy on the Netflix dataset and proposing a new top-K evaluation metric.
The paper analyzes temporal communication patterns in a university email network, defining a "network backbone" of fastest information pathways that reveals a mix of strong ties and long-range bridges, offering new insights into social network structure.
The paper proposes ABOD (Angle-Based Outlier Detection), a parameter-free method that identifies outliers in high-dimensional data by assessing the variance in angles between data points, outperforming distance-based approaches like LOF.
The paper proposes a generative model for predicting target events in categorical streams by analyzing significant frequent episodes in historical data, converting them into specialized Hidden Markov Models (HMMs), and using mixture likelihoods for prediction, validated on synthetic and real-world web interaction data.
The paper presents a microscopic analysis of social network evolution, demonstrating that edge locality and a triangle-closing model are key drivers of network growth, leading to power-law degree distributions, and introduces a parameter-free model to generate realistic synthetic networks.
The paper introduces **Cut-and-Stitch (CAS)**, a parallel algorithm for efficiently learning Linear Dynamical Systems (LDS) on multi-core architectures by addressing data dependencies, achieving near-linear speedups while maintaining accuracy, and generalizing to similar models like HMMs and SKFs.
The paper proposes novel active learning algorithms that directly construct queries for labels, bypassing the limitations of pool-based methods, and demonstrates their effectiveness in reducing predictive error with fewer queries while remaining adaptable to pool-based scenarios.
The paper proposes a spectral classification framework for domain-transfer learning that leverages labeled in-domain data to classify unlabeled out-of-domain data by optimizing consistency between supervision and intrinsic structure.
The paper proposes an unsupervised probabilistic method to generate multi-faceted topic overviews from text collections using user-defined keywords, overcoming limitations of supervised approaches and producing informative, structured summaries comparable to supervised methods.
The paper proposes a family of cost-sensitive boosting methods for multi-class classification using p-norm loss functions, demonstrates their theoretical convergence and empirical superiority over existing methods in minimizing misclassification costs.
The paper investigates two sparse, efficient online learning updates—exponential moving average and a hinge-loss-based method—that constrain feature connections to improve scalability and accuracy in large multiclass problems while preserving performance benefits.
The paper studies real weighted graphs, revealing that non-giant connected components stabilize in size, edge weights follow unexpected power laws, and proposes a generative model for graph growth.
The paper proposes a novel approach called STATPC to identify and represent non-redundant, statistically significant axis-parallel regions in high-dimensional data, outperforming existing projected and subspace clustering methods in accuracy.
The paper introduces two joint topic models, Pairwise-Link-LDA and Link-PLSA-LDA, which integrate text and citation data to improve topic coherence and link prediction, with Link-PLSA-LDA outperforming baselines in scalability and accuracy.
The paper introduces a discriminative, margin-based learning approach for classification with partial labels, formulated as a convex quadratic optimization problem, and demonstrates its effectiveness in improving performance when combined with fully-labeled data or used alone.
The paper introduces and analyzes discriminatory classification rules in data mining, demonstrates the inadequacy of simply removing discriminatory attributes to ensure fairness, and formulates the redlining problem while empirically evaluating the findings on the German credit dataset.
The paper introduces FastLDA, a collapsed Gibbs sampling method for latent Dirichlet allocation (LDA) that achieves significant speedups (up to 8×) over conventional methods while producing identical results, reducing computational costs from O(K) to fewer operations per sample without approximations.
The paper proposes **graph PLS**, an efficient iterative graph mining method using sparse partial least squares regression to extract informative subgraph patterns for improved prediction accuracy in chemical datasets.
The paper presents a nonparametric Bayesian graph model for automatically discovering semantic relationships between words by modeling subject-predicate structures as graphs, demonstrating superior clustering performance compared to baseline models.
The paper analyzes mobile call data, finding that call, talk time, and partner distributions deviate from power-law and lognormal patterns, and proposes PowerTrack to fit these to a Double Pareto LogNormal (DPLN) distribution, linking it to a generative social wealth process.
The paper proposes a Variable-order Multiple active State search (VMS) algorithm based on a Markov model to improve IT ticket routing efficiency by mining resolution sequences, reducing the number of transfers needed to identify resolvers.
The paper demonstrates that acquiring multiple noisy labels for carefully selected data points can significantly improve data quality and model performance, especially when labeling costs are low compared to processing unlabeled data.
The paper introduces *i SAX*, a multi-resolution symbolic representation for indexing and mining massive time series datasets, enabling fast exact and approximate searches to handle terabyte-scale data efficiently.
The paper proposes an efficient method using Non-Negative Matrix Factorization and the Threshold Algorithm to compute personalized aggregations of user-generated content based on trust, improving recommendation relevance and reducing computational costs by over 75%.
The paper proposes a semi-supervised labeling method that combines imperfect heuristics, expert-labeled instances, and cluster structure to classify large datasets with user-specified precision guarantees, allowing "don’t know" labels when precision cannot be met, achieving high coverage and accuracy with minimal expert input.
The paper proposes a collective matrix factorization model that simultaneously factorizes multiple relational matrices while sharing parameters across entities involved in different relations, using Bregman divergences for error measurement and efficient optimization techniques to improve predictive accuracy.
The paper proposes a Bayesian mixture model with time-varying mixing proportions via linear regression to track evolving component trends, demonstrated by analyzing temporal antibiotic resistance patterns in *Escherichia coli* and *Staphylococcus aureus*.
The paper proposes a hypergraph spectral learning method for multi-label classification that captures high-order label correlations, introduces an efficient approximate formulation equivalent to least squares, and demonstrates competitive performance on large-scale datasets.
The paper proposes a spectral framework for identifying evolving communities in dynamic multi-mode networks by leveraging temporal information, demonstrating effectiveness on synthetic and real-world datasets.
The paper introduces **Colibri-S** and **Colibri-D**, efficient methods for low-rank approximation of large static and dynamic graphs, respectively, which outperform existing techniques (CUR and CMD) in speed and space efficiency while maintaining accuracy.
The paper investigates whether complex network metrics (e.g., clustering coefficient, node degree) can better predict NBA team success compared to traditional box score statistics, demonstrating their effectiveness in capturing team dynamics.
The paper explores the effectiveness of a collapsed Gibbs sampler for document clustering using a mixture of multinomials model, demonstrating its superior performance over the EM algorithm while addressing convergence, label switching, and summarization challenges.
The paper proposes a semantic kernel method using Wikipedia to enhance text classification by embedding background knowledge, improving accuracy over traditional Bag of Words and other recent techniques.
The paper presents a discriminatively-trained joint model for simultaneously addressing schema matching, coreference, and canonicalization, demonstrating significant error reductions compared to isolated or cascaded approaches.
The paper proposes three novel techniques—taxonomy-based shrinkage, retraining, and Web supplementation—to significantly improve recall (1.76–8.71×) while maintaining precision in extracting information from Wikipedia's sparse or incomplete articles.
The paper proposes **SAIL**, a summation-based incremental learning method for INFO-K-means clustering, which replaces KL-divergence with Shannon entropy to avoid zero-value dilemmas in high-dimensional sparse data, improving clustering performance, convergence speed, and robustness.
The paper proposes Asymmetric Support Vector Machines (ASVM), a modified SVM that incorporates user tolerance for false positives to achieve lower false-positive rates while maintaining performance and efficiency.
The paper proposes **HYPER**, an approximation algorithm using overlapped hyperrectangles to succinctly summarize transactional databases with a **ln(k) + 1** approximation ratio, and demonstrates its effectiveness on real and synthetic datasets.
The paper proposes a method for anonymizing high-dimensional, unstructured transaction data to protect individual privacy while preserving data utility for research purposes.
The paper introduces the **local peculiarity factor (LPF)** as an improved measure over the peculiarity factor (PF) for characterizing data peculiarity across general distributions, demonstrates its effectiveness for normal distributions, and proposes the **LPF-Outlier** algorithm for outlier detection, which shows strong performance in experiments.
The paper introduces the randomized shortest-path (RSP) dissimilarity, a family of node-based measures in weighted directed graphs that generalizes both shortest-path and commute-time distances, controlled by a parameter θ, and demonstrates its efficient computation and applicability in graph mining tasks.
The paper proposes efficient kernel-based training algorithms for structural SVMs using approximate cutting planes and random sampling, achieving faster performance with comparable accuracy to exact methods.
The paper proposes a stable feature selection framework (DRAGS) that identifies dense feature groups to improve stability and accuracy in high-dimensional data analysis.
The paper categorizes concept drifting in data streams into Loose Concept Drifting (LCD) and Rigorous Concept Drifting (RCD), proposing Kernel Mean Matching (KMM) for LCD and Optimal Weights Adjustment (OWA) for RCD to improve classifier ensemble performance.
FastANOVA is an efficient algorithm for genome-wide association studies that accelerates SNP-pair ANOVA testing by deriving an upper bound for pruning non-significant pairs, enabling batch processing and permutation tests while ensuring no significant pairs are missed.
The paper proposes **CutS3VM**, a fast semi-supervised SVM algorithm that efficiently solves the non-convex S3VM problem by iteratively tightening relaxations via CCCP, achieving **O(sn)** convergence with guaranteed accuracy while outperforming existing methods in speed and performance.
The paper proposes a gene selection method that integrates multiple heterogeneous biological data sources to identify biologically relevant genes by leveraging their intrinsic geometric patterns and covariance analysis.
The paper proposes the **CHECK-POINT** algorithm, which efficiently computes strongly correlated pairs in dynamic databases by using checkpoints to establish upper bounds and maintain candidate pairs, reducing computational cost and memory usage.
The paper presents a case study applying data mining techniques to detect land cover changes in California, addressing limitations of traditional methods by leveraging seasonality and spatio-temporal autocorrelation in Earth Science data.
The paper proposes a gamma mixture model with BIC and EM to automatically identify authoritative users in Yahoo! Answers by distinguishing them from non-authoritative users based on their authority scores.
The paper proposes a context-aware query suggestion method by clustering queries into concepts from click-through data and constructing a concept sequence suffix tree from session logs to improve suggestion relevance and coverage.
The paper introduces *persuasive visualization*, where data is presented to emphasize a specific message or assumption, leveraging preattentive processing and deliberate design to guide viewer attention, as demonstrated in the *Morpherspective* system.
The paper proposes a corpus-based model using association rule mining and web-mining algorithms to detect and measure privacy leaks by identifying word co-occurrences that reveal sensitive topics in both public and private document repositories.
The paper investigates machine learning methods—ensemble template matching and SVM classification—for improving markerless gating in lung cancer radiotherapy using fluoroscopic images, achieving ~95% precision in target dose delivery at ~35% duty cycle.
The paper presents **ITACS**, a system combining text classification and business intelligence with an interactive labeling interface to automate customer satisfaction (C-Sat) analysis in the services industry, addressing real-world challenges in deployment and highlighting the importance of interactivity, label-set design, accuracy measurement, and interpretability.
The paper presents the design and implementation of Sector (a storage cloud) and Sphere (a compute cloud) for high-performance distributed data mining, comparing their performance to Hadoop.
The paper presents an automated cyclone discovery and tracking method using heterogeneous satellite data (QuikSCAT wind and TRMM precipitation) through feature extraction, ensemble classification, and knowledge sharing via a Kalman filter, outperforming existing techniques.
The paper proposes a geo-aware data mining method using P2P query strings to identify emerging artists by detecting localized spikes in popularity, with 30% of detected artists achieving national success.
The paper presents a method to enhance customer targeting models by incorporating actively selected web content from company websites, improving prediction accuracy while minimizing acquisition and processing costs.
The BioJournalMonitor is a decision support system that uses text mining and probabilistic topic models to anticipate MeSH term annotations and predict emerging trends in biomedical literature, enabling early identification of potential biomarkers and research trends.
The paper introduces a novel statistical methodology for discovering temporal patterns in event history data, using observed-to-expected incidence graphs and a new association measure with shrinkage to identify meaningful relationships in patient records.
The paper presents a scalable, near real-time method for detecting, characterizing, and classifying bursts in user queries within large-scale eCommerce systems, building on prior burst detection techniques to enable practical merchandising applications.
The paper proposes a method to identify developers' domain expertise by analyzing source code vocabulary and version history, clustering domain concepts using KMeans, and linking developers to these concepts.
The paper presents *ArnetMiner*, a system for automatically extracting and integrating academic researcher profiles and publications, modeling academic networks, and providing search services like expertise search, with evaluations demonstrating its effectiveness.
The paper proposes TagMark, a method using adapted mark-recapture techniques to reliably estimate RFID-tagged item quantities in retail scenarios, addressing challenges like dynamic item sets and non-random sampling while ensuring accuracy and integration with enterprise systems.
The paper experimentally compares scalable online ad-serving algorithms for maximizing revenue under relevance constraints, analyzing performance differences between revenue-focused and clickthrough-rate-focused approaches using data from Microsoft's ad network.
The paper presents a visual-analytic toolkit for exploring dynamic interaction networks, integrating visualization techniques with event-driven analysis to handle scalability and highlight critical changes, demonstrated through bibliometric and Wikipedia case studies.
The paper proposes a kernel-based method for integrating heterogeneous data (MRI, demographic, and genetic measures) to improve Alzheimer's disease prediction accuracy and biomarker identification, demonstrating significant performance gains in experiments.
The paper proposes a privacy-preserving Cox regression model for survival analysis using an optimal sparse linear projection to enable multi-institutional data collaboration without direct data sharing, achieving near-optimal performance on real-world clinical datasets.
The paper demonstrates how supervised learning models can predict invoice payment outcomes with high accuracy, enabling tailored collection strategies that reduce outstanding receivables and collection time by up to fourfold compared to non-model-driven approaches.
The paper proposes a multiple-instance learning approach to improve sub-document classification for contextual advertising tasks—such as sensitive content detection and opinion mining—using only page-level labels, achieving better performance than traditional methods.
The paper presents a PostgreSQL-based inductive database prototype that allows querying both data and their generalizations (e.g., rules, trees) through virtual mining views, integrating frequent itemset, association rule, and decision tree mining.
"Febrl is an open-source Python-based system with a GUI that provides data cleaning, deduplication, and record linkage functionalities, enabling researchers and practitioners to experiment with, compare, and integrate new techniques while serving as a training and practical tool for record linkage projects."
The paper introduces *tagFlake*, a system that uses *TMine* to organize tags from text documents into navigable, hierarchical structures for improved visualization, classification, and navigation without relying on language-specific processing.
The paper presents IBM's ITACS system, which integrates text classification, business intelligence, and interactive labeling to automate customer satisfaction analysis in the services industry, addressing challenges like label-sets, accuracy measurement, and interpretability.
**Summary:** The paper introduces *DiMaC*, a tool for detecting and cleaning disguised missing data in datasets without requiring domain knowledge, demonstrating its techniques, architecture, effectiveness, and future challenges.
Pattern-Miner is an integrated system for managing and mining data patterns throughout their lifecycle, including generation, storage, querying, comparison, and meta-mining operations.
The paper presents **CRO**, a system for structuring Chinese online product reviews by extracting features, opinions, and polarities from unstructured text into a structured table, demonstrating satisfactory performance in real-world applications.
"Morpheus is an interactive visualization tool that enables exploration, analysis, and comparison of subspace clustering results to enhance knowledge discovery and parameter tuning."
The paper presents a software system that detects buzz events from query bursts in eCommerce, generates semantic-based product recommendations around a single daily buzz query to enhance user engagement, and applies KDD principles in an industrial setting.
Pictor is an interactive wrapper induction system that minimizes labeling costs while maintaining high accuracy by using record-level wrappers and wrapper-assisted labeling to predict labels in partially or fully new webpages.
MobiDesk is a mobile virtual desktop infrastructure that virtualizes user computing sessions by decoupling display, OS, and network resources, enabling seamless mobility, high availability, and low overhead while supporting unmodified applications.
The paper proposes an Adaptive Code Collection Algorithm (ACCAL) to dynamically manage memory for large mobile applications by transparently loading and discarding code components at runtime, reducing memory usage while maintaining performance.
The paper presents an architecture and novel techniques, including *Client Conduit*, for diagnosing faults in IEEE 802.11 networks, addressing connectivity, performance issues, and unauthorized access points with low overhead.
The paper introduces the sequential Monte Carlo Localization method, which leverages mobility to enhance localization accuracy and precision in mobile sensor networks without additional hardware, outperforming static localization schemes.
The paper introduces VOR base stations (VORBA) for indoor 802.11 positioning using angle of arrival (AOA) without requiring signal strength maps, demonstrating feasibility through experiments, simulations, and analysis.
The paper presents a highly accurate and robust 802.11-based localization system that requires minimal training effort, achieving over 95% precision with standard hardware and brief calibration, even in dynamic environments.
The paper derives optimal and randomized TTL-based controlled flooding search strategies to minimize expected or worst-case search costs in large networks, depending on whether the object's location distribution is known or unknown.
The paper presents two centralized algorithms that dynamically adjust multi-hop wireless network topologies by moving or adding nodes to reduce end-to-end delay, outperforming capacity-increasing alternatives.
The paper introduces the Weighted Cumulative Expected Transmission Time (WCETT) metric for high-throughput routing in multi-radio, multi-hop wireless mesh networks, accounting for link loss rates, bandwidth, and channel interference, and demonstrates its superior performance over existing metrics in a 23-node testbed.
The paper proposes power-saving strategies for wireless sensor networks in both surveillance and tracking states, introducing metrics for surveillance quality and a collaborative messaging scheme to optimize energy use while maintaining effective target tracking.
The paper establishes that the critical value of \( \frac{np\pi r^2}{\log(np)} \) for ensuring \( k \)-coverage in mostly sleeping sensor networks is 1 across grid, random uniform, and Poisson deployments, while also correcting prior grid-based results and validating the analysis with simulations.
This paper presents an experimental evaluation and comparative analysis of various protocol stack optimization techniques to enhance application performance in commercial wireless wide-area networks (WWANs), benchmarking their impacts and interdependencies while providing practical measurement guidelines.
The paper statistically characterizes static and roaming flows in a large campus wireless network, modeling flow arrivals with a Weibull regression and demonstrating spatial similarity, while validating findings with additional traces and practical applications.
The paper analyzes a mature campus WLAN's 17-week trace, revealing shifts from web-dominated to peer-to-peer and multimedia traffic, increased on-campus usage, limited wireless VoIP adoption, and varied client mobility patterns, with most users remaining largely stationary.
The paper analyzes the resilience of ad hoc networks to DoS attacks, demonstrating that attacks like JellyFish and Black Hole can severely disrupt network performance while paradoxically increasing capacity by favoring one-hop communications.
The paper introduces **Slotted Seeded Channel Hopping (SSCH)**, a link-layer protocol that enhances IEEE 802.11 ad-hoc network capacity by coordinating channel switching to minimize interference and maximize concurrent transmissions.
The paper proposes two cross-layer mechanisms—early packet loss notification (EPLN) and best-effort ACK delivery (BEAD)—to improve TCP performance in mobile ad hoc networks by reducing timeouts caused by mobility-induced packet and ACK losses.
The paper proposes truthful multicast routing protocols for selfish wireless networks, demonstrating that VCG-based mechanisms fail to ensure honest cost reporting and introducing alternative protocols that incentivize truthful behavior while optimizing network efficiency.
The paper proposes an efficient algorithm for computing asymptotically optimal clustering in newly deployed ad hoc and sensor networks modeled as multi-hop quasi unit disk graphs with asynchronous wake-ups and limited topology knowledge, enabling synchronized sleep-listen schedules within clusters.
The paper proposes FLSS<sub>k</sub>, a fault-tolerant localized topology control algorithm that ensures k-vertex connectivity in wireless networks while minimizing maximum transmission power and outperforming existing distributed algorithms in power efficiency.
The paper examines fairness and performance in multihop wireless backhaul networks, proposing a reference model, analyzing key factors like MAC protocols and congestion control, and developing a fairness algorithm to mitigate spatial bias while studying its impact on throughput.
The paper proposes a scalable cross-layer framework for multi-cell packet data systems, integrating load-aware handoff, weighted Alpha-Rule scheduling, and MAC-layer cell breathing to enhance resource utilization, load balancing, and fairness while maintaining throughput and robustness.
The paper proposes efficient algorithms for association control in wireless LANs to achieve max-min fair bandwidth allocation by balancing user load among access points, providing constant-factor approximation solutions for both offline and online scenarios.
The paper introduces a scalable analytical framework for modeling MAC protocols in multihop ad hoc networks, enabling per-node parameterization and performance evaluation without spatial assumptions, and demonstrates its accuracy and efficiency by validating IEEE 802.11 DCF results against simulations.
The paper proposes **Medium Access Diversity (MAD)**, a scheme that enhances wireless LAN throughput by 50% over existing rate adaptation methods by leveraging multiuser diversity through selective transmissions to receivers with optimal channel conditions while ensuring fairness.
The paper introduces **Battery Aware Medium Access Control (BAMAC(k))**, a distributed MAC protocol that leverages battery-state information to enhance fairness, extend network lifetime, and ensure uniform battery discharge in ad hoc wireless networks, outperforming IEEE 802.11 and DWOP in energy efficiency.
The paper proposes a distributed topology control algorithm that constructs a planar, energy-efficient network structure for both unicast and broadcast communications, with bounded node degrees and low message complexity.
The paper introduces Multi-Radio Diversity (MRD), a wireless system that enhances loss resilience in WLANs by combining frame copies and using a low-overhead retransmission scheme (RFA), achieving up to 2.3× throughput gains over single-radio 802.11a networks.
The paper evaluates the performance of Roofnet, an unplanned 802.11b mesh network with omni-directional antennas and multi-hop routing, demonstrating usable throughput (627 kbits/s on average) despite minimal deployment planning.
The paper establishes that the capacity of multi-channel wireless networks depends on the ratio of channels (\(c\)) to interfaces (\(m\)), showing degradation when \(m < c\), except in random networks with up to \(O(\log n)\) channels, where capacity remains optimal (\(\Theta(W \sqrt{n/\log n})\)) even with a single interface, and extends this analysis to include switching delays.
The paper presents a mathematical formulation and solution for joint channel assignment and routing in multi-radio wireless mesh networks to optimize throughput while ensuring fairness among clients, demonstrating performance within a constant factor of optimal and outperforming theoretical worst-case bounds.
The paper proposes a network model for multi-radio multi-channel wireless mesh networks, derives necessary conditions for feasible rate vectors and capacity bounds, and develops static and dynamic channel assignment schemes that achieve near-optimal throughput.
The paper proposes **MAP**, a scalable and localized geometric routing protocol for sensor networks that uses the medial axis to construct compact network abstractions, enabling efficient, load-balanced routing with guaranteed delivery.
The paper evaluates the impact of routing computations on performance in content-based routing networks with mobile data sources, formalizes mobility protocols, and demonstrates that proper protocols can mitigate scalability issues caused by mobility.
The paper proposes Corsac, an integrated game-theoretic and cryptographic protocol for wireless ad-hoc networks that ensures cooperation-optimal routing and forwarding by combining VCG-based routing with cryptographic enforcement to incentivize node participation.
The paper proposes a radically new MANET architecture featuring relay-oriented physical-layer switching, path-centric MAC, and cooperative transport to achieve wireline-like performance by addressing latency, capacity, and robustness challenges.
The paper introduces CeTV and Ca-Fi, systems that enable cellular and Wi-Fi communication over existing CATV networks without modifying end-user devices, enhancing coverage and capacity while proposing modified MAC protocols for Ca-Fi.
The paper "Model T" presents an empirical, data-driven model for user mobility patterns in a campus WLAN, showing hierarchical AP clustering, heavy-tailed distributions for transitions, and strong agreement between synthetic and real traces.
The paper analyzes interference in unplanned, unmanaged 802.11 wireless networks ("chaotic deployments"), demonstrates its negative impact on performance, and proposes self-managing algorithms for power control and rate adaptation to improve client experience.
This paper analyzes twelve embedded two-flow topologies in multi-hop wireless networks, identifying critical asymmetric and symmetric channel state scenarios, modeling CSMA-induced unfairness and fairness dynamics, and validating predictions through simulations.
The paper presents an analytical model for efficiently dimensioning GPRS/EDGE networks with shared capacity constraints across multiple cells, avoiding complex multi-dimensional Markov models while maintaining accuracy and enabling fast iterative dimensioning.
MoB is a mobile bazaar architecture that enables decentralized, opportunistic trading of wide-area wireless services among users, decoupling infrastructure from service providers and promoting flexible, fine-grained service interactions.
**Summary:** The paper proposes *PeopleNet*, a wireless virtual social network that enables efficient, location-based information search by propagating queries through cellular infrastructure and peer-to-peer connections within designated geographic zones ("bazaars"), demonstrating improved performance with a swap-based query propagation model and a greedy algorithm for query matching.
The paper presents a platform for rapid prototyping of multi-channel, context-aware mobile applications, demonstrated through a tourist information system using speech and interactive paper for an arts festival.
The paper investigates how using mobile relays or mobile sinks in wireless sensor networks can extend network lifetime, showing that mobile sinks offer the greatest improvement but mobile relays can also enhance performance, with an upper bound of four times lifetime improvement and requiring only a two-hop proximity to the sink.
The paper develops theoretical foundations for *k*-barrier coverage in wireless sensor networks, proposing efficient algorithms to verify coverage, optimal deterministic deployment patterns, and probabilistic models for weak and strong barrier coverage, showing they require fewer sensors than full coverage while deriving critical conditions for weak *k*-barrier coverage.
The paper proposes a cross-layer optimization approach for routing data traffic in UWB-based sensor networks, combining link-layer scheduling, power control, and network-layer routing, and offers both exact and heuristic solutions for small and large networks, along with a closed-form analysis of maximum achievable rates.
The paper introduces a method to derive quantitative models for correlation clusters, enabling the extraction of linear dependencies and prediction of object probabilities, applicable as a post-processing step to any correlation clustering algorithm.
The paper proposes a framework for learning to rank networked entities by modeling edge conductances in Markov walks, with two approaches: one that identifies hidden preferred communities using constrained maximum entropy network flow, and another that learns fixed conductances per edge type using an approximate Newton method.
The paper presents an exact algorithm and a more efficient approximation algorithm for spatial scan statistics, extends these methods to grid-based data, demonstrates their superior performance in experiments, and proves that streaming algorithms for large datasets require linear space to achieve approximate solutions.
The paper introduces global distance-based trajectory segmentation methods, including exact, greedy, and lightweight variance-based approaches, to optimize inter-object separability for efficient storage, retrieval, and mining operations.
The paper examines how social groups form, grow, and evolve by analyzing structural network features in LiveJournal and DBLP datasets, finding that membership and growth depend on nuanced network properties like friend connectivity and topic alignment.
The paper presents a robust outlier detection method using Transductive Confidence Machines and statistical testing, which effectively identifies outliers with or without prior clustering information and works well on noisy datasets.
The paper proposes **Robust Information-theoretic Clustering (RIC)**, a noise-resistant, parameter-free clustering framework based on the minimum description length (MDL) principle, which refines and purifies any initial clustering while automatically determining optimal cluster shapes and quantities.
The paper presents a collusion-resistant, anonymity-preserving data collection protocol with linear communication rounds that avoids zero-knowledge proofs, improving efficiency for large-scale data mining scenarios.
The paper introduces an efficient, scalable out-of-core frequent itemset mining algorithm with novel I/O optimizations, achieving over 400x speedup on large datasets up to 75GB.
The paper proposes rank-based support measures (extending Kendall’s tau, Spearman’s Footrule, and rho) for mining correlated numerical attribute sets, introduces mixed numerical-categorical patterns, and provides efficient algorithms for frequent pattern discovery.
NeMoFinder is an efficient algorithm for discovering meso-scale network motifs in large protein-protein interaction networks, enabling the identification of size-12 motifs in yeast PPI data and aiding in assessing interaction reliability.
The paper presents scalable algorithms to estimate the global PageRank of pages within a localized web domain using only O(n) resources, demonstrating effectiveness with minimal additional crawling.
The paper systematically analyzes orthogonal three-factor nonnegative matrix factorization (NMF) \( X = FSG^T \), develops new update rules with convergence guarantees, and demonstrates its effectiveness in simultaneous row-column clustering through experiments and a novel evaluation approach for word clustering.
The paper proposes a fast, accurate, and user-friendly regression framework using data summarization in random decision trees, which performs comparably or better than existing methods across diverse problem types without requiring extensive parameter tuning.
The paper introduces *reverse testing*, a computationally efficient framework to select the best-performing classifier under sample selection bias by evaluating models on a possibly biased training set to predict their accuracy on an unbiased test set.
The paper introduces *quantification* and *cost quantification* as machine learning tasks focused on accurately estimating class distributions and aggregated costs in test sets, evaluates methods like Median Sweep for robustness under varying conditions, and establishes a research framework while addressing class imbalance and shifting priors but not general concept drift.
The paper introduces swap randomization, a method to assess the significance of data mining results on high-dimensional 0-1 data by generating random datasets with matching row and column margins, enabling evaluation of patterns like frequent sets, clustering, and rankings.
The paper proposes a new efficient probabilistic model for mining labeled ordered trees that significantly reduces time and space complexity while maintaining expressive power and predictive accuracy, as validated on synthetic and real glycobiology datasets.
The paper proposes a Unified Kernel Machines (UKM) framework integrating supervised, semi-supervised, and active learning, introducing Spectral Kernel Learning (SKL) for semi-supervised kernel design and demonstrating its effectiveness through the Unified Kernel Logistic Regression (UKLR) paradigm, which outperforms traditional methods.
The paper introduces **tenuous outerplanar graphs**, a generalization of trees, and presents an **incremental polynomial-time algorithm** for **frequent subgraph mining** in this class, demonstrating its effectiveness on molecular graph data.
The paper proposes an adaptive, time-varying Poisson process model for detecting anomalous events in count-based time-series data, outperforming threshold-based methods by accounting for periodicity and bursty anomalies in real-world datasets like traffic and building access logs.
The paper presents a Cutting Plane Algorithm for training linear SVMs with provably linear training time \( O(sn) \) for classification and \( O(sn \log n) \) for ordinal regression, significantly outperforming existing methods like SVM\(^\text{light}\) on large datasets.
The paper proposes an information-theoretic approach to mine **Quantitative Correlated Patterns (QCPs)** using normalized mutual information and all-confidence, introducing the **QCoMine** algorithm for efficient and high-quality pattern discovery in quantitative databases.
The paper introduces *maximally informative k-itemsets*, defined as itemsets of size *k* that maximize joint entropy to partition data uniformly, and presents efficient algorithms for their discovery.
The paper introduces "cycle-free effective conductance" (CFEC), a statistically robust proximity measure for networks that handles multiple endpoints and directed edges, provides effectiveness scores, and is demonstrated on large datasets including telecommunications, IMDB, and co-authorship networks.
The paper proposes an efficient algorithm for hierarchical topic segmentation of websites by optimizing cost measures based on topic labels to identify topically cohesive regions in a URL tree.
The paper introduces a new EM framework that optimizes both model parameters and the number of components by incorporating nonparametric density estimation, with applications in polygonal approximation of laser range data and edge pixel grouping in computer vision.
The paper introduces workload-aware anonymization algorithms that tailor data privacy protection to specific data mining workloads, improving effectiveness over traditional methods.
The paper proposes a very sparse random projection method using entries in {-1, 0, 1} with probabilities (1/2√D, 1-1/√D, 1/2√D) to achieve a √D-fold speedup while maintaining accuracy in distance estimation.
The paper proposes an OLAP-based approach called "Opportunity Map" for analyzing rule interestingness by enabling users to explore knowledge spaces systematically and visualize groups of rules in context, addressing the limitations of traditional rule mining methods that treat rules individually and discard useful context information.
The paper introduces a scalable ZBDD-based technique for efficiently mining high-dimensional contrast patterns, including emerging and disjunctive emerging patterns, outperforming existing methods.
The paper proposes a relation summary network model for unsupervised learning on k-partite graphs to identify local and global hidden structures, along with an effective algorithm and experimental validation.
The paper introduces a tensor-CUR decomposition as an interpretable data analysis tool for multi-indexed data, demonstrating its effectiveness in compressing hyperspectral images and reconstructing missing entries in recommendation systems.
The paper proposes a novel approach to generate semantic annotations for frequent patterns by analyzing their context, selecting informative indicators, and extracting representative transactions and similar patterns to enhance interpretability.
The paper proposes polynomial-time dynamic programming and greedy heuristic methods to optimally aggregate multiple time partitions of sequential data, demonstrating their effectiveness in mobile-user behavior clustering and genomic sequence segmentation.
The paper introduces **network structure indices (NSIs)**—comprising node annotations and a distance estimation function—to efficiently and accurately approximate computationally intensive network properties like centrality measures and graph diameter.
The paper proposes a linear programming-based method to learn sparse, low-dimensional distance metrics that preserve relative proximity relationships among objects while improving computational efficiency.
The paper introduces **Dynamic Tensor Analysis (DTA)** and **Streaming Tensor Analysis (STA)** for scalable, space-efficient, and automatic pattern detection in high-order, high-dimensional data streams, demonstrating their effectiveness in anomaly detection and latent semantic indexing on large real-world datasets.
The paper proposes localized solutions to improve hierarchical content classification by modifying semantically sound but suboptimal taxonomies, demonstrating enhanced performance through empirical analysis.
The paper presents an efficient algorithm for mining distance-based outliers in large databases using minimal memory (less than 1% of the dataset) and at most two dataset scans, significantly outperforming existing methods.
The paper introduces the *center-piece subgraph* problem for finding a central node connected to multiple query nodes in a network, proposes efficient solutions for various query types (OR, AND, softAND), and validates the method's effectiveness and speed on the DBLP dataset.
The paper proposes a method for anonymizing sequential data releases by preventing effective linkage between current and previous releases using the concept of "lossy join" to maintain data utility while ensuring privacy.
The paper introduces a non-Markov continuous-time topic model that captures evolving topical trends by associating each topic with a dynamic timestamp distribution, improving topic quality and timestamp prediction across diverse datasets.
The paper presents generic techniques for discovering significant association rules by allowing flexible definitions of true and false discoveries through arbitrary statistical hypothesis tests while ensuring strict control over false discovery risk.
The paper proposes a greedy algorithm to extract top-*k* patterns that maximize significance while minimizing redundancy, with an *O*(log *k*) approximation guarantee, and demonstrates its utility in applications like disk prefetching and document theme extraction.
The paper proposes an efficient algorithm for Regularized Discriminant Analysis (RDA) that enables optimal parameter estimation for high-dimensional data, improving classification performance while reducing computational costs.
The paper proposes supervised (SPPCA) and semi-supervised (S²PPCA) probabilistic PCA models that incorporate label information for improved dimensionality reduction, offering efficient EM-based learning and strong performance in multi-task settings.
The paper proposes a linear-time suffix tree algorithm to extract statistically important substring groups as features for text classification, demonstrating strong performance across multiple languages.
The paper proposes a novel method for detecting events from click-through data by clustering query-page pairs based on semantic similarity and evolutionary patterns over time, using a two-phase graph cut algorithm.
The paper proposes a Hierarchical Conditional Random Fields model to simultaneously perform data record detection and attribute labeling in web data extraction, improving accuracy over decoupled approaches by integrating hierarchical interactions and feature importance.
The paper proposes an active learning-based classification approach for outlier detection that improves computational efficiency and explanatory power while maintaining competitive performance with density-based methods.
The paper proposes an efficient heuristic algorithm to hide a minimal set of sensitive data entries while preserving privacy against adversarial data mining by exploiting correlations between fields.
The paper introduces CCCS, a top-down associative classifier using the novel Complement Class Support (CCS) measure to generate positively correlated rules without threshold parameters, particularly effective for imbalanced class distributions.
The paper proposes a mathematical and computational framework for analyzing dynamic social networks by incorporating temporal interaction data, addressing limitations of static network models.
The paper proposes an adaptive, two-stage collective resolution strategy for efficiently resolving entity references in real-time queries on unclean databases, demonstrating its effectiveness on large publication datasets.
The paper presents a method for compressing large, complex machine learning ensembles into smaller, faster models while maintaining performance, addressing storage and computational constraints.
The paper proposes and evaluates user profile attributes for detecting attacks in collaborative recommender systems, demonstrating that machine learning classification incorporating attack-derived features outperforms previous generalized detection methods.
The paper proposes a modified Margin Balanced Winnow algorithm for single-pass online learning, achieving performance comparable to linear SVM and batch methods, while enabling on-the-fly feature selection and benefiting from voting schemes.
The paper introduces an evolutionary clustering framework that balances accuracy in current data representation with temporal consistency, applying it to *k*-means and agglomerative hierarchical clustering while demonstrating effectiveness on real datasets.
The paper presents efficient algorithms for discovering bucket orders (total orders with ties) from pairwise precedence data, offering approximation guarantees and scalability, with experimental validation on synthetic and real datasets.
The paper proposes a correlation-based multiple view validation method to enhance relational data mining by integrating knowledge from interconnected relations into a meta-learning framework, improving accuracy and efficiency compared to existing approaches.
The paper proposes a novel recommendation method for subscription services that maximizes subscription period extensions by identifying frequent purchase patterns of long-term users and simulating these patterns for new users, combining survival analysis and maximum entropy models to improve recommendations.
The paper proposes a dynamic, real-time forecasting model for online auction prices using functional data analysis to incorporate price levels, velocity, and acceleration, demonstrating improved accuracy over static methods in eBay book auctions.
The paper introduces polynomial association rules, a novel method for uncovering nonlinear relationships in numeric data without discretization, which extends classic association rules to numeric attributes and improves logistic regression performance over traditional stepwise methods.
The paper proposes **CFI-Stream**, an efficient and scalable method for mining closed frequent itemsets in data streams, enabling real-time updates and adaptive handling of stream dynamics.
The paper proposes a hybrid text classification approach combining human reasoning with machine learning to reduce human effort while maintaining or improving accuracy in sentiment classification tasks.
The paper introduces *storytelling* as a data mining problem that connects disjoint object sets via chains of approximate redescriptions, proposes an efficient A*-based algorithm with CARTwheels for large datasets, and demonstrates applications in linguistics, bioinformatics, and literature analysis.
The paper analyzes the structure and evolution of large online social networks, identifying three distinct regions (singletons, isolated star-structured communities, and a persistent giant component) and proposes a growth model with passive users, inviters, and linkers to explain these patterns.
The paper proposes cryptographically private protocols for support vector machines (SVMs), including learning algorithms, classification, and kernel computation, ensuring outputs remain encrypted and can only be decrypted by mutual agreement among participants while enabling private data analysis.
The paper proposes a reinforcement model to identify reviewer bias and object controversy by analyzing evaluation deviations, emphasizing the mutual dependency between bias and controversy and validating the approach with real-world data.
The paper evaluates various graph sampling methods, finding that random-walk and "forest fire" techniques perform best in accurately representing large graphs with samples as small as 15%, while edge-based strategies underperform and uniform node sampling works surprisingly well.
The paper proposes a new clustering algorithm that generates a partially ordered set of clusters from a dissimilarity matrix, addressing limitations of classical clustering methods in preserving all available information.
The paper introduces a visual data mining framework that combines machine learning projection algorithms (like GTM and HGTM) with information visualization techniques (such as parallel coordinates and magnification factors) to enable user-involved exploration, demonstrating effectiveness on a chemoinformatics dataset and scalability for large datasets.
The paper proposes a **Contextual Probabilistic Latent Semantic Analysis (CPLSA) model**, extending PLSA by incorporating context variables to analyze topical theme variations across different contexts (e.g., time, location) in text mining tasks.
The paper proposes a multi-view regression method for estimating unobserved targets (e.g., customer wallet) by modeling them as central links in a directed graphical model, reducing the problem to convex optimization or least squares regression under Gaussian noise and conditional independence assumptions.
The paper introduces the Time Series Knowledge Representation (TSKR), a hierarchical language for interval patterns, and presents efficient mining algorithms that outperform Allen's interval relations in conciseness and explanatory power for temporal data analysis.
The paper proposes a scalable large-margin binary classification method using Second Order Cone Programming (SOCP) that leverages clustering to reduce computational complexity while maintaining accuracy comparable to SVMs.
The paper introduces **entity-topic models**, which improve upon traditional topic models by explicitly learning relationships between named entities (who/where) and topics (what) in news articles, enhancing predictive performance for entities.
The paper proposes a distributed data mining approach using a Grid Monitoring System (GMS) and an outlier detection algorithm to identify misconfigured machines in large-scale grid systems efficiently and scalably.
FEMine is an automated system for analyzing gene expression in fruit fly embryo images using latent spatial themes (LGEs), achieving high performance in classification, clustering, and image retrieval.
The paper demonstrates that adding a small number of simple filterbots (surrogate users based on item or user attributes) improves the robustness of collaborative filtering in cold-start scenarios without compromising performance when data is plentiful.
The paper proposes MONIC, a framework for modeling and monitoring cluster transitions in data streams to analyze the nature of changes, such as cluster disappearance, migration, or emergence, based on data content rather than topological properties.
The paper demonstrates that using deep linguistic structures, rather than surface text patterns, significantly improves the extraction of semantic relations (e.g., person-birthdate pairs) from web documents, as validated by experiments with the prototype system LEILA.
The paper proposes using statistical language models to extract contextual information from users' long-term search history, particularly clickthrough data, to enhance query language models and improve search accuracy for both fresh and recurring queries.
The paper proposes an efficient (1+ε)^2-approximation algorithm for maximum margin discriminant analysis (MMDA) that reduces time complexity to linear in the number of training points while maintaining competitive accuracy, outperforming traditional kernel methods like KPCA and KFD in speed and scalability.
The paper proposes a probabilistic method using Markov Random Fields to iteratively summarize large sets of frequent itemsets by modeling items as random variables and incrementally refining the model based on patterns that cannot be accurately inferred, achieving superior performance over existing approaches.
The paper proposes an efficient algorithm that optimizes classifier accuracy in concept-drifting data streams by balancing historical and recent data influence using a stochastic model to estimate the most-likely current distribution.
The paper analyzes Yahoo! search query logs from 1.35 million users over six months, demonstrating that users exhibit consistent and varied topical interests, with query history and click behavior offering significant opportunities for search personalization.
The paper proposes a semi-supervised time series classification method that efficiently leverages limited labeled data and abundant unlabeled data to build accurate classifiers across diverse domains like ECG, handwriting, and video.
The paper proposes the (α, k)-anonymity model, an enhanced version of k-anonymity, to protect both individual identification and sensitive attribute associations in privacy-preserving data publishing, proves its NP-hardness, and presents optimal and scalable algorithms with experimental validation.
The paper proposes an incremental approximate matrix factorization algorithm using Kronecker and incomplete Cholesky decompositions to accelerate SVM training while maintaining accuracy, demonstrating significant speedup and parallelizability.
The paper proposes a sampling-based outlier detection algorithm that efficiently identifies distance-based outliers with fixed computational cost and provable accuracy guarantees, outperforming state-of-the-art methods in expensive distance computation domains.
The paper proposes two models (log-linear and biased belief) to learn user-specific interestingness of patterns from interactive feedback, enabling efficient discovery of relevant patterns without requiring explicit prior knowledge.
The paper investigates how data distributions affect K-means clustering, showing that K-means favors uniform cluster sizes, entropy biases validation toward balanced clusters, and the Coefficient of Variation (CV) of cluster sizes typically falls between 0.3 and 1.0.
The paper proposes a utility-based anonymization framework using local recoding to improve discernability, query accuracy, and analytical quality in microdata, outperforming global recoding methods.
The paper introduces a biomedical literature clustering method combining a semantic-based bipartite graph representation and mutual refinement strategy, improving cluster quality by 29.5% and reliability by 26.3% over Bisecting K-means.
The paper proposes **Cocain**, an efficient algorithm for mining coherent closed quasi-cliques from large dense graph databases, leveraging novel pruning techniques and closure checking to overcome computational challenges.
The paper introduces *progressive confident rules*, a novel pattern for identifying sequences of states with increasing confidence leading to a specific end state, along with efficient mining algorithms and applications in classification.
The paper proposes a time-series-based method using adaptive window sizes to detect shilling attacks in recommender systems by analyzing rating distribution changes over time.
The paper proposes algorithms for mining and evaluating bridging rules—distinct from association rules—that link items from different conceptual clusters, using non-linear metrics to measure their interestingness based on cluster distance and importance.
The paper proposes a graph-regularized linear prediction model for web-page categorization that combines text features and link structures through convex optimization, improving classification accuracy.
BLOSOM is a framework for efficiently mining frequent minimal and closed boolean expressions (conjunctions, disjunctions, and their combinations) from binary datasets using systematic pruning techniques.
The paper proposes an asymmetric cascade of sparse hyperplane classifiers for efficient and accurate computer-aided detection in medical images, incorporating feature cost optimization and boosting to improve speed and performance in lung nodule detection from CT scans.
The paper presents four onboard classifiers for detecting cryosphere events in hyperspectral images on the EO-1 spacecraft, with the SVM and manual classifiers outperforming others, enabling autonomous targeting of dynamic events and demonstrating the potential for future missions to use onboard decision-making.
The paper presents efficient text mining techniques—including clustering, rapid categorizer training, and accurate quantification—to analyze customer-support call logs at scale without manual coding, enabling issue tracking and resource allocation at HP.
The paper presents a prototype application developed for the U.S. National Science Foundation to assist program directors in matching proposal reviewers by analyzing proposal content and reviewer expertise, detailing its implementation, alternatives considered, and operational experience.
The paper introduces **GPLAG**, a plagiarism detection tool that analyzes program dependence graphs (PDGs) to effectively identify software plagiarism, even when code is disguised, and employs a statistical filter for scalability, demonstrating high accuracy and efficiency in experiments.
The paper presents an approach using exhaustive feature generation with temporal statistics and meta-learning to create understandable semantic models of music for tasks like genre classification and similarity recommendation, achieving interpretability with minimal accuracy loss.
The paper presents a deployed data mining system at Motorola that uses class association rules, general impressions, and visualization to identify causes of cellular phone call failures, overcoming the limitations of traditional classification techniques.
The paper proposes a machine learning approach to identify optimal top web search results by analyzing past user behavior, achieving high precision and coverage compared to traditional ranking methods.
The paper demonstrates how data mining techniques can effectively analyze large, noisy citizen science datasets from the Cornell Laboratory of Ornithology to identify key environmental features influencing wild bird species prevalence and distribution.
The paper presents *iKDD*, a Java-based, component-oriented framework for bioinformatics knowledge discovery, designed to enhance reusability, productivity, and reproducibility through modular workflows, database connectivity, and rapid prototyping.
The paper introduces the *KiWi* framework to efficiently discover significant OPSM subspace clusters, including small "twig clusters," in massive gene expression data by using bounded parameters *k* and *w* to reduce computational costs while maintaining biological relevance.
The paper presents an Escalation Prediction (EP) system that uses cost-sensitive learning, particularly cost-sensitive decision trees, to predict software defect escalation risks for maximum net profit, successfully deployed in an enterprise software vendor's product group.
The paper introduces **YALE**, an open-source rapid prototyping tool for KDD and machine learning that enables efficient design, evaluation, and optimization of data mining workflows through visual programming and extensive method reuse.
The paper presents a data mining system that detects camouflaged fraud in complex domains like accounting, insider attacks, and insurance by combining multiple machine learning techniques to identify high-risk entities, outperforming domain experts and uncovering hidden patterns with early detection capabilities.
The paper proposes a gradient descent-based algorithm to directly maximize return on investment (ROI) under budget constraints by optimizing monetary measures, outperforming traditional classification, regression, and ranking methods in financial applications.
The paper presents a computationally efficient heuristic for STDMA scheduling under the physical interference model, demonstrating up to threefold throughput improvement over 802.11 in wireless mesh networks while efficiently handling large-scale networks.
The paper proposes a coordinate-based approach to exploit temporal-spatial diversity in wireless mesh networks, improving gateway downstream throughput by 10-35% through interference-aware scheduling without requiring hardware changes.
The paper proposes a network partitioning approach using channel assignment and Local Pooling to enable simple distributed scheduling algorithms in multi-radio wireless mesh networks to achieve 100% throughput.
The paper presents EAR, an efficient and accurate link-quality measurement framework for multi-hop wireless mesh networks that combines passive, cooperative, and active monitoring, minimizes overhead, detects link asymmetry, and improves network capacity utilization without requiring system recompilation or MAC firmware modification.
The paper demonstrates that grassroots Wi-Fi networks can provide viable intermittent connectivity for vehicular internet access, with median upload speeds of 30 KB/s and link durations of 13 seconds, supporting applications tolerant of disruptions.
The paper experimentally demonstrates that packet-level diversity over 802.11 frequency bands improves transmission performance and robustness despite practical challenges in implementation and channel model uncertainties.
This paper presents a performance analysis and practical insights from deploying long-distance 802.11b links (1–37 km), showing predictable behavior under ideal conditions but sensitivity to interference, while documenting hardware and system-level challenges.
This paper analyzes student contact patterns derived from university class schedules and rosters, revealing insights into mobility behaviors, the rapid spread of mobile viruses, and efficient data aggregation strategies in a large campus population.
The paper analyzes event capture by mobile sensors, determining optimal velocity, paths, and sensor numbers to minimize event loss probability, comparing performance with static sensors and providing algorithms for motion planning in linear, curved, and planar environments.
The paper introduces *Sweeps*, a class of algorithms that correctly and efficiently localize nodes in sparse bilateration networks, including those with angle and noisy measurements, achieving fast performance even for large, low-density networks.
The paper proposes a distributed, connectivity-based algorithm to detect boundary nodes and form boundary cycles in sensor networks without location information, also deriving the medial axis for routing applications.
The paper proposes a practical synchronization mechanism for multi-channel MAC protocols, addressing a critical implementation challenge to enable efficient one-hop neighbor coordination in frequency-agile wireless networks.
The paper critiques existing rate adaptation guidelines for 802.11 networks, proposes a Robust Rate Adaptation Algorithm (RRAA) that uses short-term loss ratios and adaptive RTS filtering, and demonstrates significant throughput improvements over prior methods.
The paper proposes **IQU**, a queue-based user association management system for WLANs that improves throughput and prevents network collapse under heavy load by fairly regulating access through request queues and work periods.
The paper proposes **MAXchop**, a distributed channel-hopping algorithm for 802.11 hotspots to improve fairness and performance in uncoordinated wireless environments by dynamically assigning channels with minimal coordination.
The paper demonstrates the feasibility of targeted SMS-based DoS attacks on cellular networks, proposes five mitigation techniques (queue management and resource provisioning), and analyzes their effectiveness and tradeoffs.
The paper proposes **A³ (application-aware acceleration)**, a solution suite that improves wireless network performance by addressing application-specific behaviors transparently, demonstrating its effectiveness through emulations with real traffic traces.
The paper demonstrates that third-generation CDMA cellular networks with diverse traffic types can exhibit metastability, causing unpredictable performance fluctuations in key metrics like call blocking rates, thereby complicating quality-of-service guarantees and radio resource management design.
The paper proposes a TCP-aware scheduler for CDMA networks that dynamically adjusts channel rates based on TCP sending rates, improving single-flow throughput by 15.25% through cross-layer optimization.
The paper proves that throughput-optimal scheduling in wireless networks under K-hop interference constraints is NP-hard for K>1 but admits polynomial-time approximation schemes for certain graph classes, enabling near-optimal distributed solutions.
The paper establishes upper and lower bounds for the broadcast capacity of multihop wireless networks, showing it scales as Θ(W/max(1,Δ^d)) in dense networks and remains constant under varying network parameters, while also proving backbone-based schemes achieve near-optimal throughput and highlighting key differences from unicast capacity.
The paper analyzes performance gains in multi-access networks, showing that channel-aware access selection and trunking yield the most significant improvements, while multi-access diversity offers limited benefits.
The paper presents efficient centralized and distributed TDMA link scheduling algorithms for static wireless networks under different interference models, optimizing throughput while using time slots within a constant factor of the optimum.
The paper proposes a localized algorithm (AFA) for achieving aggregate fairness in wireless sensor networks by ensuring equitable bandwidth allocation among data sources using only local operations, compatible with any routing protocol and improving energy efficiency.
The paper proposes a "double rulings" scheme for information brokerage in sensor networks, where data replicas are stored along a curve and retrieved by consumers traveling along an intersecting curve, improving query locality, enabling structured aggregate queries, and enhancing robustness compared to traditional in-network storage methods like GHTs.
The paper proposes two distributed routing algorithms for underwater sensor networks—optimizing energy consumption while considering acoustic channel conditions—for delay-insensitive and delay-sensitive applications, validated through simulations.
The paper proves that Slepian-Wolf coding with optimal routing achieves minimum communication cost for correlated data gathering in wireless sensor networks, introduces distance entropy as a fundamental limit, and proposes a practical hierarchical scheme that is asymptotically optimal.
The paper presents fast and reliable estimation schemes for determining the number of RFID tags in a system with near-constant time complexity and high accuracy, outperforming prior identification-based approaches.
The paper proposes a low-cost, robust mechanical backhaul system using buses and cars to transport data between rural internet kiosks and gateways, addressing connectivity challenges in developing regions.
The paper proposes optimal 3D node placement strategies—truncated octahedron, hexagonal prism, and rhombic dodecahedron—for full coverage and connectivity in wireless networks, determining transmission-to-sensing range ratios and node efficiency.
The paper proposes physical-layer network coding (PNC), which leverages the additive nature of electromagnetic waves to boost wireless network capacity by performing coding operations at the physical layer rather than on digital bit streams.
The paper demonstrates that optimizing the ratio of transmit power to carrier sense threshold enhances spatial reuse and network capacity in multihop wireless networks, proposing a decentralized power and rate control algorithm that outperforms existing carrier sense tuning methods.
The paper clarifies that certain face and combined greedy-face routing variants (like GFG) guarantee message delivery in specific graphs (e.g., relative neighborhood and Gabriel graphs) by ensuring recovery from greedy failures without unnecessary face changes, while failures in arbitrary planar graphs arise from improper face selection mechanisms, such as those in GPSR.
The paper proposes **Optimal Unicast Routing Systems (OURS)**, introducing novel Nash equilibrium-based mechanisms to reduce overpayment and ensure cooperation in selfish wireless networks, while addressing scenarios where both relay nodes and service requestors act selfishly, with proven performance guarantees and economic efficiency.
The paper proposes an efficient method for generating concise explanations of significant changes in hierarchical data summaries by aggregating node weights along root-to-leaf paths, enabling optimal generalization and scalability.
The paper proposes a two-stage hierarchical sampling and modeling approach to accurately estimate rare click rates for webpage-ad pairs by correcting sampling bias and leveraging tree-structured correlations across multiple resolutions.
The paper proposes a scalable discrete latent factor model for predicting large-scale dyadic data by combining covariate effects and interpretable latent interactions, demonstrated through generalized linear models and efficient EM-based algorithms.
The paper proposes a scalable hidden Markov model approach for the generalized classification problem of identifying and locating significant substructures within strings in large datasets and data streams.
The paper proposes **Xproj**, a framework for clustering XML documents by leveraging substructural information to evaluate and guide clustering solutions, addressing challenges posed by the high implicit dimensionality of XML data.
This paper develops a hybrid text mining and econometrics method to analyze consumer reviews, quantifying the impact of product features on pricing, demand, and revenue using a tensor-based approach and demonstrating its effectiveness with Amazon sales data.
The paper evaluates and compares various graphical Granger causality methods for temporal causal modeling, demonstrating the superiority of Lasso over pairwise approaches and applying them to corporate performance data.
The paper proposes a method to extract semantic relations from query logs by analyzing user queries and click actions, representing queries in a vector space derived from a query-click bipartite graph, and demonstrates the relevance of these relations through experimental analysis and an application for detecting multitopical URLs.
The paper presents an online learning method for real-time ranking that adapts to concept drift using weighted majority techniques, outperforming batch-learning and non-adaptive online approaches on both real and synthetic data streams.
The paper proposes a hybrid recommender system combining local neighborhood-based and regional matrix factorization models to improve rating prediction accuracy, outperforming existing methods like Netflix's Cinematch.
The paper presents a scalable, stateless document routing and index partitioning method for similarity-based searches by distributing features across partitions and querying only a small subset, maintaining high precision and recall while improving efficiency.
The paper proposes a **support feature machine (SFM)**—a novel multidimensional time series classification method inspired by SVM and nearest neighbor rules—and demonstrates its effectiveness in detecting epileptic seizure precursors from EEG data with over **80% accuracy** in 10 patients.
The paper proposes **NAML**, an unsupervised nonlinear adaptive metric learning algorithm that jointly optimizes kernel learning, dimensionality reduction, and clustering via trace maximization in an EM framework to enhance data separability.
The paper proposes *D-Stream*, a density-based clustering framework for real-time stream data that dynamically adjusts clusters using grid mapping and density decay, efficiently handles outliers, and detects arbitrary-shaped clusters without requiring prior knowledge of cluster count or fixed time windows.
The paper proposes using PARAFAC2, a multi-way generalization of SVD, to improve cross-language information retrieval and document clustering by enforcing shared concepts across languages in a parallel corpus, outperforming traditional LSA methods.
The paper proposes two evolutionary spectral clustering frameworks that integrate temporal smoothness to balance current data fit with historical consistency, yielding stable, noise-resistant, and adaptive clustering results.
The paper proposes *community factorization*, a method to extract and analyze blog communities by modeling their structural interactions and temporal dynamics through constrained optimization, demonstrating its effectiveness on synthetic and real data.
The paper proposes a hybrid parametric-nonparametric model that jointly estimates house prices by combining intrinsic features and latent neighborhood desirability, trained via a deterministic EM algorithm, outperforming purely parametric or nonparametric approaches.
The paper proposes a probabilistic temporal framework combining temporal logic and stochastic limit theory to determine conditions under which temporal rules maintain their confidence and support for future data, linking rule support existence to the law of large numbers.
The paper introduces *AbraQ*, an automatic query expansion method that identifies underrepresented aspects in search results and enhances them with targeted keywords, significantly improving precision for hard queries compared to traditional and interactive expansion techniques.
The paper proposes an adaptive edit distance-based method for canonicalizing database records, learning cost parameters from annotated data to generate robust and user-preferred canonical representations.
The paper proposes a co-clustering based classification (CoCC) algorithm to improve text classification for out-of-domain documents by leveraging labeled in-domain data and propagating class structure through co-clustering, demonstrating superior performance over traditional methods despite distributional differences.
The paper proposes an unsupervised anomaly detection method for categorical datasets by comparing records against marginal distributions of attribute subsets, offering a more meaningful approach than standard likelihood models that often flag rare values as anomalies.
The paper introduces an unsupervised feature selection method for text classification with theoretical worst-case guarantees on generalization performance and demonstrates its empirical effectiveness compared to common strategies on benchmark datasets.
The paper proposes an efficient incremental constrained clustering method that updates clustering solutions with new user-provided constraints without full reclustering, identifies NP-hardness but solvable cases under specific conditions, and demonstrates its advantages over batch reclustering.
The paper presents a model-based co-clustering framework that simultaneously clusters and builds predictive models for complex bi-modal or multimodal data, improving interpretability and accuracy compared to traditional two-step approaches.
The paper proposes a learning framework using Green's function for label propagation in semi-supervised and unsupervised learning, derives it from kernel regularization, demonstrates its equivalence to spectral clustering methods, and applies it effectively to recommender systems.
The paper proposes the NeuroElectroMagnetic Ontologies (NEMO) framework for mining brainwave (ERP) ontologies through pattern decomposition, clustering, and association rule mining, demonstrating alignment with expert-defined patterns while refining ontology representation for cross-study integration.
The paper proposes and evaluates hybrid generative/discriminative methods for semi-supervised classification, demonstrating their effectiveness in handling overlapping classes and outperforming traditional approaches in 75% of text classification tasks.
The paper introduces algorithms to identify small, tightly connected groups ("tribes") with anomalous employment patterns in financial data, revealing that these groups exhibit higher fraud risk, homogeneity, and geographic mobility compared to others.
The paper proposes the Time Driven Documents-partition (TDD) algorithm to construct a feature-based event hierarchy from a text corpus by identifying bursty features, extracting time-relevant documents, and organizing them hierarchically, demonstrating its effectiveness through evaluation on news articles.
The paper introduces the Minimum Consistent Subset Cover (MCSC) problem, generalizing set covering to find the fewest consistent subsets satisfying a constraint, presents applications in data mining, and proposes a graph-based algorithm (CAG) that effectively solves MCSC instances.
The paper introduces **Constraint-Driven Clustering (CDC)**, a need-driven method that automatically determines the number of clusters based on user-provided constraints (minimum significance and variance) without requiring a predefined cluster count, and proposes an efficient **CD-Tree** algorithm to solve this NP-hard problem effectively.
The paper introduces **trajectory patterns** as an extension of sequential pattern mining to analyze frequent spatio-temporal behaviors of moving objects, formalizes the problem, evaluates different mining approaches, and compares their performance on real and synthetic datasets.
The paper introduces the Enhanced Max Margin Learning (EMML) framework, a more efficient and scalable approach for multimodal data mining in multimedia databases, demonstrating superior performance and faster convergence compared to existing methods.
The paper introduces algorithms for discovering low-entropy attribute subsets and tree-structured dependencies in binary data, leveraging monotonicity properties to enable efficient search, and evaluates their performance on synthetic and real datasets.
The paper introduces a dynamic hybrid clustering method combining text mining and citation analysis to analyze 7,401 bioinformatics publications (1981–2004), identifying evolving subfields and their cognitive structure over time.
The paper presents a novel topic detection method that measures the correlation between term distribution and citation graph connectivity, using a probabilistic topic score to identify relevant terms and their synergistic effects in document collections like arXiv and Citeseer.
The paper presents a novel summarization methodology that leverages duality to efficiently construct fixed-space synopses with deterministic error guarantees, significantly improving time and space complexity compared to existing techniques.
The paper introduces **Correlated Graph Search (CGS)**, an efficient algorithm for mining correlated subgraphs in graph databases using Pearson’s correlation coefficient, leveraging occurrence probability bounds and heuristic rules to reduce computational complexity.
The paper demonstrates that proper document representation, particularly through logarithmic opinion pooling and monotonic constraints on term weighting, makes outperforming Naive Bayes in high-precision text classification (e.g., spam detection) significantly more challenging.
The paper presents efficient algorithms for discovering frequent episodes in event streams using a non-overlapped occurrences-based frequency measure, achieving improved time and space complexities by a factor of *N* compared to existing methods.
The paper presents a scalable, near-optimal algorithm for cost-effective sensor placement in networks to detect outbreaks efficiently by leveraging submodularity, achieving significant speedups and performance guarantees in real-world applications like water distribution and blog monitoring.
The paper introduces a novel algorithm that efficiently mines statistically significant itemsets by leveraging equivalence classes—represented by closed patterns and generators—to rank patterns under various test statistics, outperforming existing methods in speed and applicability to biomedical and financial domains.
The paper proposes *very sparse stable random projections* using a mixture of symmetric α-Pareto and point mass distributions to significantly speed up and reduce storage for dimension reduction in \( l_\alpha \) norms while maintaining accuracy.
The paper introduces **BoostCluster**, a boosting framework that iteratively enhances any clustering algorithm's accuracy by dynamically adjusting data representations to align with given pairwise constraints.
The paper proposes CLIPER, an efficient algorithm for mining closed iterative patterns from software traces to aid program comprehension by uncovering repetitive behaviors within and across traces.
The paper proposes a probabilistic model for relational clustering that unifies various clustering tasks, including attribute-based, semi-supervised, co-clustering, and graph clustering, and introduces parametric hard and soft algorithms applicable to diverse relational data structures.
The paper introduces and formalizes nestedness and *k*-nestedness in binary datasets, proposes measures and algorithms for detecting these structures, and demonstrates their applicability in ecology and other domains using spectral partitioning methods.
The paper proposes probabilistic methods to automatically and objectively label multinomial topic models by optimizing Kullback-Leibler divergence and mutual information, demonstrating effectiveness through user studies on diverse text datasets.
The paper evaluates expertise modeling methods for matching reviewers to papers, finding that the Author-Persona-Topic (APT) model outperforms language-model-based approaches.
The paper introduces **Connected X Clusters (CXC)**, a joint clustering method for attribute and relationship data that automatically determines the number of clusters using a dynamic two-phase algorithm (**JointClust**) without requiring prior specification of cluster count.
The paper introduces the **Multiscale Topic Tomography Model (MTTM)**, a probabilistic graphical model that uses non-homogeneous Poisson processes and Haar wavelets to track topic evolution across multiple time resolutions, demonstrating effectiveness in topic analysis and comparable performance to LDA in perplexity tests.
The paper presents DL8, an exact algorithm that efficiently mines optimal decision trees from itemset lattices under various constraints, outperforming heuristic methods like C4.5 while avoiding overfitting.
The paper proposes an association analysis-based method using h-confidence to transform protein interaction networks by removing spurious edges and adding biologically valid ones, improving function prediction performance.
The paper proposes a collaborative filtering-based ranking method for movie search, combining personal item authorities and item proximities, and demonstrates its effectiveness through offline and online tests showing improved recall and result quality over existing systems.
The paper introduces **multiple topic tracking (MTT)** for **iScore**, an improved recommendation system that maintains multiple user interest profiles to enhance news article suggestions by adapting to evolving interests and outperforming traditional methods like Rocchio, achieving a **9% performance boost**.
The paper demonstrates that active exploration in learning document rankings from clickthrough data, using a Bayesian approach to select informative rankings, significantly outperforms passive observation and random exploration in improving ranking quality.
The paper introduces a hierarchical mixture model for text clustering, proving that documents can be efficiently classified to their most specialized topic within a subset of hierarchically related topics, with performance independent of the total topics, and validates the approach on real datasets.
The paper proposes an enhanced Parametric Mixture Model with a Dirichlet prior to better analyze multiple-topic documents by capturing topic-specific biases, outperforming the original model in classification and keyword extraction.
The paper proposes using hierarchical clustering to autonomously learn ontologies for recommendation systems, demonstrating improved performance over classical methods on MovieLens and Jester datasets.
The paper proposes two active learning methods—Label Efficient learning and margin-based learners—to reduce label requests and improve performance in one-sided feedback scenarios, outperforming previous Apple Tasting-based approaches.
The paper proposes a content-based method called *Information Genealogy* to track idea flow in non-hyperlinked document databases using language modeling and likelihood ratio tests, outperforming similarity-based approaches in detecting influence and inferring citation structures.
The paper introduces a concept-based model for text categorization that improves traditional term-frequency methods by analyzing term importance at both sentence and document levels, combining statistical and ontological weights to better capture semantic meaning and enhance categorization accuracy.
The paper proposes a cost-sensitive learning framework with Partial Example Acquisition (PAS), enabling dynamic example acquisition with partial attributes to minimize total data acquisition costs, validated by UCI dataset experiments.
The paper proposes a spectral clustering method that optimally combines numerical vectors and a modular network by introducing normalized network modularity and balancing the two data sources through a weight parameter, demonstrating effectiveness on synthetic and real-world datasets.
The paper introduces the shifted mixture model (SMM), a generative classifier that addresses selection bias in semi-supervised learning when labeled and unlabeled data are differently distributed, particularly under "missing not at random" (MNAR) conditions, and demonstrates its effectiveness over existing methods through an EM-based learning approach.
The paper introduces a distribution-free **density test** for detecting changes in multi-dimensional data, demonstrating superior power over existing methods.
The paper presents a framework for generating ranked evidence trails across documents to explain connections between concepts using concept chain graphs, graph matching, and graphical models, evaluated on a cross-document summarization dataset.
GraphScope is a parameter-free, streaming algorithm that detects communities and discontinuity time-points in large, evolving graphs using information-theoretic principles, demonstrating effectiveness across diverse real-world datasets.
The paper proposes that weighting rules instead of pruning them in the LERAD algorithm improves attack detection with minimal computational cost.
The paper introduces **SCREEN**, a semi-supervised clustering method using feature projection to improve performance on high-dimensional sparse data.
The paper proposes optimization-based frameworks and algorithms for identifying evolving communities in dynamic social networks, addressing NP-hard challenges with heuristic solutions validated on synthetic and real-world data.
The paper presents a scalable, modular convex solver for regularized risk minimization that handles various machine learning problems efficiently, supports parallelization, and outperforms specialized solvers in speed and scalability.
The paper introduces **G-Ray**, a scalable method for finding exact or approximate subgraph matches in large attributed graphs with linear time complexity, demonstrated effectively on a 356K-node DBLP graph.
The paper introduces a fast, direction-aware proximity measure for directed graphs based on escape probability, offering computational efficiency (up to 50,000x speedup) and effectiveness in graph mining tasks like clustering and link prediction.
The paper introduces **Scalable Look-ahead Linear Regression Trees (LLRT)**, a decision tree method that efficiently evaluates thousands of linear regression models per split to optimize predictive accuracy, particularly for data with strong attribute dependencies, through precomputation and optimizations.
The paper proposes a compression-based pattern dissimilarity measure to effectively identify and characterize differences between two databases, demonstrating its practical utility in real-world data mining applications.
The paper proposes a preliminary approach to ensure privacy preservation in gradient descent methods, demonstrating its feasibility in specific applications.
The paper proposes a probabilistic algorithm to discover correlated bursty topic patterns across coordinated text streams with different vocabularies, demonstrating its effectiveness on news and literature datasets.
The paper introduces a structured hidden variable model that generalizes principal component analysis to heterogeneous data types (e.g., text, authors, timestamps) by enabling probabilistic encodings and efficient latent space inference, demonstrated through author and recipient prediction tasks.
The paper introduces the problem of "finding favorable facets" to identify combinations of customer preferences on nominal attributes where a target point is not dominated by others, proposes minimal disqualifying conditions (MDCs) to summarize these facets, and develops efficient algorithms for on-the-fly computation and pre-computation of MDCs.
The paper proposes a method called Classification using lOcal clusterinG (COG) that improves rare-class prediction by clustering large classes into balanced sub-classes before applying traditional supervised learning, achieving higher accuracy than existing methods.
SCAN is a fast, efficient structural clustering algorithm that identifies clusters, hubs, and outliers in networks by measuring vertex similarity, outperforming modularity-based methods.
The paper proposes **Model-Shared Subspace Boosting (MSSBoost)**, an efficient multi-label classification algorithm that reduces redundancy by sharing and combining base models across labels, improving performance and speed compared to traditional methods.
The paper introduces a simpler, more robust algorithm for detecting time series motifs that is invariant to uniform scaling, overcoming limitations of previous methods while reducing parameter complexity.
The paper proposes a computationally efficient quadratically constrained quadratic programming (QCQP) approach for learning the kernel matrix in Regularized Kernel Discriminant Analysis (RKDA), extending it to multi-class problems and demonstrating its effectiveness on benchmark datasets.
The paper proposes a novel method for discovering semantically meaningful visual patterns in image databases by combining frequent itemset mining, self-supervised metric learning, and noise-resistant pattern summarization.
The paper develops a revised theory of information distance to address practical pitfalls and applies it to construct a question-answering system, supported by extensive experimental validation.
The paper proposes a domain-independent statistical method using a graph model to automatically mine templates (tags and texts) from search result records for accurate data extraction in metasearch engines, comparison-shopping, and Deep Web crawling.
The paper proposes a joint optimization approach for wrapper generation and template detection by using page similarity rather than URL patterns to improve extraction accuracy.
The paper proposes an integrated probabilistic model that jointly segments and labels webpage structure and text content by leveraging layout and tag-tree information to improve webpage understanding, demonstrating effectiveness in research homepage extraction tasks.
The paper presents an event-based framework for analyzing the evolution of interaction graphs over time, identifying behavioral patterns of individuals and communities to model evolution, predict links, maximize influence, and develop a diffusion model for dynamic networks.
The paper describes the development and testing of three on-board data mining techniques—from thresholding to reduced-set SVMs—for analyzing uncalibrated Mars Odyssey spacecraft data under resource constraints, leading to the discovery of a water ice annulus around Mars' north polar cap.
The paper proposes *iLink*, a learning-based framework for social search and message routing in peer production systems, demonstrating its application in FAQ generation and military knowledge sharing while connecting it to the theoretical SQM model.
The paper presents relational data pre-processing techniques—including consolidation, link formation, employment-based association inference, and normalization—to enhance securities fraud detection by transforming complex, dynamic datasets into a suitable format for statistical relational modeling.
The paper proposes a heuristic-based method to identify and clean disguised missing data—values that appear valid but are actually missing—without requiring domain knowledge, demonstrating effectiveness and efficiency on real datasets.
The paper provides a practical guide to conducting controlled online experiments (A/B tests) to optimize web development by prioritizing data-driven insights from user behavior over executive opinions, covering key methodologies, challenges, and best practices for reliable results.
The paper proposes an ensemble-based distributed classification method for P2P networks, combining local classifiers via a communication-efficient Distributed Plurality Voting (DPV) protocol that handles dynamic topologies and data updates while maintaining scalability and low communication overhead.
The paper proposes high-quantile modeling (e.g., 90th percentile) for customer wallet estimation and other applications, adapting nearest-neighbor and regression-tree methods to outperform traditional regression in estimating potential spending and detecting outliers.
The paper proposes a two-stage method—mining Local Correlation Network Patterns (LCNP) and using kernel-based classification—to predict power network instability and prevent blackouts by analyzing complex structures and system variables efficiently.
The paper presents a language-independent bootstrapping method that corroborates known facts and learns new ones from web pages by iteratively extracting fact mentions and discovering HTML patterns, demonstrating effectiveness in experiments on country facts and large-scale Wikipedia-derived data.
The paper presents an automated expense reimbursement system that uses a conditional random field (CRF) framework to extract relevant named entities from unstructured receipt images, reducing processing time and costs.
The paper proposes an efficient online framework for classifying and segmenting massive audio streams using micro-clustering algorithms, which outperforms traditional Gaussian Mixture Models in both speed and accuracy by avoiding overtraining.
The paper presents a case study on detecting changes in large payment card datasets by building and monitoring over 15,000 separate baseline models across a multi-dimensional data cube to address scalability challenges in heterogeneous data.
The paper proposes a semi-supervised learning approach using conditional random fields and domain constraints to improve mobile object tracking accuracy in sensor networks while reducing the need for extensive labeled training data.
The paper proposes *event summarization*, a novel approach to concisely interpret chaotic temporal event data in system management by decomposing it into independent subsets and fitting models to each for actionable insights.
"LungCAD is an FDA-approved, machine learning-based CAD system that significantly improves radiologists' accuracy in detecting solid pulmonary nodules in CT scans, as demonstrated in a large-scale clinical trial."
The paper introduces Prototype Ranking (PR), a machine learning method for selecting top-performing stocks by predicting their ranks using competitive learning, demonstrating superior returns and risk-adjusted performance compared to Coopers method in trading simulations from 1978-2004.
The paper presents **IMDS**, an intelligent malware detection system that uses **Objective-Oriented Association mining** on Windows API execution sequences to outperform traditional antivirus tools and other data mining-based approaches in accuracy and efficiency.
The paper proposes *TruthFinder*, an algorithm that identifies trustworthy websites and true facts by analyzing mutual reinforcement between site trustworthiness and information accuracy in conflicting web data.
The paper discusses challenges in analyzing large-scale social network data, focusing on inferring social processes and ensuring individual privacy, while highlighting limitations of anonymization techniques in protecting sensitive information.
The paper reflects on the successes, failures, and lessons learned in data mining over 18 years since its inception, aiming to guide future progress in the field.
The paper proposes and analyzes generalized MaxWeight-based scheduling algorithms for multi-carrier wireless systems (like WiMAX) that optimize performance by simultaneously allocating multiple carriers to users with known channel rates, addressing various optimization problems with provable bounds and numerical validation.
The paper proposes two localized channel assignment algorithms for multi-radio multi-channel wireless mesh networks using s-disjunct superimposed codes, ensuring interference-free communication, supporting unicast and broadcast, and achieving high throughput with low overhead under primary interference constraints.
The paper derives optimal channel probing and transmission scheduling strategies for opportunistic spectrum access, showing that the optimal policies have a threshold structure and can be described by an index policy, with a two-step lookahead strategy proven optimal in practical cases.
The paper demonstrates that a hybrid wireless sensor network with mostly static sensors and a small fraction of limited-mobility sensors achieves constant-density \( k \)-coverage independent of network size while bounding maximum sensor movement to \( O(\log^{3/4} L) \).
The paper proposes the Rendered Path (REP) protocol, a range-free localization method that achieves accurate and scalable sensor positioning in anisotropic networks with holes using only a constant number of seed nodes.
The paper introduces *local barrier coverage*—a practical, localized approach to detect movements within a subset of a sensor network's region—and presents the *Localized Barrier Coverage Protocol (LBCP)*, which significantly improves network lifetime while maintaining near-global coverage, outperforming existing methods by up to 6×.
The paper introduces new techniques to derive reliable density estimates for achieving coverage and connectivity in finite, thin strip deployment regions, addressing practical limitations of asymptotic results and providing accurate solutions for real-life wireless network applications.
The paper introduces **DARWIN**, a distributed and adaptive reputation mechanism for wireless ad-hoc networks that ensures robust cooperation by mitigating false perceptions of selfishness due to collisions and interference while being collusion-resistant.
The paper demonstrates that 802.11 devices can still be tracked using implicit identifiers in network traffic, even when pseudonyms and encryption are employed, undermining privacy protections.
The paper proposes a temporal link signature method to reliably detect changes in a transmitter's location, improving detection rates and reducing false alarms compared to existing techniques.
The paper proposes data-driven paging techniques using call and mobility data to reduce cellular network signaling load by up to 80% with minimal delay increase.
The paper proposes an adaptive joint network coding and scheduling framework to maximize wireless network throughput, demonstrating that greedy network coding can reduce performance and introducing XOR-Sym as a lower-complexity alternative to COPE.
The paper proposes a method to improve WLAN reliability by using physical-layer confidence values from multiple access points or retransmissions to cooperatively recover corrupted packets, reducing loss rates by up to 10x compared to existing approaches.
The paper proves that bounded domains in mobility models lead to exponentially distributed inter-meeting times in MANETs, while unbounded domains result in power-law distributions, providing insights for mobility model design and protocol performance analysis.
The paper presents a measurement-based model for accurately estimating wireless network throughput and goodput under interference from multiple heterogeneous nodes, improving upon existing models by handling arbitrary sender numbers, unicast transmissions, and node diversity.
The paper finds that inter-contact times between mobile devices follow a power law up to a characteristic time (~half a day) and decay exponentially beyond it, with simple mobility models replicating this behavior and human return times to favorite locations explaining the dichotomy, suggesting prior power-law-based performance analyses may be overly pessimistic.
The paper analyzes bus-to-bus contact patterns in the UMass DieselNet disruption-tolerant network, showing that route-level inter-contact times exhibit periodic behavior, and demonstrates that route-level mobility modeling improves routing performance predictions compared to coarse-grained aggregated models.
The paper demonstrates that multi-packet reception (MPR) improves the order capacity of wireless ad hoc networks by a factor of Θ(log n) and outperforms network coding (NC) for scalability, proposing new protocol designs to enable many-to-many communication for truly scalable networks.
The paper proposes using peer-to-peer networks over mobile Internet instead of VANETs to overcome adoption barriers for traffic information systems while maintaining decentralization and robustness.
The paper introduces Device-free Passive (DfP) localization, a system that detects, tracks, and identifies entities without requiring them to carry or actively participate in localization, using WiFi signal changes, and demonstrates its feasibility with high detection accuracy and low false positives.
The paper proposes STAR, an adaptive contact probing algorithm that dynamically adjusts probing intervals based on self-similar contact arrival patterns to minimize energy consumption while reducing missed contacts in delay-tolerant applications.
The paper presents a measurement-based model for predicting link capacity in 802.11 wireless networks by combining MAC-layer and PHY-layer behaviors, validated with high accuracy (within 10% error for 90% of cases) in a 12-node mesh network.
The paper proposes **MDG (Measurement-Driven Guidelines)**, a framework derived from large-scale testbed measurements to optimize 802.11 WLAN performance by selectively combining frequency allocation, load-balancing, and power-control schemes based on network conditions, achieving significant capacity improvements (22–274%).
The paper establishes tight asymptotic bounds on the multicast capacity of large-scale wireless ad hoc networks, showing that the total capacity is Θ(√(n/log n) · W/√k) when k = Ω(n/log n) and Θ(W) when k = Θ(n/log n), unifying prior results on unicast and broadcast while also extending the analysis to group-multicast and arbitrary networks.
The paper demonstrates that achieving Group Strategyproofness in incentive-compatible routing for wireless ad hoc networks is impossible, but proposes a scheme that converges to a Strong Nash Equilibrium while preventing profit transfer among colluding nodes through cryptographic methods.
The paper proposes **Weak State Routing (WSR)**, a scalable and dynamic routing mechanism for large networks that uses probabilistic hints and biased random walks to achieve high delivery rates (over 98%) with low overhead, trading off slightly longer paths for robustness and adaptability.
The paper revisits reliable routing in wireless mesh networks by proposing a new path metric called ETOP that accounts for finite link-layer retransmissions and the relative positions of lossy links, demonstrating its effectiveness through analytical computation and experimental results on a 25-node testbed.
The paper establishes a necessary and sufficient condition for useful localized network coding in wireless networks, proposes a robust "loop coding" technique, and demonstrates its effectiveness in improving both network and TCP throughput.
SecNav is a secure wireless navigation protocol that prevents a wide range of attacks on localization and time synchronization by leveraging devices' awareness of navigation station coverage, demonstrated as practical through an 802.11b implementation.
The paper explores optimal real-time scheduling in multi-rate wireless networks, analyzing trade-offs between transmission speed and packet error rates to provide theoretical insights and heuristic design guidelines for latency-sensitive applications.
The paper proposes **Context-Aware Spectrum Agility (CASA)**, which dynamically adjusts spectrum sensing parameters using application hints and channel conditions, improving throughput by 35% over conventional SA while better meeting application demands.
The paper proposes "altruistic cooperation," a strategy using specialized nodes (altruists) to share channel usage information, enabling energy savings of up to 70% without throughput loss while requiring minimal additional node deployment.
The paper introduces LEGO, a localized, self-reconfiguration algorithm for multi-radio wireless mesh networks that autonomously detects and recovers from link failures while optimizing channel efficiency and QoS, achieving up to 92% improvement in real-world tests.
The paper proposes practical MAC protocols leveraging superposition coding to significantly enhance throughput in dense wireless mesh networks, demonstrating gains of 10–154% in various scenarios.
The paper proposes "authentication on the edge," a distributed certificate-based authentication method for global open Wi-Fi networks, using segmented revocation lists and a social lookup network to enhance scalability and resilience.
The paper proposes a TRACE approach to analyze wireless user behavior by clustering location-preference data from university WLAN logs, identifying distinct behavioral groups following a power-law distribution, which can inform network management and behavior-aware applications.
The paper proposes speculative scheduling as a pragmatic solution to mitigate interference in enterprise WLANs by addressing the challenges of centralizing the data plane while managing wireless medium variability and ensuring bounded delay and jitter.
The paper proposes a cross-layer "code tree" system for detecting and mitigating jamming attacks in wireless broadcast networks by enabling transmitter-receiver cooperation, reducing required transmission codes while improving resilience compared to single-code approaches.
The paper demonstrates that in unplanned WiFi deployments, cumulative throughput is primarily determined by the number of interfering access points rather than the number of clients, as TCP flow control limits backlogged stations to twice the number of access points, increasing collisions and reducing performance.
The paper introduces CHARM, a low-overhead channel-aware rate adaptation algorithm that leverages channel reciprocity to outperform probe-based methods in dynamic and static channel conditions.
The paper proposes VERITAS, a strategy-proof and computationally efficient spectrum auction mechanism that maximizes spectrum utilization while supporting diverse bidding formats and market objectives.
The paper proposes an efficient periodic in-band spectrum sensing algorithm that optimizes sensing-frequency and sensing-time by minimizing overhead while meeting detectability requirements, dynamically selecting between energy or feature detection based on SNR and deriving an RSS threshold for preference.
The paper introduces a **complex temporal signature** method for wireless location distinction, combining channel gains and impulse response to outperform existing techniques while reducing false alarms from temporal variations.
The paper introduces Wide-Area Wireless Networks (WANETs), which use Software Defined Access Points (SoDA) to create a globally distributed single-hop or multi-hop wireless network by mixing RF signals over the Internet, and discusses the resulting network models and research challenges.
The paper *BreadCrumbs* proposes a system that predicts mobile connectivity changes by learning a user's movement patterns and past network observations, enabling optimized network usage with improved performance and reduced power consumption.
The paper proposes a user-centric prediction scheme that uses historical colocation data to optimize media sharing among commuters in urban transport by identifying reliable content sources based on movement regularity, improving communication efficiency over memoryless approaches.
The paper presents *Thedu*, a system that enhances interactive web applications in hybrid networks by using aggressive prefetching and opportunistic mobile-to-mobile transfers to mitigate WiFi disruptions, showing significant improvements in web page delivery for vehicular users but limited benefits from m2m routing in dense AP deployments.
The paper examines cost-performance trade-offs of infrastructure enhancements (base stations, meshes, relays) in mobile networks, finding that meshes and relays can be more cost-effective than base stations and that even minimal infrastructure outperforms mobile-to-mobile routing schemes.
GAnGS is a secure, user-friendly protocol for authenticating public keys among physically collocated groups, resisting insider and bystander attacks while being robust to user errors.
The paper proposes using clock skews derived from IEEE 802.11 TSF timestamps to detect unauthorized wireless access points quickly and accurately, overcoming MAC address spoofing limitations through linear programming, least square fit, and a heuristic for distinguishing fake APs, demonstrating robustness across various conditions.
The paper presents PARADIS, a method for identifying wireless network interface cards (NICs) with over 99% accuracy by analyzing unique hardware-induced signal imperfections in IEEE 802.11 frames.
The paper presents a protocol that enables two users to generate a shared cryptographic key by exploiting the unique and rapidly decorrelating properties of the wireless channel, achieving practical key establishment rates without requiring an authenticated channel.
The paper establishes that, under a Gaussian channel model with random node placement and multicast sessions, each session can achieve a capacity of at least \( c_8 \sqrt{n} / (n_s \sqrt{k}) \) with high probability, provided \( k \leq \Theta_1 n / (\log n)^{2\alpha+6} \) and \( n_s \geq \Theta_2 n^{1/2+\beta} \), generalizing prior unicast results.
The paper presents **ACES**, an adaptive clock synchronization algorithm using Kalman filtering to efficiently estimate and adjust clock offset and skew in resource-constrained networks while maintaining high accuracy.
The paper proposes efficient scheduling algorithms for WiMAX relay networks that exploit multiuser diversity and channel frequency selectivity, achieving near-optimal performance with low computational complexity while enhancing throughput and extending coverage.
The paper analyzes the critical phase transition time in wireless multi-hop networks under random node failures, showing that the last connection time and first partition time scale as \(\log(\log n)\) for exponential node lifetimes and \((\log n)^{1/\rho}\) for Pareto lifetimes, with these times serving as bounds for network resilience.
The paper presents a measurement-efficient framework for accurately predicting the well-served areas of urban wireless mesh networks by combining sector-based estimation, terrain-aware propagation modeling, and refinement techniques, reducing required measurements by 2–5x compared to grid sampling while maintaining high accuracy.
"Cabernet improves vehicular WiFi content delivery by introducing QuickWiFi for faster connection setup (under 400 ms) and CTP for enhanced throughput (2x over TCP), achieving an average of 38 MB/hour per vehicle in real-world taxi deployments."
The paper experimentally analyzes CDMA 1xEV-DO network performance, revealing significant wireless channel variability over long timescales but high predictability over short timescales, and shows that loss-based TCP variants achieve over 80% of system capacity despite channel fluctuations due to large buffers.
The paper empirically analyzes EPC Class-1 Generation-2 UHF RFID system performance, identifying physical layer factors that degrade reliability and demonstrating that optimized reader configurations and better physical-MAC layer integration can significantly improve read rates.
The paper demonstrates that CSMA/CA networks suffer from severe fairness issues, proposes a novel rate control protocol called PISD (Proportional Increase Synchronized multiplicative Decrease) that achieves fairness without relying on overhearing, and proves its convergence and effectiveness.
"Horizon is a practical wireless mesh network system that optimizes multi-path TCP performance by balancing resource utilization and fairness using a modified back-pressure approach, compatible with unmodified TCP and 802.11 MAC, while minimizing packet reordering."
JazzyMac is an adaptive, high-performance TDMA-based MAC protocol for long-distance multihop wireless networks that improves throughput and delay by dynamically adjusting transmission slot lengths based on local traffic demands and works efficiently across arbitrary topologies.
The paper proposes a framework for automating the design of high-performance networking protocols through optimization-based waveform generation and protocol extraction, aiming to accelerate development and improve performance beyond human-designed solutions.
Ditto is a system that opportunistically caches overheard data in wireless mesh networks using content-based naming to improve throughput and reduce gateway load, achieving up to 7x performance gains over on-path caching and 10x over no caching.
The paper proposes two congestion control protocols, WCP and WCPCap, for multi-hop wireless mesh networks to address fairness and efficiency issues caused by interference, demonstrating their effectiveness through analysis, simulations, and real deployments.
The paper presents the first incentive-compatible opportunistic routing protocols for wireless mesh networks, ensuring honest user participation while improving throughput by 5.8%-58.0% compared to non-incentive protocols.
The paper presents a cross-layer framework for evaluating modulation rate adaptation protocols, demonstrating that trained SNR-based methods outperform loss-based approaches in urban and vehicular environments by better tracking channel conditions and handling heterogeneous links.
The paper proposes an Automatic Code Embedding (ACE) wireless link-layer protocol that jointly ensures reliability and stability, improving throughput and performance for various traffic types without requiring modifications to higher or lower layers.
The paper proposes and demonstrates a practical interference cancellation technique for unmanaged wireless networks, significantly reducing packet loss and improving spatial reuse by enabling successful reception of overlapping transmissions from unsynchronized sources.
**Summary:** ZipTx is a software-only solution that improves 802.11 network throughput by efficiently utilizing correct bits in corrupted packets without requiring hardware modifications, adapting to dynamic modulation and coding schemes.
The KDD-2009 panel report discusses the role of open standards like PMML in data mining, emphasizing interoperability, business benefits, and the impact of cloud computing on predictive analytics.
The paper proposes a regression-based latent factor model for predicting dyadic data responses by estimating row and column factors through feature regressions, offering a unified solution for cold and warm start scenarios with scalable fitting methods and demonstrated performance improvements.
The paper extends various frequent pattern mining algorithms to uncertain data, demonstrating that hyper-structure and candidate generate-and-test methods outperform tree-based approaches, contrary to deterministic settings, and validates these findings on real and synthetic datasets.
The paper introduces a structured probabilistic topic model for automatically retrieving and summarizing biological information from captioned figures in scientific literature, using an efficient inference algorithm based on Gibbs sampling.
The paper presents a name-ethnicity classifier using hidden Markov models and decision trees trained on public data to categorize names into 13 ethnic groups, achieving accuracy comparable to binary classifiers, and applies it to analyze trends in a large news corpus.
The paper proposes **Information-theoretic Meta-clustering (ITMC)**, a model-based anomaly detection method that identifies unique temporal segments with divergent dynamics by leveraging lossy compression principles, achieving high precision and recall in detecting irregularities in noisy stochastic processes.
The paper presents a collusion-resistant, efficient anonymous data collection protocol using cryptographic and shuffling techniques to ensure participant anonymity unless all N-1 participants are compromised, while reducing communication overhead by 15-42% compared to prior methods.
The paper proposes a viewpoint-based approach to analyze dynamic interaction graphs by focusing on the neighborhood and evolution of selected nodes to identify patterns, critical events, and structural transformations.
The paper presents an algorithm to optimize website traffic by scheduling media content to balance freshness and peak-time engagement, maximizing clicks based on traffic patterns and declining clickthrough rates.
The paper proposes the **Clustering Agreement Process (CAP)**, a method that improves clustering stability and quality by refining multiple input clusterings into more consistent outputs using a bipartite combinatorial Markov Random Field, achieving state-of-the-art results on four datasets.
The paper proposes an interactive framework for analyzing workflow process logs by incorporating temporal information through Temporally-Annotated Sequences (TAS) and presenting condensed execution patterns via a Temporally-Annotated Graph (TAG) to enable user-driven exploration of alternative process executions.
The paper introduces a probabilistic framework for mining frequent itemsets in uncertain databases, defining itemset frequency based on possible world semantics and providing an efficient solution that significantly outperforms naive approaches.
The Offset Tree algorithm optimally reduces partial-label learning to binary classification, achieving at most \((k-1)\) times the regret of the underlying binary classifier while maintaining computational efficiency.
The paper proposes a new framework for analyzing concept drift in data streams and introduces two ensemble methods, ADWIN Bagging and ASHT Bagging, which outperform existing approaches in handling evolving data streams.
CoCo is a parameter-free outlier detection method that identifies outliers based on coding cost (data compression principles) using a general data model, avoiding distributional assumptions and providing interpretable results.
The paper proposes a framework for efficient real-time anomaly detection in trajectory streams using local clustering and a piecewise metric index to optimize processing speed and space usage.
The paper introduces a probabilistic topic model that analyzes text corpora to infer and describe relationships between entities, enabling the automatic annotation and prediction of connections in large networks like Wikipedia.
The paper proposes a domain adaptation method for text mining that minimizes distribution differences between source and target domains using a shared concept space encoded by linear transformation while optimizing empirical loss on labeled source data and handling multiple classes simultaneously.
The paper proposes a constrained optimization approach for conditional random field (CRF) training that incorporates cross-validation accuracy as constraints to improve generalization, using a novel algorithm based on extended saddle point theory to handle discontinuities.
The paper proposes improved greedy algorithms and new degree discount heuristics for efficient influence maximization in social networks, demonstrating that fine-tuned heuristics achieve near-optimal influence spread with significantly faster runtime compared to greedy approaches.
The paper presents a scalable Hadoop MapReduce-based solution for large-scale behavioral targeting that achieves a 20% CTR lift by efficiently processing fine-grained user data with optimized parallelism, in-place feature generation, and reduced disk I/O.
The paper investigates the compressibility of social networks compared to Web graphs, proposing combinatorial models and heuristics that leverage network properties like reciprocity, and demonstrates significant differences in their compression characteristics.
The paper introduces *NoRegret KLRank*, an online ranking algorithm that uses clickthrough feedback to improve rankings over time, guarantees convergence to the best fixed hindsight policy even as new items are added, and demonstrates strong empirical performance, including a greedy variant that outperforms existing methods.
The paper introduces a generalized Co-HITS algorithm that integrates content and link information from both sides of a bipartite graph with relevance constraints, demonstrating its effectiveness through iterative and regularization frameworks, with applications in query suggestion showing improved performance.
The paper introduces novel methods for ranking predictions by their accuracy in dyadic data through localized models and co-clustering, focusing on reliable predictions via "certainty lift" and robust modeling of coherent data regions.
The paper introduces **IEThresh**, a strategy for active learning that efficiently learns the accuracy of multiple labeling sources by estimating confidence intervals for their reliability, selectively querying the most accurate experts to minimize labeling effort while maintaining high accuracy.
The paper discovers surprising clique patterns and edge weight power-laws in large, weighted, time-evolving human communication networks and proposes a utility-driven graph generator to replicate these observed patterns.
The paper proposes a Bayesian method to detect bacterial cultures using optical-scattering technology when the training dataset is non-exhaustive by simulating missing classes with a common prior, outperforming existing approaches in experiments with 28 bacterial subclasses.
The paper presents a submodular optimization approach for selecting blog posts to cover important stories efficiently, introduces a personalized online learning framework to adapt to user preferences, and demonstrates its effectiveness in reducing information overload through empirical evaluation and user studies.
The paper proposes a feature reshaping method to enhance linear SVM classifiers by ensuring features are linearly correlated with the target variable, improving performance on text classification and UCI datasets.
The paper proposes a Voronoi-based neighborhood definition and parallelized rule-learning method for spatial classification within a multi-relational framework, demonstrating improved performance on real-world crime data.
The paper introduces a scalable hybrid random field model and the Markov Blanket Merging algorithm for efficient pseudo-likelihood estimation in high-dimensional datasets, demonstrating superior accuracy and computational efficiency compared to existing methods.
The paper proposes a framework using prequential error with forgetting mechanisms (sliding windows or fading factors) for reliable evaluation of evolving stream learning algorithms in non-stationary environments.
The paper proposes an efficient heuristic method for consensus learning among heterogeneous supervised and unsupervised models by constructing a belief graph to propagate and negotiate predictions, improving accuracy and clustering quality by up to 10% while maintaining scalability.
The paper introduces **multi-focal learning**, a method where training data is partitioned into distinct focal groups to improve model accuracy, and demonstrates its effectiveness in customer service problem classification by enhancing SVM performance.
The paper proposes a Dual Regularized Co-Clustering (DRCC) method that incorporates manifold structures of both data and features via graph regularization within a semi-nonnegative matrix tri-factorization framework, demonstrating superior performance over existing clustering methods.
The paper empirically analyzes user content generation patterns in three knowledge-sharing online social networks, finding that posting behavior follows daily/weekly cycles and stretched exponential distributions, with core users not dominating contributions, and high-quality content having smaller stretch factors.
The paper proposes randomization methods using Metropolis sampling to iteratively generate datasets preserving discovered patterns (e.g., clusters or correlations), enabling assessment of whether new findings are independent or derived from prior results.
The paper proposes a novel document clustering method that enhances traditional "bag of words" representations by incorporating Wikipedia concepts and categories, significantly improving clustering performance on three datasets.
The paper proposes *TrustWalker*, a random walk model that combines trust-based and item-based recommendations to improve accuracy and handle cold-start users while measuring recommendation confidence.
The paper presents a method for automating the annotation of *Drosophila* gene expression images using sparse feature representation and term-term interaction modeling to improve accuracy over traditional bag-of-words approaches.
The paper introduces *Cartesian contour*, a concise representation for frequent itemsets that provides a complete and generative view of their interactions, proves its NP-hardness via a connection to biclique set cover, and develops efficient approximation and heuristic methods for its construction.
The paper demonstrates that email class-label noise is content-specific, shows genre modeling effectively identifies mislabeling and improves spam-filtering accuracy, outperforming benchmarks on trec05-p1 and ceas-2008 datasets.
The paper introduces **RDB-KRIMP**, an algorithm that combines global and local relational data mining by selecting a small set of detailed relational patterns using the MDL principle, enabling concise yet comprehensive database summarization.
The paper proposes a time-sensitive collaborative filtering model that tracks evolving customer preferences and product popularity to improve recommendation accuracy, demonstrating superior performance on the Netflix dataset.
The paper proposes a collective disambiguation approach for linking text spans to Wikipedia entities by balancing local compatibility and global coherence, demonstrating superior performance over existing methods through various optimization techniques.
The paper introduces the **TEAM FORMATION** problem, which involves selecting a subset of skilled individuals from a social network to perform a task while minimizing communication costs, proves its NP-hardness, and presents novel algorithms with practical validation on the DBLP dataset.
The paper proposes a parameter-free, linear-time method to model term burstiness using discrepancy theory and introduces a burstiness-driven search framework for efficient retrieval in timestamped document collections.
The paper proposes projective sampling, a method that estimates optimal training set size by projecting learning and run-time curves from a small data subset, demonstrating its cost-effectiveness over progressive sampling in data mining.
The paper develops a meme-tracking framework to analyze the news cycle, revealing a 2.5-hour lag between media and blog attention peaks and persistent temporal patterns in online content spread.
**Summary:** DynaMMo is a method for mining and summarizing coevolving time sequences with missing values by discovering latent variables and their dynamics, offering compression, feature extraction, and linear complexity.
The paper argues that directly comparing privacy and utility in data anonymization is inappropriate, proposes a framework inspired by financial risk-return tradeoffs to analyze their relationship, and provides guidelines for balancing privacy and utility in data publishing.
The paper proposes **MetaFac**, a framework that uses **relational hypergraph factorization** to discover **time-evolving, multi-relational communities** in social media, demonstrating superior performance over baseline methods in scalability and prediction tasks.
The paper proposes the Bayesian Browsing Model (BBM), a scalable, efficient, and exact-inference method for estimating URL relevance from large-scale click logs, demonstrating superior performance (29.2% higher log-likelihood) and speed (57x faster) compared to state-of-the-art models while processing petabyte-scale data in just 3 hours.
The paper proposes **Lassplore**, an efficient algorithm for large-scale sparse logistic regression using an adaptive line search scheme within Nesterov's method to optimize step size while maintaining optimal convergence.
The paper proposes a discriminative pattern mining approach to classify software behaviors by extracting and selecting features from execution traces, training a classifier to detect failures, and demonstrates its effectiveness with a 24.68% accuracy improvement over baselines.
The paper proposes a stable feature selection framework that identifies consensus feature groups from subsampled data to improve stability and generalization performance in high-dimensional, small-sample settings.
The paper introduces grouped graphical Granger modeling methods, including a novel group boosting approach, to improve causal inference in time series data by leveraging variable group structure and handling non-linearity, demonstrating superior performance over standard methods in simulations and gene regulatory network analysis.
The paper proposes a hybrid active learning method that combines graph-based community detection and centrality measures with empirical risk minimization (ERM) to efficiently select high-quality instances for labeling in networked data, reducing computational costs while maintaining performance.
The paper proposes a cascading non-homogeneous Poisson process model to characterize individual e-mail communication patterns, demonstrating its generalizability across datasets and its utility in classifying users into persistent behavioral types.
The paper introduces a large-scale graph mining method using backbone refinement classes to efficiently mine significant tree-shaped subgraph descriptors, reducing feature set size and running times while maintaining classification accuracy compared to traditional approaches.
The paper explores adapting leading Netflix Prize recommendation algorithms to ensure differential privacy while maintaining competitive accuracy by carefully calibrating noise and post-processing steps.
The paper proposes *WhereNext*, a location prediction method that uses *Trajectory Patterns*—frequent movement sequences—to build a *T-pattern Tree* for predicting a moving object's next location based on spatio-temporal behavior patterns.
The paper presents a constraint programming approach for correlated itemset mining in ROC space, introducing an improved bound, an iterative pruning algorithm, and an adaptation for mining convex hull itemsets, significantly outperforming existing methods in efficiency and scalability.
The paper introduces **TANGENT**, a parameter-free, efficient graph-based recommendation algorithm that suggests surprising yet related items by balancing proximity to a user's preferences and connections to unrelated choices.
The paper proposes two efficient large-scale one-class collaborative filtering algorithms that incorporate weighting for missing data, addressing sparsity and scalability while demonstrating effectiveness on datasets like Netflix Prize.
The paper proposes a novel association analysis framework called RAP for exhaustively mining real-valued biclusters (range support patterns) with reduced information loss compared to discretization-based methods, demonstrating superior performance in identifying biologically enriched gene patterns in microarray data compared to standard biclustering algorithms.
The paper proposes *CP-summary*, a concise representation of frequent itemsets using conditional profiles (*c-profiles*) that enables independent analysis, guarantees error bounds, avoids false positives/negatives, and supports browsing all frequent itemsets, along with an efficient algorithm for generating it.
The paper proposes an efficient and exact method for mining proportional fault-tolerant frequent itemsets by addressing non-anti-monotonicity and support computation challenges through provable bounds and exact counting.
This paper proposes privacy-friendly methods for extracting quasi-social networks from browsing behavior to target brand advertising audiences effectively, demonstrating that these networks enhance brand affinity without compromising user privacy.
The paper proposes a principled and flexible constrained optimization framework for finding alternative clusterings that allows users to specify desired properties of the original clustering to retain or modify.
The paper proposes **RTF (ranking with tensor factorization)**, a tensor factorization-based tag recommendation method that directly optimizes personalized ranking by handling missing values and pairwise constraints, outperforming state-of-the-art approaches in both accuracy and efficiency.
The paper presents a scalable multi-level graph clustering algorithm using stochastic flows, which improves efficiency and quality by coarsening the graph, simulating flows on coarse levels, refining iteratively, and clustering high-flow regions, outperforming state-of-the-art methods.
The paper examines how preprocessing choices, network forces (selection and influence), attribute drift, and historical data strategies impact dynamic network modeling, while validating key assumptions and proposing alignment metrics.
The paper proposes a rank-one binary matrix factorization method reformulated as a 0-1 integer linear program (ILP) and its efficient LP relaxation, which achieves a guaranteed error bound and is solvable via maximum flow algorithms.
The paper proposes the SSLIP method for detecting anomalous contiguous data windows along intersecting linear paths using scan statistics, likelihood ratios, and Monte Carlo simulations, demonstrating effectiveness in identifying traffic accident hotspots on highways.
The paper analyzes user grouping behavior in online forums, finding regularities in community joining patterns and demonstrating that weak reply-based relationships exhibit diffusion patterns similar to stronger ties, while proposing a Bipartite Markov Random Field model to evaluate predictive features, revealing that user similarity alone is insufficient but combining it with node-level features improves model accuracy.
The paper proposes a method to quantify causality between symbolic or numerical time series in bits, enabling effective structuring and modeling of multivariate time series even with limited data.
The paper introduces **NetClus**, an iterative ranking-based clustering algorithm for heterogeneous information networks with star network schemas, which outperforms PLSA and RankClus by leveraging multi-typed object interactions to produce high-quality clusters and informative rankings.
The paper proposes **Topical Affinity Propagation (TAP)**, a distributed model for analyzing and quantifying topic-level social influence in large networks, with applications in identifying representative nodes and measuring neighbor influence.
The paper proposes a relational learning method that extracts latent social dimensions from network data to capture diverse affiliations and uses them as features for discriminative learning, outperforming collective inference methods, particularly with limited labeled data.
The paper proposes two constant-factor approximation algorithms for identifying dynamic communities in social networks, with one handling fully observed actors and the other addressing partially observed cases, both demonstrating efficiency and effectiveness on real datasets.
The paper "DOULION: counting triangles in massive graphs with a coin" introduces a practical and highly accurate method that significantly speeds up triangle counting in graphs, achieving up to ≈130× faster performance while maintaining over 99% accuracy.
The paper introduces a hierarchical mean shift approach for rare category detection that identifies anomalies at different scales without requiring prior knowledge of dataset properties, outperforming existing methods.
The paper introduces indexable fault signatures that capture temporal and topological event patterns in networks, presents an efficient learning algorithm to extract these signatures from noisy data, and demonstrates their effectiveness for online fault diagnosis through space-time indexing and experimental validation.
The paper proposes a framework for extracting broad latent query aspects from search session logs to improve query reformulation, demonstrating significant gains in accuracy, click-through rates, and consistency over baseline methods.
This paper systematically analyzes 16 external validation measures for K-means clustering, discussing their normalization, properties, interrelationships, and equivalence, while providing guidelines for selecting the most suitable measures in practice.
The paper presents a likelihood ratio test (LRT) framework for efficiently detecting anomalous rectangular regions in spatial data on an n x n grid by pruning non-significant regions to accelerate computation.
The paper proposes and evaluates quantification-based, semi-supervised, and hybrid methods to adapt classifiers to changing class distributions when only unlabeled data is available, showing significant improvements in accuracy and F-measure.
The paper introduces a fast approximate spectral clustering framework (KASP and RASP) that significantly reduces computational complexity while maintaining high accuracy, enabling efficient clustering of large datasets.
The paper proposes a novel multi-label active learning approach for text classification that reduces labeling effort by selecting unlabeled data that maximally decreases expected model loss, using SVM and label prediction to achieve high accuracy with fewer labeled examples.
The paper proposes a discriminative model that combines link and content analysis for community detection by introducing hidden variables for node popularity and filtering irrelevant content attributes, outperforming existing generative approaches.
The paper evaluates efficient topic inference methods for streaming documents, introducing a fast classification-based approach and SparseLDA, a Gibbs sampling algorithm that is 20× faster than traditional LDA while using less memory.
The paper introduces *time series shapelets*—interpretable, representative subsequences that enable faster, more accurate classification than nearest neighbor methods while providing insights into the data.
The paper proposes a graph-based algorithm that leverages social tags to improve web object classification by utilizing semantic features and inter-object connections, demonstrating superior performance over existing methods.
The paper proposes a personalized email prioritization (PEP) method that leverages social network analysis and semi-supervised learning to extract social features from email interactions, significantly improving prioritization accuracy over baseline approaches.
The paper presents a dynamic graph-based relational mining approach that learns structural patterns and transformation rules in evolving biological networks by analyzing sequences of graphs and their rewrite rules.
The paper extends Affinity Propagation to a streaming framework (StrAP) by reducing its complexity and incorporating change detection, enabling real-time clustering of job flows in grids for improved failure analysis.
The paper proposes a parallel community detection algorithm using propinquity dynamics, which iteratively refines community structures by balancing network topology and node proximity, enabling efficient large-scale processing and identification of overlapping communities.
The paper proposes a novel generative model that captures the co-evolution of social and affiliation networks, explaining both link and group formation while revealing that users often join groups independently of their friends, and it successfully replicates real-world network properties.
The paper introduces semi-supervised boosting algorithms that use information-theoretic regularization to leverage both labeled and unlabeled data, achieving superior performance over supervised and existing semi-supervised boosting methods.
The paper proposes a kernel-based transfer learning method that maps source and target domain data into a common kernel space and aligns their distributions, achieving superior performance over existing approaches, particularly in text categorization.
The paper introduces **ClickRank**, a scalable algorithm that leverages user browsing sessions to estimate page and site importance, improving web search ranking and other applications with better performance and lower computational costs.
The paper introduces an \( l_1 \)-norm regularized max-margin Markov network (\( l_1 \)-M\(^3\)N) that achieves both primal and dual sparsity, proposes three learning methods (projected sub-gradient, cutting-plane, and an EM-style algorithm), and demonstrates its effectiveness in feature selection and predictive performance across synthetic and real datasets.
The paper introduces a novel distance measure and algorithms to enhance the generalized Hough transform, enabling efficient data mining of large petroglyph collections.
The paper analyzes user behavior in sponsored search results compared to organic results, identifies patterns to predict on-site actions, and develops a predictive classifier and generative model using query, ad, site, and user properties based on large-scale toolbar data.
The paper proposes a managed analytics service framework for CRM that enhances efficiency and repeatability by equipping business analysts with specialized tools to bridge technical and business expertise gaps.
The paper evaluates four syntactic similarity algorithms—three based on Broder's "shingling" technique and one using "content-based chunking"—identifying optimal parameter settings for practical performance and validating their effectiveness in detecting near-duplicates in enterprise document collections.
The paper presents a scalable tensor segmentation method for large-scale conjoint analysis on Yahoo! Front Page clickstream data, enabling efficient user preference segmentation and content optimization.
The paper "Seven pitfalls to avoid when running controlled experiments on the web" identifies common challenges in online A/B testing, such as misapplying statistical formulas, ignoring bots, and encountering Simpson’s paradox, based on Microsoft's extensive experimentation experience.
The paper presents a highly efficient, parallel dataflow implementation of weighted co-clustering for collaborative filtering on the Netflix dataset, achieving real-time predictions (9.7 µs per rating) with competitive accuracy (RMSE 0.88846) while leveraging pervasive parallelism on commodity hardware.
The paper proposes pattern-based entity discovery and comparative-sentence-mining-based entity assignment methods to identify and infer products discussed in opinion mining, demonstrating effectiveness in forum posts and commercial applications.
The paper proposes a novel trajectory mining algorithm to discover frequent multi-year migration patterns ("migration motifs") of individual stocks in financial markets, using graph-based approximate pattern matching and spatiotemporal constraints, validating existing theories and suggesting new financial models.
The paper demonstrates that automatically extracted training data from toolbar and click logs can significantly improve classification accuracy for inferring commercial intent in search queries at a lower cost compared to manually labeled data.
The paper proposes a latent semantic association (LaSA) model for address standardization that reduces reliance on labeled data and handcrafted rules by leveraging unlabeled text to capture domain-specific word relationships, improving performance with minimal training effort.
The paper presents an efficient online learning algorithm, Amnesiac Averaged Perceptron, that leverages clickthrough data to identify high-quality broad matches for keywords in online advertising, improving relevance, monetization, and advertiser reach.
The paper introduces COA (Claim Originality Analysis), a patent ranking tool that evaluates patent value by analyzing the recency and impact of key phrases in patent claims, demonstrating its effectiveness through comparisons with other metrics and real-world testing in IBM's IP department.
The paper proposes an unsupervised network anomaly detection method using eigen equation compression and incremental anomalousness scoring to identify and localize significant network-wide anomalies while ignoring local ones.
**Summary:** OpinionMiner is a machine learning-based system using lexicalized HMMs to extract and classify product opinions from web reviews as positive or negative, improving upon rule-based and statistical methods.
The paper proposes *Hydra*, a hybrid subspace clustering algorithm that organizes object-level search results by leveraging diverse similarity notions from web data and feature-based subspace measures to improve user experience in product search.
The paper proposes a grocery shopping recommender system using a basket-sensitive random walk model to improve recommendations by focusing on product similarity, incomplete neighborhood information, online basket adaptation, and infrequently purchased items, outperforming existing collaborative filtering methods.
The paper proposes a dynamic temporal graph model using hidden Markov regression and lasso-type algorithms to infer evolving dependency structures in time series data, improving interpretability and prediction accuracy in systems like oil-production monitoring.
The paper proposes a unified Conditional Random Fields model to simultaneously perform web classification and information extraction, improving accuracy in both tasks compared to traditional decoupled approaches.
The paper proposes a machine learning approach to detect malicious websites by analyzing lexical and host-based features in URLs, achieving 95-99% accuracy with low false positives.
The paper introduces IPLoM, a novel iterative partitioning algorithm for clustering event logs, which outperforms existing methods by achieving significantly higher accuracy (78% F-Measure) without relying on frequent pattern mining.
The paper introduces **SNARE (Social Network Analysis for Risk Evaluation)**, a scalable link analytic system that improves fraud detection and graph labeling by combining domain knowledge with belief propagation, achieving high accuracy and efficiency.
The paper presents a unified framework for sentiment analysis that combines lexical knowledge with text classification, outperforming methods that use either approach alone.
The paper proposes *LKC-privacy*, a new anonymization model and algorithm, to address privacy challenges in sharing blood transfusion data while preserving data utility and scalability.
The paper proposes a multi-label statistical clustering approach using simulated annealing to automatically classify service provider forms from business directories, enabling the creation of a universal web marketplace covering thousands of sectors and hundreds of providers per sector.
The paper presents a temporal data mining approach to optimize data center chiller performance by transforming sensor data into symbolic representations, identifying frequent operational patterns, and evaluating their sustainability impact, with promising experimental results from an HP-managed data center.
The paper introduces *BGP-lens*, an automated tool that identifies patterns (e.g., self-similarity, power-law distributions) and anomalies in BGP routing updates, offering effectiveness, scalability, and admin-friendly insights over a two-year dataset.
The paper proposes large-scale learning methods to predict bounce rates in sponsored search ads by analyzing ad creatives, keywords, and landing pages, demonstrating that bounce rate is an effective measure of user satisfaction.
The paper proposes a novel block coordinate descent algorithm for sparse inverse covariance estimation to analyze brain region connectivity differences in Alzheimer's disease, Mild Cognitive Impairment, and Normal Controls using FDG-PET imaging data.
The paper proposes a template-independent machine learning approach for news article extraction that learns from a single training site, achieving high accuracy across multiple sites while preserving visual elements like text, images, and animations.
The paper introduces *pSkip*, a metric derived from clickthrough data to estimate web search ranking quality by measuring the probability of users skipping non-relevant results, demonstrating its effectiveness in capturing aspects of search quality that existing metrics miss while acknowledging its limitations due to non-relevance-related system factors.
The paper proposes **Weakly Supervised Latent Dirichlet Allocation (WS-LDA)**, a topic modeling approach that leverages click-through data and partial human supervision to accurately mine and disambiguate named entities for applications like web search and recommendation systems.
The paper proposes a list-wise incremental crawling strategy for web forums that leverages site-level knowledge (via sitemap reconstruction) and thread-based post-arrival prediction, achieving 260% faster new-content retrieval and high coverage compared to state-of-the-art methods.
The paper presents an intelligent file scoring system (IFSS) that uses an ensemble of heterogeneous classifiers to detect malware in the gray list, improving detection efficiency and accuracy compared to existing tools like NOD32 and Kaspersky.
The paper presents an OLAP system for analyzing search logs to support data-driven applications like query suggestion and keyword bidding, demonstrating its effectiveness with real-world data.
The paper proposes design rules, architecture, and dynamic spectrum allocation algorithms for WiFi-like unlicensed access in DTV whitespaces, ensuring FCC compliance, efficient spectrum utilization, and superior performance compared to traditional ISM bands.
The paper presents a large-scale spectrum measurement study in South China, analyzing usage patterns and correlations across frequencies and locations, and proposes a 2D frequent pattern mining algorithm to predict channel availability based on observed exponential-like vacancy distributions and spectral-spatial correlations.
The paper proposes a quorum-based channel hopping (QCH) system for establishing robust and efficient control channels in dynamic spectrum access networks, offering optimal time-to-rendezvous and rendezvous convergence, with both synchronous and asynchronous variants.
The paper proposes an optimal sequential channel sensing and probing strategy for cognitive radio networks that maximizes throughput by considering both channel availability and quality, using optimal stopping theory to balance exploration and exploitation under practical constraints like sensing errors and overhead.
The paper presents **SAM**, a cross-layer system that enables efficient spatial multiple access in WLANs by using chain-decoding for asynchronous transmissions and a new MAC protocol (CCMA), achieving a 70% uplink throughput improvement over 802.11.
The paper demonstrates that transmission reordering in wireless networks, enabled by the Message in Message (MIM) capability, improves throughput and proposes a link-layer protocol to achieve optimal performance gains.
The paper presents PRO, an opportunistic retransmission protocol for IEEE 802.11 WLANs that leverages overhearing relays with strong connectivity to improve throughput, particularly under channel contention, fading, and mobility, while maintaining compatibility with legacy devices.
The paper introduces **Esense**, a communication method enabling devices with different or identical physical layers to exchange data by sensing and interpreting energy profiles, demonstrated through cross-standard communication between IEEE 802.11 and 802.15.4 devices using a constructed alphabet of signature packet sizes.
The paper demonstrates that large-scale CSMA wireless networks can achieve the same optimal throughput scaling order of Ω(1/√n) as centralized TDMA networks by using a dual carrier-sensing and dual channel scheme, efficient routing, and careful parameter tuning.
The paper proposes a "scheduling partition" method that decomposes large wireless networks into smaller zones with localized scheduling, proving that this approach achieves the same order of capacity as global scheduling while requiring only partition sizes of Θ(√log n) or Θ(1) for optimal scaling in random and arbitrary node placements, respectively.
The paper demonstrates that random walk mobility with non-trivial velocity enhances connectivity in k-hop clustered wireless networks, reducing energy consumption and improving power-delay trade-offs compared to stationary networks.
The paper introduces **Opportunistic Flooding**, a delay-efficient and energy-saving flooding protocol for low-duty-cycle wireless sensor networks with unreliable links, leveraging probabilistic forwarding based on next-hop delay distributions to minimize redundancy and improve performance.
The paper proposes a compressive data gathering scheme for large-scale wireless sensor networks that reduces communication costs, balances load, extends network lifetime, and handles abnormal readings effectively, validated through analysis, simulations, and real-world testing.
The paper demonstrates that data fusion in wireless sensor networks significantly improves sensing coverage compared to traditional disc models, with the required sensor density scaling as \( \rho_f = O(\rho_d^{1-1/k}) \), where \( k \) is the signal path loss exponent.
The paper presents a distributed, polynomial-time algorithm for maximizing sensor network lifetime by activating energy-efficient sensor covers, achieving a near-optimal lifetime guarantee of \(O(\log n \cdot \log nB)\) while operating without location or directional information.
The paper analyzes and optimizes neighbor discovery algorithms in wireless networks, showing that an ALOHA-like approach reduces to the coupon collector's problem without collision detection, achieves logarithmic improvements with feedback, and remains efficient even without synchronization or prior knowledge of neighbor counts.
The paper introduces FARA, a frequency-aware rate adaptation and MAC protocol that leverages per-frequency SNR measurements to independently adjust bitrates and allocate optimal frequencies for sender-receiver pairs, achieving a 3.1x throughput improvement over conventional systems.
The paper proposes optimal and greedy algorithms for efficient multicast beam scheduling in wireless networks using composite beams, achieving significant performance gains over omni-directional and prior directional schemes.
The paper proposes a scalable two-tier hybrid wireless/wired interconnect architecture called WCube for chip multiprocessors (CMPs), utilizing sub-terahertz wireless communication to reduce latency and power consumption compared to traditional wired designs.
The paper presents *Glia*, a software solution that enables effective use of multiple orthogonal Wi-Fi channels simultaneously to achieve near-optimal high data rates (close to 600Mbps), addressing performance limitations observed in conventional multi-channel Wi-Fi setups.
The paper investigates the link layer behavior of 2.4 GHz Body Area Networks (BANs) through extensive real-world measurements, revealing that environmental factors significantly impact packet delivery, same-side routing is preferable, and errors often occur in short bursts, providing key insights for BAN design.
The paper outlines the design challenges and research directions for ultra-low-power, energy-harvesting networked tags (EnHANTs), which enable novel tracking applications by attaching to everyday objects while addressing cross-layer energy efficiency and communication constraints.
"SurroundSense uses mobile phone sensors to create ambience fingerprints from sound, light, color, and motion, achieving 87% accuracy in logical localization across 51 stores."
The paper proposes a fine-grained I/O access control mechanism in Xen-based mobile virtualization to prevent CPU overuse, performance degradation, and battery drain caused by malicious guest domains through precise CPU usage accounting and scheduling.
The paper establishes that for VANET data aggregation to be scalable, the bandwidth of information about an area at distance \( d \) must decrease asymptotically faster than \( 1/d^2 \), and proves this bound is tight by demonstrating a scheme achieving \( 1/d^{(2+\epsilon)} \).
The paper proposes **CENTAUR**, a hybrid data path for enterprise WLANs that combines distributed DCF with centralized scheduling to mitigate hidden terminals, exploit exposed terminals, and improve performance without requiring client modifications.
The paper *OpenLIDS: a lightweight intrusion detection system for wireless mesh networks* demonstrates that resource-constrained mesh devices can effectively detect malware using lightweight anomaly detection instead of unreliable deep packet inspection.
The paper evaluates secret key extraction from wireless signal strength (RSS) in real environments, finding that dynamic scenarios yield high-entropy keys while static ones are vulnerable, and proposes an adaptive scheme that outperforms existing methods in bit rate and entropy while passing NIST randomness tests.
The paper introduces **Snap-Inducing Shaped Residuals (SISR)**, a robust localization method that mitigates errors in wireless node positioning by automatically downweighting unreliable nodes and links while accurately snapping well-measured nodes to their true locations.
The paper introduces *CacheCloak*, a system that enhances location privacy by using a trusted server to anonymize real-time location data through mobility predictions and cached queries, preventing tracking by untrusted services while maintaining accuracy and availability.
The paper presents a scalable, automated pipeline using doubly robust estimation and nonparametric testing to evaluate online ad campaign effectiveness from observational data without randomized experiments.
The paper describes Google's overlapping experiment infrastructure, tools, and processes for enabling more, better, and faster experimentation to improve user experiences in search engines and web applications.
The paper proposes two adaptive strategies to balance exploitation and exploration in contextual advertising, demonstrating their superior performance in ad reach and CTR through offline simulations with real-world data.
The paper describes **MineFleet**, a commercially successful distributed vehicle performance data mining system that processes onboard data streams, transmits analytics wirelessly, and provides fleet managers with actionable insights through user interfaces.
The paper presents a multiple kernel learning approach for detecting heterogeneous anomalies in aviation safety data by integrating discrete and continuous flight parameters, demonstrating improved detection of operationally significant events compared to existing methods.
The paper presents a system that analyzes large volumes of social and mainstream media data to detect and highlight emerging trends related to products or companies, improving signal-to-noise ratio for brand managers by identifying significant shifts in public discourse.
The paper presents a machine learning system that predicts and prevents health insurance claim processing errors, reducing rework and saving costs by improving precision over existing methods.
The paper proposes a constrained reinforcement learning approach to optimize debt collection processes, demonstrated through a real-world deployment at the New York State Department of Taxation and Finance.
The paper proposes a coupled Hidden Markov Model (HMM) approach to detect abnormal group-based manipulative trading behaviors by analyzing interactions among multiple trading sequences (buy-orders, sell-orders, and trades), outperforming traditional single-sequence HMM methods.
The paper proposes an Automatic Malware Categorization System (AMCS) that uses a cluster ensemble framework to group malware samples into families based on instruction frequency and function-based sequences, incorporating domain knowledge and hybrid clustering algorithms for improved accuracy and efficiency.
The paper presents a machine learning approach to classify vulnerabilities and predict their likelihood and timing of exploitation, outperforming traditional heuristics by analyzing features from vulnerability disclosure reports.
The paper presents a graph mining approach to detect memory leaks by analyzing heap dump dominator trees, identifying leak candidates and their access paths more effectively than existing tools like Eclipse MAT.
The paper presents a web-based Business Continuity Information Network (BCIN) system that uses data mining techniques—including information extraction, content recommendation, report summarization, and spatial clustering—to improve real-time disaster information sharing and collaboration between public and private sectors, with a prototype tested in Miami-Dade County.
The paper proposes a framework for similarity search of tropical cyclone events using LCSS parameter learning driven by dimensionality reduction and metric learning to compare arbitrary-length spatio-temporal sequences, demonstrating its feasibility and robustness through synthetic and real data experiments.
The paper introduces **MalStone**, a benchmark designed to evaluate cloud computing middleware performance for data-intensive analytics, along with **MalGen**, a tool for generating benchmark data.
TIARA is an interactive visual text analytics system that combines topic modeling, time-sensitive keyword extraction, and visualization to help users explore and summarize large text collections.
The paper introduces *MetricForensics*, a scalable multi-level framework for analyzing volatile graphs by progressively applying sophisticated metrics at finer temporal resolutions to efficiently detect interesting events, demonstrated on real-world datasets including network traffic and sensor data.
The paper presents a novel active learning strategy for biomedical citation screening that incorporates expert-provided domain knowledge and ranked features, outperforming existing methods while proposing a tailored evaluation framework and utility measure for real-world deployment.
The paper proposes an integrated machine learning approach for stroke prediction, combining a novel automatic feature selection algorithm with Support Vector Machines and a margin-based censored regression method, outperforming traditional Cox models in AUC and concordance index while identifying new risk factors.
The paper introduces a multi-label large-margin classifier that learns inter-code relationships in medical coding, improving accuracy by leveraging shared information and outperforming existing models on a clinical dataset.
The paper proposes a time-constrained probabilistic factor graph model (TPFG) to efficiently and accurately mine hidden advisor-advisee relationships from research publication networks, achieving 80-90% accuracy and improving expert search performance.
The paper proposes LMMH, a scalable log-linear model for estimating rare event rates in high-dimensional hierarchical categorical data, leveraging multi-resolution correlations and a spike-and-slab prior for variable selection, demonstrating superior accuracy and scalability on large-scale advertising datasets.
The paper demonstrates that changes in click-through rates (CTR) for sponsored search results are influenced not only by shifts in user examination probability but also by query-specific relevance, proposes three new browsing models accounting for these factors, and finds that attributing CTR changes solely to relevance outperforms examination-based models in predicting both CTR and relevance.
The paper introduces an algorithm that suggests friends by analyzing users' implicit social graphs—derived from interaction patterns rather than explicit connections—and demonstrates its effectiveness through experiments and real-world applications in Gmail Labs.
This paper introduces a supervised learning framework for link prediction in sparse networks, addressing key factors like observational period, imbalance, and sampling, and achieves over 30% higher AUC than unsupervised methods.
The paper proposes **UP-Growth**, an efficient algorithm for mining high-utility itemsets using a **UP-Tree** structure and pruning techniques to reduce candidate generation, significantly improving performance over existing methods, especially for long transactions.
The paper introduces *regular itemsets*—a concise yet interpretable representation of frequent itemsets—and proposes the *RegularMine* algorithm to efficiently mine them while maintaining semantic clarity comparable to closed itemsets.
The paper proposes efficient bottom-up and top-down algorithms for mining frequent patterns and association rules from probabilistic databases under Possible World Semantics, validated through extensive experiments.
The paper proposes a memory-efficient algorithm for mining the top-*k* most frequent items in a data stream under the max-frequency measure with dynamically changing sliding windows, addressing scalability issues while maintaining accuracy under reasonable conditions.
The paper proposes a Bayesian statistical approach to measure itemset quality by scoring them based on their probability in random exponential models, enabling efficient discovery of high-quality itemsets while mitigating pattern explosion.
The paper introduces **Grafting-Light**, a fast and efficient algorithm for feature selection and structure learning in Markov random fields by iteratively performing one-step gradient descent and feature selection, improving upon the slower Grafting method while maintaining global convergence guarantees.
The paper proposes a scalable two-stage approach for solving a class of dimensionality reduction techniques formulated as generalized eigenvalue problems, rigorously establishing its equivalence to the original formulation (even with regularization) and demonstrating its linear scalability with sample size and dimensionality.
The paper proposes an Efficient Fused Lasso Algorithm (EFLA) with a novel Subgradient Finding Algorithm (SFA) to solve fused Lasso problems more efficiently than existing methods, demonstrating superior performance and scalability.
The paper proposes **Multi-Cluster Feature Selection (MCFS)**, an unsupervised feature selection method that preserves multi-cluster data structures by solving a sparse eigen-problem and an L1-regularized least squares problem.
The paper proposes a probabilistic prediction-based wrapper feature selection method for SVR, demonstrating superior performance, especially on sparse datasets, through approximations of feature importance.
The paper proposes a versatile publishing scheme using Guardian Normal Form (GNF) to anonymize data by decomposing it into sub-tables that satisfy customizable privacy rules while preserving utility, with two algorithms (GD and UAD) for efficient decomposition.
The paper proposes a secure privacy-preserving outsourcing method for SVM training using random transformation to protect sensitive data while maintaining low computational and communication overhead.
This paper investigates the likelihood of inferring individuals' interests from their social connections in a workplace setting using a privacy-preserving distributed sensor system, combining network analysis and regression models to predict inference quality, showing significant improvements in both implicit and explicit interest detection.
The paper introduces **DUST**, a generalized similarity measure for uncertain time series that accounts for uncertainty while reducing to Euclidean distance when uncertainty is negligible, and validates its effectiveness in classification, motif search, and nearest-neighbor queries.
The paper introduces *cold start link prediction*, a method to predict social network structure when only node information (e.g., interest groups) is available, using a two-phase approach involving a *bootstrap probabilistic graph* and graph-based measures, validated on Flickr data.
The paper introduces **CISVM**, a cost-sensitive support vector machine that uses cost intervals instead of precise misclassification costs, and demonstrates its superiority over traditional methods, along with **CODIS**, a general approach incorporating cost distribution to further enhance performance.
The paper introduces a modular, open-source data generation environment using a graphical data flow tool to create customizable numeric and categorical data generators, demonstrated through a complex customer/product database example.
The paper demonstrates that under extreme class imbalance, guided search for training examples outperforms active learning for classifier induction, with hybrid strategies offering benefits when cost and skew make the choice between methods equivocal.
The paper proposes a relaxed order-preserving submatrix (ROPSM) model and an efficient mining method called ROPSM-Growth to detect biologically significant patterns in noisy gene expression data, outperforming the AOPC model in capturing noise-corrupted patterns and identifying more high-quality patterns.
The paper introduces "topic dynamics," a physics-inspired model that redefines bursts in topic streams using kinetic concepts like momentum and acceleration, offering a hierarchical and expressive alternative to traditional rate-based burst detection methods, and demonstrates its effectiveness on the PubMed/MEDLINE database.
The paper proposes a method for extracting interpretable temporal signatures from complex biological network models by formulating feature selection in rank-order space and demonstrates its effectiveness on yeast cell cycle data.
The paper introduces an efficient algorithm for mining collaborating negative correlations between sets of variables by transforming data into a bipartite graph and mining transpose closures, demonstrating its effectiveness on Yeast gene expression data.
The paper proposes *k-support anonymity* using a pseudo taxonomy tree to protect sensitive items in outsourced frequent itemset mining by generalizing items with similar supports, ensuring privacy while maintaining data utility with moderate overhead.
The paper proposes a collusion-resistant, fully private, and efficient method for secure multiparty computation of summations in privacy-preserving data mining, ensuring no sensitive information is revealed even if all other parties collude.
The paper explores how to effectively integrate differential privacy into data mining, particularly for decision tree induction, showing that naive approaches yield poor results while optimized methods maintain accuracy and privacy with significantly fewer samples.
The paper presents two differentially private algorithms for accurately discovering and releasing the top-k most frequent patterns in sensitive data while ensuring privacy and utility.
The paper introduces **RWDISK**, a disk-based graph clustering and nearest-neighbor search method that improves efficiency by treating high-degree nodes as sinks and reduces page-faults while enhancing link prediction accuracy and clustering quality compared to existing approaches.
The paper presents scalable combinatorial algorithms for balanced bipartite allocation using min-cost flow for L₁ penalties and convex cost flow for general convex penalties, enabling constant-time edge allocation approximations and demonstrating practical efficiency on large real-world graphs.
The paper presents a novel Eulerian data structure for compressing social networks that enables efficient sublinear-time neighbor queries (both in-neighbor and out-neighbor) without decompression, supported by theoretical bounds and empirical validation.
The paper proposes a GPU-accelerated, iterative aggregation-based method for efficient and parallel SimRank computation on large static and dynamic graphs.
The paper proposes and refines a mathematical model to describe the structure and dynamics of online conversations across Usenet, Yahoo! Groups, and Twitter, examining how identities influence communication patterns.
The paper proposes a flexible, principled framework for constrained spectral clustering that explicitly encodes weighted Must-Link and Cannot-Link constraints while preserving the original graph Laplacian, guarantees constraint satisfaction via a user-specified threshold, and is solvable deterministically in polynomial time.
The paper proposes a hierarchical information-theoretic algorithm to discover nonlinear alternative clusterings by maximizing mutual information with data while minimizing information sharing with a reference clustering, demonstrating superior performance in nonlinear and competitive results in linear scenarios.
The paper proposes **Sync**, a clustering method inspired by synchronization dynamics, where data objects are treated as phase oscillators that synchronize over time to form clusters, enabling automatic detection of arbitrary cluster shapes and sizes while naturally handling outliers.
The paper presents a unified optimization framework for dependent and disparate clustering of non-homogeneous data, allowing smooth constraint satisfaction or violation by maximizing or minimizing the same objective function, and demonstrates its effectiveness on synthetic and real-world datasets.
The paper presents a fast dual-tree algorithm for computing the Euclidean Minimum Spanning Tree (EMST) with rigorous runtime guarantees, demonstrating its scalability on large astronomical datasets.
The paper proposes a three-step algorithm to automatically mine concurrent program workflows from interleaved event traces by analyzing temporal dependencies, constructing a basic model, and refining it for minimal state transitions and threads.
The paper presents an efficient algorithm for automatically generating coherent chains of news articles to reveal hidden connections between two given stories, incorporating user feedback for refinement and demonstrating effectiveness through real-world evaluation.
The paper introduces a measure called φ-frequent probability to mine frequent subgraphs in uncertain graph databases, proposes an efficient approximation algorithm with probabilistic guarantees, and validates its effectiveness through experiments.
The paper proposes a new boosting algorithm that incorporates structural relationships among base learners in the functional space, using a combination of \( L_1 \) and Laplacian-based \( L_2 \) penalties to enforce sparsity and smoothness, and demonstrates its effectiveness for graph classification.
The paper proposes a **Discriminative Topic Model (DTM)** that enhances manifold learning by both bringing neighboring pairs closer and separating non-neighboring pairs, improving classification accuracy and robustness compared to existing methods.
The paper proposes an online multiscale dynamic topic model that captures evolving word distributions across multiple timescales, enabling efficient sequential inference without storing past data.
The paper proposes a novel topic model using the Pitman-Yor process to capture power-law word distributions and multiple topics, outperforming Latent Dirichlet Allocation (LDA) in document modeling.
The paper proposes the **Topic-Perspective Model**, a probabilistic generative model that separates tag generation (influenced by document topics and user perspectives) from content term generation (influenced only by topics), improving tag prediction and generalization over prior models.
The paper demonstrates that linearly combining diverse collaborative filtering algorithms through ensemble blending improves recommendation accuracy on the Netflix Prize dataset, outperforming individual methods.
The paper proposes FOBFM, a fast online learning method for time-sensitive recommendation that initializes item-specific factors using offline features and dimensionality-reducing linear projections to improve convergence speed and performance.
The paper demonstrates that leveraging missing rating data improves recommender system accuracy and introduces unbiased performance measures and training methods for data missing not at random (MNAR).
The paper proposes a Session-based Temporal Graph (STG) model and Injected Preference Fusion (IPF) algorithm to fuse long- and short-term user preferences for temporal recommendation, achieving 15%-34% improvement over state-of-the-art methods.
The paper introduces the Optimized Network Model (ONM), a generative model that uses ticket content and routing history to predict optimal routing paths, reducing resolution time by minimizing unnecessary ticket transfers among expert groups.
The paper proposes EnLR, a novel ensemble classifier that dynamically adjusts weights for individual discriminative classifiers based on their confidence (inverse variance) for each test instance, achieving statistically significant accuracy improvements over single classifiers and traditional ensemble methods like Bagging and AdaBoost while maintaining efficiency and scalability.
The paper introduces a technique that mines both positive and negative patterns in text documents to improve relevance feature discovery, outperforming traditional term-based and pattern-based methods in precision, recall, and F-measure on benchmark datasets.
The paper proposes **DPMFS**, a Dirichlet process mixture model with feature selection, to automatically determine the number of document clusters and identify discriminative words, demonstrating robustness and effectiveness in experiments.
The paper introduces novel tree kernels for semantic relation extraction using typed dependency parse trees, demonstrating significant performance improvements over existing state-of-the-art kernels on a benchmark dataset.
The paper proposes a probabilistic rating regression model to analyze latent aspect ratings in review texts, enabling detailed opinion analysis and supporting tasks like aspect summarization and entity ranking.
The paper proposes **gSSC**, a semi-supervised feature selection method for graph classification that efficiently identifies optimal subgraph features using both labeled and unlabeled data via a branch-and-bound algorithm and a novel evaluation criterion (**gSemi**), improving classification performance while pruning the search space.
The paper proposes a latent class generative model for dyadic relational events, with EM and Gibbs sampling inference methods, and validates it on email, political, and animal interaction datasets.
The paper introduces *community outliers*—anomalous nodes in information networks that deviate from their local communities—and proposes an efficient detection method using a hidden Markov random field (HMRF) model to jointly analyze data and links, demonstrating its effectiveness on synthetic and real-world datasets.
The paper proposes a constraint-based clustering method (CPPC) that redefines class labels by integrating expert-provided probabilistic constraints and data separability, demonstrating improved land cover classification accuracy.
The paper proposes a block minimization framework for training linear classifiers on data too large to fit in memory, implementing it for primal and dual SVMs with demonstrated effectiveness on datasets 20 times larger than memory capacity.
The paper introduces class-specific error bounds for ensemble classifiers that account for unequal error costs in binary classification, providing guidelines for optimizing performance across different regions of the ROC curve based on the strength and correlation of base classifiers.
The paper proposes a method to optimize a stochastic soft cascade of classifiers, maximizing accuracy while controlling feature acquisition costs, and demonstrates its effectiveness on clinical datasets.
The paper proposes **uHARMONY**, a novel algorithm that directly mines discriminative patterns from uncertain data for classification, avoiding costly feature selection and achieving 4–10% higher accuracy than state-of-the-art methods when used with SVM.
The paper proposes **EPIC**, an ensemble pruning method that selects the most valuable classifiers based on their accuracy and diversity contributions, achieving better performance with smaller subensembles compared to the original ensemble and a state-of-the-art pruning approach.
The paper proposes efficient approximation methods (fingerprinting, particle filtering, and truncation) to speed up Path-Constrained Random Walk (PCRW) queries by 2–100× with minimal accuracy loss in recommendation and retrieval tasks.
The paper proposes the Trust Antecedent Factor (TAF) Model, a probabilistic generative model that jointly infers trust/distrust relationships and ratings in online systems, validated through Collapsed Gibbs Sampling and real-world data.
The paper proposes an energy-efficient mobile recommender system that uses location traces to recommend optimal pick-up or parking sequences for taxi drivers, employing a Potential Travel Distance (PTD) function and algorithms (LCP and SkyRoute) to maximize business success and reduce energy consumption.
The paper presents a Bayesian mixture model framework for identifying and learning low-dimensional roles that influence high-dimensional data attributes, with applications like retail transactions, using MCMC algorithms for inference.
The paper proposes *mobility-based clustering*, a novel non-density-based method that uses moving vehicles as sensors to identify urban traffic hot spots by analyzing their instant mobility, overcoming limitations of traditional density-based approaches and achieving accurate results with minimal sample sizes.
The paper proposes PET, a statistical model that tracks popular events in social communities by jointly modeling the interplay between evolving textual topics and network structures using a Gibbs Random Field and topic model, demonstrating effectiveness on Twitter and DBLP datasets.
The paper introduces the *community-search problem*, which involves finding a densely connected subgraph containing a given set of query nodes in a graph, proposes an optimal greedy algorithm for a minimum-degree-based density measure, extends it to handle monotone constraints, and presents efficient heuristics for bounded-size communities, validated through experiments on real datasets.
The paper presents a relational clustering approach to construct deeper, more accurate folksonomies by integrating sparse and noisy structured metadata from personal hierarchies in social platforms like Flickr.
The paper proposes a Bayesian probabilistic model for personalized tag prediction that integrates ego-centric effects, environmental influences, and webpage content, improving F-measure by over 30% compared to existing methods in real-world scenarios.
BioSnowball is an automated bootstrapping framework that uses Markov Logic Networks to jointly perform biography ranking and fact extraction from the web, generating Wikipedia-style pages for individuals with minimal initial seeds and outperforming decoupled methods.
The paper introduces Combined Regression and Ranking (CRR), an efficient method that simultaneously optimizes regression and ranking objectives, achieving competitive or improved performance on both tasks, especially in cases of rare events or skewed distributions.
The paper introduces mass estimation as an efficient, theoretically grounded data mining technique with constant time and space complexities, demonstrating superior or comparable performance to eight state-of-the-art methods in information retrieval, regression, and anomaly detection.
The paper proposes a Bayesian network-based approach to efficiently model label dependencies in multi-label learning, decomposing the problem into single-label classifications with parental labels as features, and demonstrates its effectiveness through extensive experiments.
The paper introduces **DivRank**, a reinforced random walk algorithm that optimizes both prestige and diversity in network-based rankings, outperforming existing methods in empirical evaluations.
The paper presents a method to infer hidden diffusion networks and trace influence paths by analyzing node infection times, using an efficient approximation algorithm to handle large datasets, and applies it to reveal core-periphery structures in online media information flow.
The paper proposes a scalable heuristic algorithm for influence maximization in large social networks, balancing runtime and influence spread, outperforming existing methods in scalability and performance.
The paper proposes a Community-based Greedy algorithm that efficiently identifies top-K influential nodes in mobile social networks by detecting communities and using dynamic programming, significantly improving speed while maintaining accuracy compared to existing methods.
The paper proposes a **Noise Tolerant Time-varying Factor Graph Model (NTT-FGM)** to track and predict social actions by modeling user attributes, network structure, and action history, demonstrating improved accuracy over baselines in real-world datasets.
The paper introduces the **k-Effectors** problem, which involves identifying a set of *k* active nodes in a network that best explain observed activations under an information-propagation model, proves its NP-hardness and inapproximability for arbitrary graphs, provides polynomial-time solutions for special cases, and validates the approach experimentally on the DBLP co-authorship graph.
The paper proposes a Generalized Local Statistical (GLS) framework for spatial outlier detection (SOD), addressing limitations in existing local-based methods by analyzing statistical properties, incorporating robust estimation, and demonstrating superior performance on data with linear or nonlinear trends.
The paper proposes **evolutionary hierarchical Dirichlet processes (EvoHDP)**, a method for discovering cluster evolution patterns in multiple correlated time-varying text corpora by incorporating time dependencies and using cascaded Gibbs sampling.
The paper introduces the first online algorithm for real-time discovery and maintenance of time series motifs over streaming data, offering linear-time updates and extensibility to multidimensional motifs, with applications in robotics, acoustics, and compression.
The paper *Mining periodic behaviors for moving objects* proposes *Periodica*, a two-stage algorithm that detects multiple interleaving periods using observation spots and Fourier transform with autocorrelation, then models periodic behaviors probabilistically through hierarchical clustering.
The paper proposes a two-phase causal discovery algorithm using partial correlation-based conditional independence tests to efficiently learn Bayesian networks from continuous data with arbitrary distributions, improving upon the limitations of the existing TPDA method.
The paper analyzes Fisher's Linear Discriminant (FLD) classifier on randomly projected data, showing that the required projection dimension grows logarithmically with the number of classes while avoiding strict distance preservation assumptions and improving generalization with more training data.
The paper proposes a scalable kernel-based hashing algorithm that generates efficient, low-redundancy hash codes for diverse data formats (vectors, graphs, sequences, etc.) while preserving similarity and outperforming state-of-the-art methods.
The paper proposes a semi-supervised sparse metric learning method that leverages unlabeled data and optimizes sparsity using an alternating linearization approach, improving performance over existing methods.
The paper proposes a unified, modular, and scalable iterative framework for solving various multi-dimensional scaling (MDS) problems with guaranteed convergence and high accuracy, while also extending the Johnson-Lindenstrauss Lemma to spherical embeddings.
The paper proposes an unsupervised transfer classification framework using a generalized maximum entropy model to build a classifier for a target class without labeled data by leveraging labeled auxiliary classes and their correlations with the target class, demonstrating effectiveness in text categorization.
The paper proposes a nonnegative shared subspace learning framework to improve social media retrieval by leveraging auxiliary tagging data through controlled subspace sharing via joint Nonnegative Matrix Factorization, validated on image and video retrieval tasks.
The paper proposes a convex surrogate for learning incoherent sparse and low-rank patterns from multiple tasks via a projected gradient optimization framework, demonstrating effectiveness on benchmark datasets.
The paper proposes a multi-task learning algorithm using boosted decision trees that combines task-specific and shared parameters to improve web-search ranking performance across varying-sized datasets from different countries.
The paper proposes **transfer metric learning (TML)**, a convex optimization method that leverages related source tasks to improve metric learning for a target task by jointly learning task relationships and metrics in a unified framework.
The paper proposes a method to efficiently maintain materialized views by filtering irrelevant updates to base relations and applying a differential algorithm for relevant updates, ensuring optimal view maintenance without unnecessary re-evaluations.
The paper proposes a multi-layered abstraction model to justify nonserializable and nonrecoverable schedules in transaction processing, enabling higher data integrity in broader information systems while focusing on recovery aspects.
The paper presents a prototype temporal database system extending Ingres with TQuel support, evaluates its performance across four database types via benchmarks, analyzes key performance factors, and suggests solutions for identified bottlenecks.
The paper introduces Neptune, a prototype hypertext system, and discusses its potential applications in CAD, highlighting the underutilization of hypertext despite its conceptual origins in Vannevar Bush's "memex."
The paper introduces "traversal recursions" as a class of recursive algorithms for directed graph traversals, presents a taxonomy to classify and optimize them, and demonstrates their efficiency and integration into query optimizers for recursive queries and views.
The paper introduces a function-based method to compile recursive queries, including mutual recursion, by computing them as limits of function series, enabling optimized relational algebra or SQL implementations.
The paper advocates for extensible database management systems (DBMSs) that allow modular customization through standardized interfaces, enabling rapid integration of new data types, operations, and performance optimizations while introducing the role of a database architecture administrator (DAA) to oversee system adaptation.
The paper examines and analyzes a hierarchy of algorithms for optimizing multiple queries in relational and deductive databases, ranging from serial execution to exhaustive search for shared processing.
The paper presents an algorithm for converting set-oriented relational algebra queries into efficient iterative programs that process individual tuples, leveraging techniques from functional programming and program transformation.
The paper proposes using Gray codes instead of binary codes for multiattribute hashing to improve record clustering and reduce random disk accesses, achieving 0%–50% performance savings.
The paper proposes an architecture and query processing techniques for main-memory relational databases, introducing the T Tree index for efficient selections and joins, and advocating hashing for projections and joins while retaining sort-merge joins as viable.
The paper presents an insertion-deletion algorithm for sequentially ordered records stored in M memory pages, achieving a worst-case time complexity of approximately \( O(\log_2 M / \text{page-size}) \) when handling \( O(M) \) records.
The paper proposes a rule-based object/task modeling approach integrating object-oriented logic programming and network-oriented formalism to enhance semantic modeling, integrity control, and knowledge representation, combining forward and backward chaining for flexible, extensible, multi-domain applications.
The paper presents MINOS, a multimedia information system with advanced presentation and browsing capabilities, including symmetric text and voice features, to enhance man-machine communication on modern workstations.
The paper introduces POSTGRES, a successor to INGRES, designed to support complex objects, user extendibility, active databases, inferencing, simplified crash recovery, and hardware advancements while preserving the relational model.
The paper presents the design and prototyping of an extended NF² (Non-First Normal Form) DBMS that uniformly handles flat relations, lists, and hierarchical structures, addressing conceptual and performance limitations of existing hierarchical DBMS extensions.
The paper classifies the computational complexity of containment, membership, and uniqueness problems for incomplete databases represented in a hierarchy from Codd-tables to conditioned-tables, showing coNP and Π₂ᴾ-completeness for certain cases, and provides polynomial algorithms for positive existential queries on bounded possible facts while proving NP-completeness for queries with negation or recursion.
The paper introduces *magic counting methods*, a family of techniques combining the efficiency of the counting method and the broader safety of the magic set method for optimizing recursive query evaluation in logic-based languages.
The paper proposes PNLP, a Petri Net-based model for representing and analyzing logic queries in deductive databases, unifying the description of recursive query evaluation algorithms and highlighting their similarities, differences, and potential parallelism.
The paper introduces the Datacycle architecture, which leverages optical bandwidth to enable high-throughput, multiprocessor database systems with unlimited query potential, simplified management, and efficient concurrency control.
The paper introduces improved algorithms for designing non-first normal form relational databases in nested normal form by leveraging embedded multivalued dependencies and distinguishing between functional and multivalued dependencies to reduce redundancy.
The paper presents the design and evaluation of a query optimizer generator for the EXODUS database system, which translates transformation rules into an efficient optimizer that avoids exhaustive search while producing near-optimal plans with significantly reduced search time.
The paper proposes a modular design for query optimizers in database systems using transformation rules to generate efficient query evaluation plans, aiming to enhance extensibility and implementation flexibility.
The paper proposes a graph-based algorithm that optimizes user queries by eliminating redundant joins and restrictions while leveraging semantic integrity constraints to enhance performance through indexed attributes.
The EXODUS extensible DBMS project provides kernel facilities, tools, and an extended C++ language (E) to simplify the development of high-performance, application-specific database systems by enabling customization of data types, access methods, and operations.
The paper presents a database management system architecture enabling relational database extensions through customizable storage methods and attachments (access paths, constraints, triggers), defining required operations and execution coordination mechanisms.
The paper proposes an enhanced voting-based algorithm that allows file updates in a partition with a majority of up-to-date copies, improving availability while maintaining consistency, and analyzes its benefits through a stochastic model.
The paper explores recovery mechanisms for nested transactions by introducing a model that accommodates synchronous/asynchronous invocation and single/conversational interfaces, analyzing recovery properties and dependencies when transactions or savepoints serve as recovery units.
The paper introduces *population analysis*, a method to model node occupancy distributions in hierarchical data structures by describing dynamic transformations between node populations, applies it to quadtrees, and explains discrepancies via aging and phasing effects.
The paper describes ROSE, an experimental CAD/CAM database system that extends the relational model and algebra, leverages OS services, and implements five optimizations for improved performance.
The paper presents ORION, an object-oriented database system that supports schema evolution, defining its framework, semantics, and implementation for dynamic changes in data-intensive applications.
The paper demonstrates that every linearly recursive query can be expressed using transitive closure combined with relational algebra operations, simplifying both the theoretical design of recursive query languages and the practical implementation of deductive databases.
The paper proposes a framework for addressing inference-based security threats in multilevel database and knowledge-based systems by defining safety criteria, application semantics, and enforcement algorithms.
The paper analyzes database transactions against integrity constraints to provide feedback—such as suggested tests, updates, and post-conditions—using a system that generates tailored theories to verify transaction compliance.
The paper proposes and evaluates six parallel and pipelined join algorithms (combining sort-merge and hashing techniques) for distributed databases, finding that hash-based methods generally outperform sort-merge due to higher parallelism, with hybrid hash and hash-based sort-merge methods performing similarly, while relation replication and shared memory further enhance efficiency.
The paper compares storage reclamation algorithms in database systems, finding that Baker's algorithm, combining Copy-Compact's reclustering and Reference Count's cost amortization, performs best for disk-based data by minimizing I/O operations.
The paper compares the performance of query modification, immediate view maintenance, and deferred view maintenance for relational views, showing that the optimal method depends on database structure, view definition, and query/update patterns.
The paper extends the relational algebra to support transaction time, formalizes these extensions with denotational semantics, and shows they can be combined with valid-time historical algebras while preserving the relational algebra's properties.
The paper proposes the ADABTPL data model to define document schemas and standards, ensuring conformity to architecture standards while accommodating the variable structure and multimedia content of office documents.
The paper explores the use of iterative improvement and simulated annealing for optimizing large join queries, finding that iterative improvement outperforms other methods, including simulated annealing, and that general algorithms perform well within time constraints.
The paper describes ORION's transaction management system, which introduces sessions, hypothetical transactions, and extended locking mechanisms for an object-oriented database, alongside conventional logging for recovery.
The paper describes VISION, an object-oriented database system designed for investment analysis and statistical applications, focusing on its solutions for handling time, versions, and concurrency control without combinatorial complexity in object protocol.
The paper proposes an extended relational algebra and a sound and complete query evaluation algorithm for relational databases with null values, improving upon Reiter's incomplete approach.
The paper presents a method to derive additional summary tables (both aggregated and disaggregated) from two given statistical tables with overlapping classification criteria, introducing a "data refinement" procedure and an acyclicity condition to determine when a unique representative table exists.
The paper proposes **Redundant Arrays of Inexpensive Disks (RAID)** as a cost-effective, high-performance alternative to **Single Large Expensive Disks (SLED)**, detailing five RAID levels and comparing their performance to traditional storage systems like the IBM 3380 and Fujitsu Super Eagle.
The paper proposes a multi-copy algorithm that improves performance for both read and update transactions in distributed databases by leveraging application-specific semantics, transaction categorization, and commutativity to ensure correctness while reducing costs.
The paper proposes a hierarchical paradigm for ensuring one-copy serializability in replicated databases by dividing concurrency control into local (group-level) and global (inter-group) policies, simplifying correctness proofs and unifying existing protocols.
The paper develops a formal model and concurrency control theory for multidatabase environments, proposing a protocol that ensures consistency and deadlock freedom while allowing concurrent execution of global and local transactions.
The paper extends Datalog with single-valued data functions, analyzes consistency (undecidable in general but decidable in restricted cases), introduces a syntactic restriction (pairwise consistency) to ensure consistency, and explores implications of dependencies, showing undecidability for some cases and decidability for others.
The paper introduces **NESTED_DATALOG**, a syntactical extension of DATALOG for reasoning with complex objects, and proposes a **token object model** to map nested predicates to relational databases, enabling compilation of complex object rules into classical DATALOG.
The paper introduces an optimal twin grid file method that achieves ~90% storage space utilization—significantly higher than the standard grid file's 69%—while maintaining comparable construction speed and query performance for spatial searches.
The paper shows that deciding whether a full join dependency and a set of functional dependencies imply an embedded join dependency is solvable in polynomial time when the embedded join dependency's schema is a subset of the full join dependency's schema and the functional dependencies are embedded in it.
The paper introduces a first-order situational logic formalism for specifying database dynamics, including integrity constraints and transactions, and discusses its application to transaction verification and synthesis.
The paper proposes a generalized relational model for temporal databases using multidimensional Boolean algebra-based time stamps, enabling symmetric handling of temporal dimensions and applications like querying past states, classifying errors, and managing updates.
The paper describes Tacoma, a relational-based, object-oriented database management system designed for large-capacity CAD/CAM applications, specifically for managing LSI and VLSI mainframe computer designs at Amdahl Corporation.
The paper presents a specialized data management system designed for particle physics codes in shared-memory multiprocessor environments, offering parametric control over physical design, retrieval, and distribution to optimize efficiency beyond standard database systems.
The paper presents a knowledge base management system (KBMS) using CML to model and control database software development from requirements to implementation, enhancing maintenance through decision-centered documentation.
The paper introduces a specialized algorithm for efficiently evaluating selections on separable recursive queries, achieving linear time complexity (O(n)) in some cases, outperforming existing methods like Generalized Magic Sets (Ω(n²)) and Generalized Counting (Ω(2ⁿ)).
The paper classifies linear recursive formulas in deductive databases using a graph model, identifies common characteristics for optimization, and derives compiled formulas and query evaluation plans for each class.
The paper introduces *predicate decomposability*—a property enabling parallel evaluation of logic programs without inter-processor communication—and characterizes decomposability for three classes of single-rule programs (nonrecursive, linear, and simple chain), showing nonrecursive programs are always decomposable while providing conditions for the other two classes.
The paper summarizes a benchmark showing that NonStop SQL scales linearly from 14 to 208 Debit Credit transactions per second as hardware expands from 2 to 32 processors, outperforming a record-at-a-time file system.
NonStop SQL achieves high performance by integrating SQL with existing disk I/O and transaction subsystems, reducing message traffic and CPU usage through server-level optimizations like data filtering, buffer pool management, and SQL-aware locking, while maintaining high availability and distributed functionality.
The paper evaluates the Gamma database machine's performance by analyzing query response times, scalability with processors, and the impact of memory and disk configurations, comparing it to a Teradata system and identifying its strengths and weaknesses.
The paper introduces a formal model for transaction commitment protocols in partitioned distributed databases, ensuring consistency and proposing more efficient solutions than existing methods.
The paper describes the design and implementation of a two-layer data/knowledge base management testbed that uses logic programming (Horn clauses) to unify queries, data, and knowledge, compiling them into embedded-SQL for execution on a relational database system, and shares insights from testing and implementation.
The paper introduces the Relational Production Language (RPL), which integrates expert systems with relational databases by modeling working memory as a relational database and rules as relational queries with database updates, enabling parallelism and meta-rule-based control.
The paper formally describes the object-oriented data model of the O₂ database system, introducing a type system with inheritance and strong typing within a set-and-tuple framework.
The paper proposes a language feature to handle contradictions in class definitions within object-oriented systems by separating "class" and "type" notions, ensuring subclasses remain subsets and subtypes while enabling explicit acknowledgment of inconsistencies.
The paper proposes a probabilistic model for evaluating how relational operations affect attribute originality (distinct values) without relying on uniformity or independence assumptions, providing exact results for independent attributes and accurate approximations for dependent ones.
The paper presents an integrated garbage collection and recovery algorithm for managing a stable heap, ensuring object accessibility across system crashes and media failures while coordinating with the recovery system to maintain consistency.
The paper introduces ODE, an object-oriented database system and environment built on C++ (via O++), offering persistent and versioned objects, sets, constraints, triggers, and integrated database and general-purpose manipulation.
This paper investigates how parallelism affects the performance of four concurrency control algorithms (two-phase locking, wound-wait, timestamp ordering, and optimistic) in distributed database machines, analyzing scalability, partitioning benefits, and overhead impacts across varying system loads.
The paper proposes a declarative database logic that integrates object-oriented features like identity, inheritance, and methods with a formal semantics and a sound, complete proof procedure, enabling unified schema and data querying.
The paper develops a formal framework to analyze the expressive power and complexity of OODB queries, comparing three set-modeling approaches—object-based, value-based, and algebraic—with varying computational complexities, and introduces a relational query language equivalent to the algebraic OODB model.
The paper introduces IQL, a powerful object-based query language that uses object identities (oids) to represent cyclic and shared data structures, manipulate sets, and express any computable query while supporting static type checking and bottom-up evaluation.
This paper proves that the general implication problem involving multiple comparison operators, conjunctions, and disjunctions is NP-hard, but presents a polynomial-time algorithm for a restricted version without "≠" in the query or disjunctions in the target, while also analyzing special cases where these constraints can be relaxed.
The HiPAC project proposes an architecture for an active database management system (DBMS) that supports Event-Condition-Action (ECA) rules, enabling automated responses to database events within transactions and introducing a new paradigm for database application development.
The paper presents an algorithm for efficiently evaluating recursive queries by rewriting and semi-naive bottom-up evaluation, achieving an \( O(n) \) speedup over magic sets by reducing the arity of recursive predicates.
The paper proposes a grammatical model for hierarchical information using grammar-generated trees and introduces two equivalent transformation languages, an algebra and a calculus, for manipulating such structures.
The paper introduces a recursive algebra for nested relations that enables efficient and succinct querying and optimization by allowing direct access and modification of tuples at all nesting levels without restructuring.
The paper explores how optimizing redundancy in z-order-based spatial indexes improves query accuracy and search performance, identifying an optimal redundancy level for given datasets.
This paper proposes a run-time clustering algorithm for object-oriented databases, leveraging inheritance and structural relationships to improve buffering and clustering strategies, demonstrating a 200% response time improvement in high read/write ratio CAD applications while advocating for limited I/O during reclustering.
The paper presents a simple and responsive concurrency control algorithm for real-time groupware systems, which avoids locks by leveraging operation semantics, illustrated with a group text editing example.
The paper proposes a modular version control mechanism that decouples version and concurrency control in multiversion protocols, improving extensibility, correctness verification, and read-only transaction independence.
The paper presents RIDL*, a tool that transforms NIAM semantic networks—rich in integrity constraints—into relational database designs (normalized or not) using a rule-driven, stepwise synthesis process, while allowing control by a database engineer.
The paper proposes a canonical relational representation for Extended Entity-Relationship (EER) structures, proves its correctness by defining equivalence in information-capacity, and demonstrates that this representation can be normalized to Boyce-Codd Normal Form under specific conditions.
The paper introduces *Bayan*, an object-oriented Arabic text database management system designed specifically to address the linguistic complexities of Arabic, including word derivation, morphology, and writing rules, along with a tailored user interface and keyboard layout.
The paper introduces independence-reducibility based on modified key dependencies, presents a polynomial-time algorithm to design BCNF databases with minimal relation schemes that are independent and update-efficient, and extends the method to constant-time-maintainable and separable BCNF schemes.
Volcano is an extensible, single-process-designed dataflow query processing system that uses the novel *operator model* and an *exchange operator* to enable efficient intra- and inter-operator parallelism while simplifying parallel algorithm implementation.
The paper presents parallel, bottom-up Datalog evaluation methods using discriminating predicates to partition computation non-redundantly across processors, analyzing trade-offs between redundancy and interprocessor communication.
The paper proposes a secure concurrency control protocol for multilevel secure replicated databases that ensures one-copy serializability while minimizing trusted code size.
The paper proposes a unified framework for querying both data and complex knowledge in databases, introducing *retrieve* and *describe* statements to access data and metadata, respectively, along with algorithms for evaluating knowledge queries.
The paper introduces **ACTA**, a formal framework that unifies and extends existing transaction models by characterizing the full spectrum of interactions in cooperative and competitive environments, enabling specification of transaction structure, behavior, and reasoning about concurrency and recovery properties.
The paper proposes a model using triggers and extended nested transactions to flexibly organize and control long-running, multi-step activities by leveraging event-condition-action rules and coupling modes for modular control flow, concurrency, integrity, and exception handling.
The paper proposes a fault-tolerant multidatabase transaction management algorithm with two-phase locking, ensuring global consistency and deadlock freedom despite local failures.
The paper presents the design and implementation of the RDL1 system, which integrates production rules, a programming environment, and abstract data types into a relational database to make deductive database technology more practical for real-world applications.
The paper introduces an efficient algorithm using interval binary search trees (IBS-trees) to improve predicate-matching speed in forward-chaining rule systems, particularly for large sets of equality and inequality predicates on ordered domains.
The paper argues that a rule system in a next-generation DBMS can unify and enhance views, procedural data types, and update semantics while enabling performance optimizations like caching.
The paper introduces and analyzes three pointer-based join algorithms (nested-loops, sort-merge, and hybrid-hash variants), showing they outperform standard methods in many cases but highlighting the poor performance of the pointer-based nested-loops join for medium to large joins.
The paper proposes a Two Phase Optimization algorithm combining Simulated Annealing and Iterative Improvement for query optimization, demonstrating its superiority in cost efficiency and runtime over traditional methods by analyzing the cost function's structure in project-select-join queries.
The R*-tree outperforms existing R-tree variants by optimizing area, margin, and overlap of enclosing rectangles, offering superior performance for spatial queries and operations while maintaining low implementation cost.
The paper evaluates various methods for mapping multi-dimensional data to one-dimensional space, proposing a Hilbert space-filling curve as the most effective solution based on analysis and simulation.
The Time-Split B-tree is a versioned database index that migrates data from current to historical storage during node splits, introducing controlled redundancy, with analysis and simulation evaluating redundancy, space utilization, and insertion/update performance under varying workloads and splitting policies.
Kaleidoscope is a cooperative query interface that guides users in constructing accurate database queries through context-sensitive menus and derived predicates, eliminating the need for prior knowledge of query languages or database structures.
The paper proposes extending traditional query optimization techniques to minimize response time for parallel execution of Select-Project-Join queries while maintaining throughput constraints, addressing challenges in cost modeling and dynamic programming adaptation.
The paper proposes a scheduling algorithm for parallel query processing in shared-memory systems that maximizes resource utilization by dynamically balancing IO-bound and CPU-bound tasks and extends query optimization to incorporate inter-operation parallelism.
The paper describes the Ariel active DBMS's rule system, which efficiently tests rule conditions using a discrimination network with a specialized data structure and the A-TREAT algorithm (a modified TREAT with virtual α-memory nodes), and executes rule actions by binding matched data at fire time and leveraging the query processor.
The paper presents static analysis methods for assessing termination, unique final state, and unique action stream guarantees in database production rules, identifies problematic rules, and provides criteria to ensure these properties within the Starburst Rule System.
The paper proposes real-time database locking protocols using ordered sharing to eliminate blocking and reduce missed deadlines, outperforming traditional two-phase locking.
The paper analyzes five integrated buffer coherency policies, classifying them by recovery complexity, and evaluates their performance trade-offs in throughput, response time, and recovery overhead.
The paper introduces the Term Retrieval Abstract Machine (TRAM), a specialized interpreter that optimizes data structure traversal in complex objects by compiling navigation and schema information into efficient executable programs, significantly reducing interpretation overhead compared to naive methods.
The paper analyzes recovery time in a database system using a Write-Ahead Log protocol with strict LRU buffer replacement, deriving analytical models for uniform and skewed access patterns, showing that data I/O dominates recovery time under uniform access but not necessarily under extreme hot-set conditions.
The paper proposes Multi-Level Recovery (MLR), a method that enhances database concurrency by using high-level compensation actions for subtransaction undo, logging these operations with commit records in a single log, and unifying nested and multi-level transaction handling.
The paper proposes a "Multiplexed" R-tree with a proximity index (PI) heuristic for distributing nodes across multiple disks to maximize throughput for range queries, achieving near-linear speedup and outperforming other heuristics and disk-stripping approaches.
The paper evaluates the performance of a 1-safe remote backup algorithm (epoch algorithm) against a 2-safe approach and explores the use of multiple log streams for data propagation between primary and backup databases.
The paper proposes a cost-based optimization method for recursive object-oriented queries that delays selective operation decisions until their impact can be evaluated by a cost model, enhancing efficiency and optimization opportunities.
The paper presents a temporal complex-object data model centered on the concept of *time slices*, which represent object states and are mapped onto the non-temporal MAD model while preserving its properties for handling overlapping objects.
The paper analyzes and compares the segment-based storage structures and algorithms in EXODUS, Starburst, and EOS for managing large unstructured objects, evaluating performance metrics like creation time, scan time, storage utilization, and I/O costs for various operations.
The paper presents an algorithm that partitions transactions into the finest possible pieces to enhance concurrency while ensuring serializability, assuming user knowledge of transaction sets and conventional consistency options, with an efficient runtime of O(n × (e + m)).
The paper investigates the performance benefits of using non-volatile semiconductor memory (e.g., solid-state disks, disk caches) in transaction processing through simulation, comparing different storage allocation and caching strategies for the debit-credit workload.
The paper introduces Tapestry, a system that efficiently processes continuous queries (including joins and temporal conditions) on append-only databases like mail and news messages by converting static queries into incremental ones without relying on database triggers.
The paper presents an asymptotically efficient sequential random sampling procedure for estimating query result sizes with guaranteed precision and confidence, without requiring pilot samples or prior data assumptions, while also addressing undercoverage and cost reduction via stratified sampling.
The paper presents a recovery-compatible concurrency control method for B-link trees by decomposing structural changes into atomic, level-specific actions that maintain tree integrity and work with various recovery schemes.
The paper introduces **ARIES/IM**, a high-concurrency method for B+-tree index management in transaction systems, ensuring serializability and recovery via write-ahead logging while allowing concurrent operations during structural modifications.
ObjectStore is an object-oriented database system featuring persistence orthogonal to type, transaction management, associative queries with nested query support, dynamic indexing, and multi-strategy query execution.
The paper extends classical datalog with object-oriented features like methods, classes, inheritance, and late binding, explores resolution approaches, and connects the framework to view specification and dynamic class hierarchy derivation.
The paper presents DATEX, a database rule system with a novel indexing technique for efficient rule activation, demonstrating its performance and minimal modifications needed to standard DBMS environments for handling complex rule programs.
This paper evaluates and compares existing memory-adaptive hash join algorithms for priority-scheduled database systems and introduces a new family of algorithms that better handle memory fluctuations.
The paper proposes an event interface in object-oriented databases to enable reactive capabilities by treating events and rules as first-class objects, allowing dynamic event detection and rule specification across objects while separating event detection from rule execution.
The paper advocates for a **co-existence approach** combining C++ (object-oriented) and Starburst (relational) systems to achieve the benefits of both paradigms efficiently without creating a hybrid system.
The paper studies the INSPECT database of abstracts using trace-driven simulation to analyze physical index design, inverted index caching, and database scaling in a distributed shared-nothing system, showing their significant impact on performance.
The paper describes the Glue-Nail database system, which combines the declarative Nail language for queries and the procedural Glue language for non-query tasks, compiling both into IGlue code optimized by static and runtime techniques, achieving effective performance synergism.
The paper presents efficient incremental algorithms—counting for nonrecursive views and DRed for recursive views—to update materialized views in relational and deductive databases in response to base relation changes, handling SQL/Datalog with operations like negation, aggregation, and recursion.
The paper presents an efficient algorithm for mining significant association rules from large customer transaction databases, incorporating buffer management, estimation, and pruning techniques, and demonstrates its effectiveness on retail sales data.
The paper introduces *temporal modules*—a framework with windowing functions and a query language—to resolve temporal model and time unit mismatches in federated databases.
The paper proposes and evaluates signature file techniques—sequential and bit-sliced—for efficient set predicate queries in object-oriented databases, demonstrating that the bit-sliced approach outperforms nested indexing in retrieval, storage, and update costs.
The paper describes the development of the Open OODB query optimizer, the first to use a complete extensible optimization framework, demonstrating its effectiveness in generating efficient query plans and highlighting the importance of a comprehensive framework for optimization.
The paper introduces LH*, a scalable and efficient distributed hashing scheme that generalizes Linear Hashing to parallel or distributed environments, enabling dynamic growth, high load factors, and low message complexity for insertions and retrievals.
The paper introduces *lazy updates*, a scalable and low-overhead replication method for distributed search structures like the dB-tree, which avoids synchronization and enables high concurrency while ensuring correctness.
The paper explores how nomadic computing—enabled by mobile devices and wireless networks—impacts database systems by altering cost metrics (e.g., power consumption), requiring new transaction protocols, influencing interfaces due to limited screens, and introducing security challenges from data mobility across autonomous networks.
The paper proposes a schema-flexible database management system for unstructured data in applications like science, hypermedia, and finance, enabling incremental schema development during data processing.
"Open DECdtm provides portable transaction management services on OSF DCE, supporting X/Open interfaces (TX, XA, TxRPC) and enabling interoperability with OSI TP and OpenVMS systems through constraint-based protocols."
The paper describes how the KSR1 computer uses the Kendall Square Query Decomposer and Oracle Parallel Server to parallelize decision-support queries and transactions for improved performance in a shared-memory environment.
The paper discusses the development of a prototype scheduler for multidatabase transactions in telecommunication workflows, addressing concurrency control and recovery using application-specific properties for efficient solutions.
The paper explains how the Encina® distributed transaction processing software enables reliable distributed applications by detailing its toolkit components for ACID compliance, integration in DCE environments, and the Encina Monitor for organizing servers and machines.
The paper introduces Sybase's Replication Server, a product that dynamically maintains distributed data subsets with flexible transaction models for robust, high-performance applications by moving transactions rather than data, optimizing network bandwidth and reducing costs compared to traditional methods.
The paper evaluates a framework for specifying and enforcing interdatabase dependencies to improve data consistency, reduce manual interventions, and enhance data quality in multidatabase industrial environments.
The Carnot Project at MCC aims to unify heterogeneous, distributed enterprise information by enabling efficient navigation, consistent updates, and simplified application development through a prototype featuring enterprise modeling, query expansion, and consistency management, with a focus on transaction processing via task-based scheduling to satisfy intertask dependencies.
The NAUDA system is a cooperative German-language database interface for relational databases that enhances user interaction through features like over-answering, handling presupposition failures, and providing natural language responses and geographical maps, particularly for environmental protection domains.
"Logres is a declarative, object-oriented deductive database system that integrates complex object manipulation, rule-based operations, and procedural qualifiers for controlling rule application."
VODAK is an object-oriented distributed database system prototype that enhances parallelism in concurrent transactions through open nested transactions, offering insights into database internals for users, programmers, and developers.
The Rufus system enhances management of semi-structured data by integrating database functionality without requiring data migration, addressing limitations of traditional filesystems and databases.
The paper introduces **ASSET**, a system providing customizable transaction primitives to support diverse extended transaction models (e.g., nested, split, sagas) for complex applications like CAD and software engineering, implemented in the Ode database.
The paper introduces ARIES/CSA, a recovery algorithm for client-server architectures that ensures correct recovery using write-ahead logging, fine-granularity locking, and flexible buffer management without requiring clock synchronization.
The paper introduces staggered striping as a novel disk declustering technique to enable continuous retrieval of multimedia objects across multiple disks, improving performance for concurrent user access.
The paper proposes *timed streams* as a fundamental abstraction for modeling time-based media, introducing media-independent structuring mechanisms and a data model to address the complex organization and relationships in multimedia data.
The paper presents a combinatorial pattern discovery method for identifying explanatory patterns in protein sequences using string edit distance with variable-length wildcards, demonstrating its effectiveness through experiments on synthetic and real protein data, with results that complement existing classifiers.
The paper introduces *GlOSS* (Glossary of Servers Server), a practical method for solving the text database discovery problem by estimating query result sizes across databases, and evaluates its effectiveness and storage costs using real user queries.
The paper proposes a query feedback-based curve-fitting method for estimating database query selectivities without additional statistics gathering, achieving accuracy comparable to traditional approaches while adapting dynamically to data changes.
The paper presents an efficient two-phase algorithm for estimating B-tree index scan page fetches, where an initial LRU buffer modeling phase builds an empirical model, followed by a lightweight estimation procedure used by the query optimizer.
The paper proposes a processor allocation scheme and hash filtering technique to optimize parallel execution of pipelined hash joins, demonstrating improved performance through simulation.
The paper introduces a Priority Memory Management (PMM) algorithm for real-time database systems that dynamically adjusts multiprogramming levels and memory allocation to minimize missed deadlines, with simulations confirming its effectiveness.
AlphaSort, a cache-sensitive memory-intensive sort algorithm using commodity hardware, outperforms existing benchmarks by achieving a seven-second sort time and processing over a gigabyte in a minute, while proposing new benchmarks (Minutesort and DollarSort) to account for startup costs.
The paper introduces the PARADISER architecture and its kernel rule language PARULEL, enabling parallel and distributed rule evaluation with dynamic load balancing to enhance scalability and performance, as demonstrated through transitive closure benchmarks.
The paper proposes a dual-structure index for efficient incremental updates of inverted lists by dynamically separating long and short lists, optimizing retrieval, updates, and storage, and evaluates trade-offs between update time and query performance using real-world data and simulations.
The paper presents a unified framework for accessing and manipulating data across databases and file systems, leveraging text indexing and optimization techniques to achieve efficient query performance, sometimes surpassing traditional database implementations, while analyzing the efficiency-indexing tradeoff.
The paper proposes redefining the outerjoin operator as a disjunctive operation (combining results from either the join or its arguments) rather than preserving arguments, enabling commutativity and associativity, and explores evaluation strategies and limitations of binary outerjoins for data merging queries.
The paper demonstrates that an adaptive page server architecture, which combines object-level and page-level locking, outperforms pure page and object servers in terms of concurrency and efficiency for various workloads.
The XSB system is a high-performance, in-memory deductive database engine based on Prolog, enhanced with tabling for efficient query processing, improved indexing, and flexible data modeling via HiLog, outperforming other deductive database systems for many queries.
The paper outlines key challenges facing the database community, proposes an agenda to address them (as pursued by Microsoft's Enterprise Group), and concludes with a 2001 vision of "Information at your fingertips."
The paper proposes crash recovery protocols for shared memory database systems on cache-coherent multiprocessors to prevent unnecessary transaction aborts caused by node failures while ensuring correct transaction rollback and minimal overhead.
The paper introduces **LyriC**, an object-oriented query language for databases handling spatial, temporal, and constraint data, unifying constraint-based design, optimization, and spatial databases by extending XSQL with path expressions and first-class constraint objects.
The paper introduces the monoid calculus as an effective database calculus for object-oriented query languages like OQL, demonstrating its ability to handle diverse collection types, aggregations, nested queries, and extensions for vectors, arrays, identity, and updates.
The paper presents an efficient branch-and-bound R-tree traversal algorithm for finding k nearest neighbors to a point in space, evaluates search strategies and pruning metrics, and demonstrates its performance through experiments.
The paper presents a method to ensure predictable worst-case performance in multi-dimensional index structures by preserving B-tree-like logarithmic access times and minimum node occupancy through a controlled recursive partitioning approach.
The paper analyzes how Minimum Bounding Rectangles (MBRs) convey topological relations in spatial databases, applies these findings to optimize query performance in R-trees and their variants, and explores handling complex spatial queries.
The paper introduces and compares the *sorted neighborhood* and *clustering-based* methods for efficiently and accurately merging data from multiple databases (the *merge/purge* problem), proposing a *multi-pass* approach with Transitive Closure to enhance accuracy.
The paper introduces *indexing by class-division (CD)*, an efficient extension of the class hierarchy index (CH) technique for OODBs that improves range query performance while maintaining practical implementation using B+-trees, albeit with modest space and update overhead.
The paper proposes an efficient indexing technique using Hidden Markov Models (HMMs) and a modified trie structure to enable fast and scalable retrieval of handwritten text in large ink databases, with implementations for both main-memory and disk-based systems.
The paper proposes a fast, linear-time algorithm to map objects into k-dimensional points while preserving dissimilarities, enabling efficient retrieval and visualization, outperforming traditional Multi-Dimensional Scaling (MDS) in speed and scalability.
The paper proposes a hash-based algorithm to efficiently generate smaller candidate 2-itemsets for mining association rules in large transaction databases, significantly improving performance by reducing computational costs in early iterations.
The paper compares four crash recovery techniques in an OODBMS (page differencing, sub-page differencing, whole-page logging, and redo-at-server) using the OO7 benchmark and finds that differencing-based methods outperform whole-page logging.
The paper concludes that the most effective histograms for database query optimization balance accuracy and efficiency by precisely tracking frequencies of a few attribute values while assuming uniformity for the rest, with optimality determined by self-join queries.
The paper examines efficient methods for importing materialized view updates in real-time databases, analyzing update properties and comparing four scheduling algorithms to balance freshness and transaction timeliness.
The paper proposes a microeconomics-inspired resource allocation framework using a profit-maximizing broker that dynamically manages resources (e.g., memory and disk bandwidth) for multi-query workloads to minimize slowdown while ensuring fairness.
The paper introduces a hypergraph abstraction and algorithms to efficiently reorder complex queries with outer joins and duplicates, enabling optimizers to explore a larger plan space and reduce costs without exponential intermediate joins.
The paper introduces the Eager Compensating Algorithm (ECA) to eliminate anomalies in warehouse view maintenance caused by decoupled data sources, along with optimized variants and a performance comparison.
The paper investigates efficient materialization and maintenance of mediated views across heterogeneous data sources, proposing improved algorithms for handling updates to both views and underlying sources, including a novel fixpoint operator \( W_P \) for accurate update semantics without recomputation.
The paper presents a simulation study of a scalable video-on-demand system, comparing advanced disk scheduling, prefetching, and buffer management algorithms against simpler approaches and demonstrating near-linear scalability with increasing resources.
The paper proposes COPS, a system for registering and detecting complete or partial document copies in digital libraries, presenting algorithms, evaluation metrics, and experimental results for effective copy detection.
The paper introduces the *Enterprise Objects Framework*, a second-generation tool that integrates object-oriented programming with relational databases to enhance productivity, flexibility, and adaptability in business application development while supporting distributed object management.
The paper discusses the rapid advancements in storage technology, including increased areal density, higher data rates, RAID adoption, and the need for parallel storage systems to evolve beyond traditional RAID levels.
QBI is an icon-based query interface for distributed databases that uses pure iconic manipulation and metaquery tools to simplify query formulation without requiring database schema knowledge, making it accessible to both novice and expert users.
The paper discusses the challenge of managing vast, heterogeneous information in organizations, leading to "write-only" databases where data is stored but difficult to retrieve or utilize effectively.
The VisDB system, developed at the University of Munich, leverages human visual perception to enable efficient exploration and analysis of large databases through interactive visualizations.
The Paradise project aims to develop a parallel GIS system using object-oriented and parallel database technologies to manage massive multi-terabyte datasets, with Phase 1 achieving a functional client-server version and Phase 2 focusing on tertiary storage and cluster support.
The paper describes a toolkit for combinatorial pattern matching and discovery in sequences and trees, with applications in molecular biology and document comparison.
The paper proposes efficient algorithms for mining association rules between two numeric attributes (e.g., Age, Balance) and one Boolean attribute (e.g., CardLoan) by identifying optimal rectangular or admissible planar regions that maximize gain, support, or confidence, and includes a visualization system for these rules.
The paper presents a client-server solution for business data analysis that integrates exploration and analysis tasks by combining a history mechanism with reusable graphical representations and leveraging database capabilities to meet identified requirements.
The paper presents two fault-tolerant approaches with data placement and admission control algorithms to ensure continuous media servers maintain guaranteed retrieval rates during disk failures while maximizing client support.
The paper proposes methods for optimizing queries in multimedia repositories with multiple attribute types by minimizing index searches and efficiently handling independent predicates, extending traditional filtering to include ranking and top-k retrieval.
The paper introduces **BIRCH**, an efficient clustering algorithm for large datasets that minimizes I/O costs, handles noise effectively, and achieves high-quality clustering with minimal scans, outperforming existing methods like CLARANS.
The paper presents an efficient online reorganization method for sparsely-populated B+-trees, involving leaf compaction, optional reordering, upper-tree shrinking, minimal locking, forward recovery, and a heuristic to reduce swaps while ensuring consistency and concurrency.
The paper proposes a cost-based query optimization technique for mediator systems that uses cached statistics of source calls and query results, along with semantic invariants, to improve performance and handle remote source challenges.
The paper introduces **OLE DB**, a Microsoft-developed set of extensible COM-based interfaces enabling uniform, transactional access to diverse data sources (both DBMS and non-DBMS) by modularizing core database functionalities like query processing, transactions, and schema management.
The paper demonstrates that anywhere-anytime-anyway transactional replication becomes unstable under scaling workloads, proposes a two-tier replication algorithm with commutative updates to mitigate deadlocks and reconciliations, and highlights the advantages of master-copy replication.
The paper proposes "hot mirroring," a storage management scheme for disk arrays that partitions space into high-performance mirrored regions for hot data and high-efficiency RAID5 regions for cold data, optimizing performance and minimizing rebuild degradation.
The paper proposes greedy algorithms to select optimal subsets of views to materialize in a data cube for efficient query processing, demonstrating their near-optimal performance under various models and analyzing tradeoffs in hypercube lattices.
The paper proposes **Q2P**, a quantified query processor using multidimensional and boolean matrix structures, to efficiently handle complex quantified queries in relational databases, demonstrating significant performance improvements for decision support applications.
The paper introduces a functional-based query language for multidimensional arrays, treating them as functions from indices to values, and presents its calculus, implementation, optimization rules, and expressiveness analysis.
The paper introduces a spatial hash-join framework that outperforms tree-based spatial join methods by using dynamic partition functions for the inner dataset and an immutable, replicating partition function for the outer dataset, eliminating the need for pre-computed indices.
Bifocal sampling is a novel join size estimation technique that classifies tuples as sparse or dense, employs distinct estimation procedures for different combinations, and provides accurate estimates within a constant factor with high probability using a sample size of \( O(\sqrt{n} \log n) \), outperforming prior methods.
The paper presents a comprehensive taxonomy of histograms for database systems, introduces new types and construction methods, and identifies the most effective histogram types for selectivity estimation through empirical evaluation.
The paper proposes an abstract-object storage model to uniformly manage both internal and external data by exporting database services like indexing and query processing, using geospatial metadata files as an example.
The paper introduces *Class Fencing*, a new goal-oriented buffer allocation algorithm based on *hit rate concavity*, which improves responsiveness, robustness, and simplicity while maintaining accuracy and stability in achieving multi-class workload response time goals.
The paper surveys existing object placement algorithms for database systems, introduces a new algorithm for the Shore database system that outperforms current methods in CPU, memory, I/O, and disk utilization, and validates its effectiveness through experiments.
The paper introduces **KOLA**, a combinator-based query algebra designed to simplify rule-based query optimization by enabling concise, code-free rule formulation and manipulation, even for complex nested queries.
The paper proposes an extension of an object algebra with new operators and rewriting techniques to optimize the evaluation of queries with generalized path expressions, improving efficiency over naive algorithms.
The paper compares memoization, sorting, and Hybrid Cache (a variant of unary hybrid hashing) for optimizing queries with expensive methods on duplicate values, showing that Hybrid Cache outperforms the others while offering broader applications and raising new optimization challenges.
The paper presents a method for optimizing SQL view maintenance by materializing additional views, formulating it as a global optimization problem, and developing a memoing-based solution that accounts for shared subexpressions, with applications in efficiently checking SQL-92 integrity constraints.
The paper presents new algorithms for deferred incremental view maintenance that avoid a state bug, explores three scenarios for using auxiliary tables to balance per-transaction overhead and refresh time, and demonstrates how to optimize both metrics with proper auxiliary table selection.
The paper presents efficient algorithms for detecting minimal-cost changes between hierarchically structured data trees, improving performance over general-purpose methods, with applications in document management and other domains.
The paper proposes UnQL, a query language for tree-like and graph-structured data that generalizes relational algebra, supports deep and cyclic structures, and can be efficiently optimized for both vertical (depth) and horizontal (breadth) query dimensions.
The paper proposes using Logic++, a higher-order Horn clause logic language, to simplify GUI programming by modeling event handlers as query-dependent updates with materialized views, demonstrating that database techniques like view maintenance and active databases are beneficial for GUI applications.
The paper proposes a cross-language variable substitution mechanism between HTML and SQL to enable seamless web-based database access, implemented in the DB2 WWW Connection system for building user-friendly database applications.
The paper introduces **DB-Miner**, a data mining system that interactively extracts multi-level knowledge from large relational databases using techniques like attribute-oriented induction and meta-rule guided mining to perform generalization, characterization, association, classification, and prediction efficiently.
The Prospector Multimedia Object Manager is a parallel-processing server for analyzing and manipulating multimedia content through user-defined functions, supporting applications like fingerprint matching, face recognition, and speech processing.
The Garlic project develops a multimedia information system that integrates heterogeneous data sources—including databases and non-database servers—while preserving server autonomy, using an object-oriented schema, query optimization, and a graphical interface (PESTO) for seamless querying and browsing.
The paper presents a parallel nearest-neighbor search method optimized for high-dimensional spaces by declustering data across disks based on quadrant assignments, achieving near-linear speed-up and outperforming the Hilbert curve approach by up to 5x.
The paper proposes a set of linear transformations on Fourier series representations for efficient similarity queries on time-series data, demonstrating competitive performance with R-tree indexing compared to exact match queries and sequential scanning.
The paper presents a heuristic algorithm for detecting meaningful changes in hierarchical data by transforming the problem into computing a minimum-cost edge cover in a bipartite graph, supporting operations like subtree moves and copies for semantically richer descriptions.
The paper reviews existing indexing technologies in data warehousing, introduces Bit-Sliced and Projection indexing, demonstrates their performance advantages for SQL operations, and proposes a new efficient method for multi-dimensional group-by queries.
The paper proposes a *summary-delta table* method to efficiently maintain aggregate views in data warehouses, addressing both single-summary-table maintenance and batch updates of multiple summary tables to minimize downtime.
The paper presents a MOLAP algorithm for computing the Cube operator, demonstrating its superior speed over ROLAP methods when using appropriate compression techniques, and suggests its potential utility even in ROLAP systems.
The paper proposes an **online aggregation** interface that allows users to monitor and control aggregation queries interactively, providing progressive results with confidence intervals, and describes its implementation in POSTGRES.
The InfoSleuth project integrates agent technology, domain ontologies, and brokering to dynamically retrieve and process information in scalable, open environments, overcoming limitations of static federated databases and keyword-based web search.
The paper introduces **STARTS**, a protocol developed collaboratively by Stanford's Digital Library project and multiple organizations to standardize and improve interoperability when querying diverse document sources across internal networks and the Internet.
The paper presents efficient garbage collection techniques for large persistent object stores by independently collecting partitions while maintaining and recovering inter-partition reference information on disk, using optimized global marking to collect cyclic garbage across partitions with minimal disk and memory overhead.
"CoDecide is a user interface toolkit extending spreadsheet concepts to support cooperative, multi-dimensional data analysis with features like drill-down/roll-up and synchronized multi-user views via 'tapes,' enabling rapid interface composition and linkage to data sources."
The SR-tree, which combines bounding spheres and rectangles, outperforms the SS-tree and R*-tree in nearest neighbor queries for high-dimensional data by reducing region volumes and improving partitioning efficiency.
The paper proposes "wave indices" for efficiently maintaining and querying dynamic data windows by enabling quick updates and expiration of daily data while comparing their performance across storage, query speed, and maintenance.
The paper introduces **2VNL**, a concurrency control algorithm that enables serializable, lock-free concurrent execution of maintenance transactions and read-only queries in data warehouses by logically maintaining two database versions.
The paper introduces TREPL, a composite event language with advanced temporal aggregation capabilities that surpass existing active database systems and match Time Series Management Systems, while providing a formal semantics based on Datalog¹S.
This paper evaluates secure concurrency control protocols in real-time database systems, identifies OPT-WAIT as the best-performing method, and proposes a hybrid dual approach that further improves performance while ensuring security.
The paper presents the implementation of a Context Interchange Prototype, demonstrating how a context mediator detects and resolves semantic conflicts among heterogeneous systems by comparing contexts.
The paper introduces MDM, a tool with Model Manager and Schema Manager components that allows model engineers to define various data models and perform automated schema translations between them, supporting customizable CASE environments for information system design.
The paper introduces **GeoMiner**, a spatial data mining system prototype that extracts characteristic, comparison, and association rules from geo-spatial databases using a **SAND architecture**, spatial data cubes, OLAP, and a specialized query language (**GMQL**), with plans to extend it for classification and clustering rules.
The paper introduces *TreeDiff*, a tool for structural matching and discovery in SGML/HTML documents by representing them as ordered labeled trees and computing optimal edit operations (insert, delete, change) to transform one document into another.
The paper proposes a constraint-based, human-centered architecture for exploratory association rule mining, introducing constrained association queries and efficient pruning techniques (anti-monotonicity and succinctness) to improve performance significantly.
The paper proposes buffer-aware query evaluation and ranking-aware buffer replacement techniques to improve the efficiency of information retrieval (IR) queries by addressing differences in query flexibility, access patterns, and iterative refinement compared to database systems.
The Pyramid-Technique is a high-dimensional indexing method that outperforms existing structures like the X-tree and Hilbert R-tree by efficiently mapping data to a 1-dimensional space using a pyramid-based partitioning strategy and a B+-tree, significantly improving range query performance.
The paper introduces a novel multi-step k-nearest neighbor search algorithm that minimizes candidate generation, significantly outperforming the state-of-the-art method with up to 120× fewer candidates and 48× faster runtime.
The paper proposes efficient techniques for dynamically updating SVD-based dimensionality reduction in multimedia databases by using aggregate index data and incremental updates, reducing computation time by 20x with minimal error.
The paper proposes restricted higher-order views to resolve schematic heterogeneity in data integration, enabling schema browsing, data independence, and semi-structured querying while minimizing query engine modifications.
The paper proposes WHIRL, a logic-based system that efficiently reasons about the similarity of name constants in databases using information retrieval techniques, demonstrating its accuracy and speed in integrating heterogeneous data without relying on global domain normalization.
The paper proposes a chunk-based caching method and a "chunked file" organization for OLAP systems to improve query performance by enabling fine-grained reuse of cached data and enhancing multidimensional query efficiency.
The paper presents three algorithms for optimizing multiple related dimensional queries in OLAP systems, introduces shared evaluation primitives, and demonstrates significant performance improvements through global optimization and common subtask sharing.
NoDoSE is an interactive, semi-automatic tool that helps users extract structured data from unstructured or semi-structured documents by hierarchically decomposing them and inferring their grammar, with a GUI and mining component to expedite the process.
The paper introduces concise and counting samples as efficient, incrementally maintainable sampling-based summary statistics for providing fast, approximate query answers in large data warehouses, demonstrating their advantages over standard sample views in accuracy and scalability, particularly for hot list queries under continuous data insertion.
This paper presents a framework for analyzing the space-time tradeoffs in bitmap indexing for selection queries, identifying optimal designs for time, space, and space-time efficiency, and evaluating the impact of compression and buffering.
The paper proposes a framework for parallel processing of user-defined functions in object-relational DBMS, focusing on partitionable functions and parallel sorting to enhance performance in decision support queries.
The paper explores the tradeoffs of using Java for user-defined functions (UDFs) in database extensibility, evaluating security, efficiency, and portability, and concludes that Java-based UDFs are performant but pose integration challenges.
The paper introduces a conservative error metric and optimal sampling bounds for equi-height histograms, proposes an adaptive page sampling algorithm, demonstrates the difficulty of distinct value estimation, and validates the approach with experiments on Microsoft SQL Server 7.0.
The paper presents a multiresolution wavelet-based technique for building space-efficient histograms that accurately approximate data distributions, enabling effective selectivity estimation and approximate OLAP query answers, even for correlated attributes, outperforming random sampling and prior methods.
The paper proposes an integrated method for high application availability in client-server systems by combining database server recovery with transparent application recovery using efficient message logging, minimizing forced I/Os and restart times while allowing independent server recovery.
The paper demonstrates that replacement selection with best-fit memory management achieves high efficiency (90% memory utilization and 1.8× longer runs) for variable-length records in external sorting, making it viable for commercial databases while reducing I/O overhead and improving scalability.
The paper introduces Microsoft Universal Data Access, a platform for multi-tier enterprise applications that enables efficient access to diverse data sources via OLE DB and ActiveX Data Objects (ADO).
The paper describes Oracle8's scalability mechanisms, enabling support for 50,000 concurrent users without middle-tier software through high concurrency, efficient resource sharing, parallel execution, and portability across systems.
The paper presents a fault-tolerant system for running parallel data mining applications on workstation networks, demonstrating its effectiveness with combinatorial pattern discovery and classification tree algorithms applied to protein sequence motifs and foreign exchange rate prediction.
The CQ project at OGI, funded by DARPA, develops a scalable toolkit for personalized update monitoring using continual queries, blending client-pull and server-push technologies.
The paper introduces WHIRL, a logic-based system for integrating heterogeneous databases by reasoning about the similarity of name constants in natural language text using the vector-space model from information retrieval, and demonstrates its effectiveness in real-world web data integration.
"RasDaMan is a domain-independent array DBMS supporting multidimensional arrays of any size and structure, offering SQL-based querying, efficient optimization, and storage with tiling and compression, used in geo and healthcare projects."
Xmas is an extensible main-memory storage system for embedded databases, offering core DBMS features, customizable high-level operations (composite actions), and improved performance, as demonstrated in a mobile communication simulation.
The paper introduces *MultiMediaMiner*, a prototype system for mining high-level knowledge (summarization, comparison, classification, association, clustering) from multimedia databases using a multimedia data cube for multidimensional analysis based on visual content.
SuperSQL extends SQL to enable query results to be formatted and presented in multiple output media (e.g., LaTeX, HTML, Excel) through a universal transformation mechanism based on tree-structured grouping and target-specific constructors.
The paper introduces the IDEA Web Laboratory, a web-based software design environment that demonstrates a new approach to software production on the Internet.
"DataSpot is a no-code tool that enables web designers and database developers to publish databases for web access, allowing non-technical users to query and navigate data using plain language and hypertext, powered by a schema-less Web View and dynamic HTML generation."
The paper proposes a novel **slot index spatial join (SISJ)** algorithm for efficiently joining two spatial datasets when only one is indexed by an R-tree, extends it to multiway spatial joins, and introduces a dynamic programming approach for optimizing complex spatial queries.
The paper proposes and evaluates several spatial selectivity estimation techniques for GIS queries, identifying the Min-Skew BSP partitioning as the most accurate and efficient method for estimating selectivity in point and range queries over two-dimensional rectangular data.
The paper presents a dynamic programming algorithm to find an optimal lattice path clustering for fact tables in data warehouses, introduces a "snaking" technique to further improve performance, and demonstrates their effectiveness through theoretical analysis and TPC-D benchmark experiments.
This paper introduces a refined write graph for redo recovery that enables more flexible cache management by flushing smaller sets of objects, allowing cost-effective recovery through optimized flush operations and generalized recovery LSNs.
The paper proposes two new lazy replica update protocols that ensure serializability with weaker data placement requirements and introduces a hybrid extension for broader applicability, demonstrating superior performance in distributed systems like data warehouses.
The paper presents an algebra for querying interactive multimedia presentation databases, supporting selection, join, and set operations on tree-structured data to locate or create presentations, along with equivalence proofs for query optimization.
This paper introduces quasi-succinctness to optimize pruning for constrained frequent set queries with 2-variable constraints, characterizes applicable constraints, proposes heuristic techniques for non-quasi-succinct cases, and presents a query optimizer ensuring efficiency in constraint checking and support counting.
The paper presents a wavelet-based method for efficiently computing approximate answers to high-dimensional OLAP aggregation queries on sparse data sets by constructing a compact data cube that enables fast, space-efficient, and progressively refinable query responses.
The paper proposes static and dynamic query optimization strategies for bitmap indexes in data warehouses, addressing both continuous and discrete selection criteria through optimal design, tree-based algorithms, logical reduction, and inclusion-exclusion approaches.
The paper compares nonparametric estimation methods—including histograms, kernel estimators, and a proposed hybrid approach—for selectivity estimation of range queries on metric attributes with large domains, highlighting the impact of sample size and smoothing parameters on accuracy and demonstrating that kernel estimators excel for continuous data while the hybrid method performs best on real-world datasets.
This paper presents two improvements to approximate quantile-finding algorithms: a method that eliminates the need for prior knowledge of input sequence length using non-uniform random sampling, and a space-efficient algorithm for estimating extreme quantiles by leveraging the advantages of random sampling over general quantile computation techniques.
The paper introduces ripple joins, a family of adaptive join algorithms for online aggregation queries that prioritize quick, statistically precise estimates over exact results, significantly reducing response times compared to traditional offline join methods.
The paper presents a query optimization algorithm for databases with limited access patterns, which searches and prunes annotated query plans efficiently using a best-first strategy to produce viable plans early.
The paper introduces the Iceberg-CUBE problem, proposes the BUC algorithm for efficient computation by pruning low-aggregate partitions, and demonstrates its superior performance over existing methods for both sparse and iceberg CUBEs.
The paper proposes a flexible *signature table* index for efficient similarity search in market basket data, enabling scalable peer recommendations based on customer buying behavior.
The paper presents a geometry-based solution using hierarchical approximations (cuboid and octree) for efficient similarity searches in large databases of 3D-volume objects, demonstrating significant performance improvements over existing methods.
The paper proposes **STORED**, a technique for mapping semistructured data (e.g., XML) to relational databases, enabling efficient storage and querying while leveraging data-mining for automatic schema generation and DTDs for optimization.
The paper presents a sound and complete algorithm for rewriting semistructured TSL queries using views, generalizing relational techniques like containment mappings, the chase, and query composition, while also supporting structural constraints like DTDs.
The paper presents a fast and accurate heuristic-based method for identifying record boundaries in unstructured or semistructured web documents by analyzing HTML tag trees and combining multiple heuristics to select consensus separator tags.
The paper compares INFORMIX IDS/UD 9.2 with SQL-99, highlights its unique features, and argues that further improvements are needed in object-relational DBMSs despite SQL-99's advancements over SQL-92.
The paper describes Microsoft Repository's version and workspace features, which enable fine-grained object management, version control (including branching, merging, and delta storage), and compatibility with version-unaware applications on SQL Server.
The paper discusses the challenges and experiences in developing Telecom Italia's enterprise integrated database and data warehouse, highlighting the integration of over 50 internal databases and the creation of a dedicated data store for customer and traffic analysis.
The paper proposes a distributed Java-based database system that dynamically integrates external data, functions, and computing resources from diverse providers over the Internet, enabling flexible query execution with externally supplied operators.
The paper introduces the EVE system, which maintains data warehouses over dynamic, autonomous information sources by using view synchronization and adaptation techniques to handle schema changes and evolving data without extensive human intervention.
The paper introduces the SERF framework, a flexible and reusable system for complex database restructuring beyond simple schema evolution, including enhancements like SERF Templates for portability and a semantic optimizer for improved performance.
Aqua is a system that provides fast, approximate answers with quality guarantees for aggregate OLAP queries by precomputing and maintaining synopses of data in a relational DBMS.
The Cornell Jaguar Project investigates ubiquitous query processing by eliminating client-server boundaries and adapting database techniques for mobile devices, extending the PREDATOR engine.
The paper introduces Jungle, a database search engine prototype that extracts and indexes database data and meta-data via JDBC to enable information retrieval using the intuitive AQUA query language, addressing limitations of current search engines in accessing database content.
The paper *Delaunay MM* presents an interactive interface with pre- and post-query refinement and customizable multimedia display to improve information retrieval in digital media.
The paper introduces the snoop protocol, which improves TCP performance over wireless networks by caching and locally retransmitting packets at the base station, achieving up to 20× throughput gains compared to standard TCP.
The paper proposes an efficient FP-tree structure and FP-growth algorithm for mining frequent patterns without candidate generation, significantly outperforming Apriori and other methods in speed and scalability.
The paper proposes a "full speed" online backup method that is loosely coupled with the cache manager, supports general logical log operations, and ensures media recoverability through additional logging, while also reducing logging overhead for constrained log operations.
The paper introduces **DR**, a lightweight resumption algorithm for data warehouse loads that reduces resumption time by tenfold without imposing overhead or relying on transformation specifics.
The paper proposes a scalable, generalized projected clustering method for high-dimensional data by identifying clusters in arbitrarily aligned lower-dimensional subspaces specific to each cluster, using extended cluster feature vectors to enhance efficiency.
The paper proposes **Density Biased Sampling**, a method that under-samples dense regions and over-samples sparse regions in large datasets to better preserve small clusters, outperforming uniform sampling by up to six times in effectiveness.
The paper introduces the **Local Outlier Factor (LOF)**, a measure quantifying an object's degree of outlierness based on its local neighborhood, demonstrating its effectiveness in identifying meaningful outliers compared to binary outlier detection methods.
The paper presents a novel algorithm for rewriting complex SQL queries using materialized views by employing a graphical representation and bottom-up node matching to handle joins, arithmetic operations, multidimensional aggregation, and nested subqueries.
The paper presents an algorithm for incremental materialized view maintenance that performs refreshes as small, asynchronous steps to reduce contention and supports point-in-time refresh.
The paper introduces XMill, an XML data compression tool that combines zlib, datatype-specific compressors, and optional user-defined compressors to achieve roughly double the compression ratio of gzip at similar speeds.
The paper introduces a power law governing spatial join selectivity, termed the "pair-count exponent" (PC), which holds for both self-joins and distinct point sets across diverse real-world datasets, and proposes the efficient BOPS method to compute it in linear time, enabling constant-time selectivity estimates with low error.
The paper introduces and formalizes the concept of influence using reverse nearest neighbor (RNN) queries, presents an efficient R-tree-based method for large datasets, and demonstrates its superior scalability and effectiveness compared to traditional range and nearest neighbor approaches.
The paper introduces a dynamic self-tuning, index-based method for efficient online data reorganization in parallel database systems, demonstrating its scalability and effectiveness in maintaining system throughput through simulations and empirical results on the Fujitsu AP3000.
The paper demonstrates that heuristic-based multi-query optimization is practical and beneficial, proposing three cost-based algorithms (Volcano-SH, Volcano-RU, and a greedy heuristic) that significantly reduce evaluation costs with acceptable overhead, as validated by TPC-D benchmark workloads.
The paper introduces *eddies*, a dynamic query processing mechanism that continuously reorders query operators during execution to adapt to fluctuating resource conditions in federated databases, outperforming static optimization in dynamic environments while remaining competitive in static ones.
The paper proposes a formal framework for combining user preferences to enhance search queries and order results, offering flexibility through a generic combine operator and demonstrating its applicability in personalization systems and real-life applications.
The paper describes how Microsoft TerraServer transforms and stores terabytes of geo-spatial images into an SQL database for efficient online access via standard web browsers, demonstrating the feasibility of relational databases for large-scale image repositories.
The paper proposes a data model for moving objects databases, introducing a sliced representation to handle evolving spatial structures and demonstrating its mapping to physical data structures in a DBMS.
The paper proposes an R*-tree-based indexing technique for efficiently querying current and projected future positions of moving objects in one-, two-, or three-dimensional space, including dynamic updates and performance evaluation.
The paper introduces the Onion technique, an indexing structure based on layered convex hulls to efficiently answer linear optimization queries by progressively retrieving top-N records from outer layers inward, significantly outperforming sequential scans for small N.
The paper presents an automatic content-based video organization and indexing framework using camera-tracking-based shot segmentation, hierarchical browsing, and variance-based similarity retrieval.
The paper proposes a method to build accurate decision-tree classifiers from perturbed data by reconstructing the original data distribution, achieving comparable accuracy to models trained on unperturbed data while preserving privacy.
The paper presents techniques for maintaining cube materialized views (Automatic Summary Tables) in IBM DB2 UDB to enhance aggregation query performance.
The paper proposes a three-layer representation (external, conceptual, internal) for business rules, similar to the ANSI/SPARC architecture, and is developing a concise scheme to formalize rule derivation and enable automated conflict analysis.
The paper describes Telcordia's network traffic warehouse, detailing its design, challenges, and applications in SLA monitoring, web traffic analysis, capacity planning, and billing.
The paper discusses the impact of the XML revolution on database systems research and development.
The paper introduces **FACT**, a system that enhances search results by navigating from initial URLs to find pages containing queried segments using learned navigation and classification knowledge, without requiring user profiles or page preprocessing, and evaluates its performance with different training strategies.
The i³ project enhances OLAP systems with advanced operators (DIFF, RELAX, INFORM) to automate and simplify complex data exploration tasks traditionally done manually.
The ESPRIT Project DWQ developed logic-based and quantitative techniques to enhance data warehouse quality by improving semantic foundations, optimizing design and operational processes, and demonstrating their integration in a real-world Telecom Italia application.
MLPQ/GIS is a constraint database system specializing in spatio-temporal data, offering tools for data entry, icon-based queries (e.g., intersection, union, optimization), Datalog queries, animation of linear constraint relations, and visualization of spatially distributed variables.
The paper proposes efficient methods, including Top-*k* Apriori, Top-*k* BUC, and Top-*k* H-Cubing, for computing iceberg cubes with complex measures like *average*, leveraging anti-monotonic pruning and a hypertree structure (H-tree) to improve scalability, with Top-*k* H-Cubing showing superior performance.
The paper proposes single-pass, space-efficient algorithms for approximating correlated aggregates (e.g., percentage of international calls longer than the average domestic call duration) over data streams, demonstrating their accuracy through experiments and theoretical analysis of monotonicity and convergence properties.
The paper introduces an automated method for classifying web-accessible databases into hierarchical schemes using query probes, achieving high accuracy without retrieving documents.
The paper proposes a scalable hierarchical clustering approach using Data Bubbles—compressed data representations—to efficiently approximate clustering structures in large datasets while maintaining high accuracy.
The paper proposes a two-phase rule induction method for classifying rare target classes, which separately optimizes high recall and high precision, outperforming RIPPER and C4.5 rules in handling splintered false positives and error-prone small disjuncts on synthetic and real-world datasets.
The paper proposes **TANGO**, a temporal query processing middleware that optimizes and executes temporal SQL queries by dynamically partitioning them between algebraic operations (handled by the middleware) and conventional SQL (processed by the underlying DBMS), leveraging cost-based feedback for adaptive performance.
The paper proposes the CR-tree, a cache-conscious R-tree variant that compresses MBR keys to increase node fanout, reducing tree height and improving search performance by up to 2.5× while using 60% less memory than traditional R-trees.
The paper proposes **pkT-trees** and **pkB-trees**, which reduce CPU cache misses in main-memory indexes by storing partial-key information, improving performance for OLTP workloads with large keys.
The paper proposes a metasearch engine method that incorporates document linkage information into database representatives to improve database selection and retrieval effectiveness, establishing optimal ranking conditions and an efficient estimation approach.
The paper proposes a novel histogram-based synopsis method that combines a statistical interaction model to capture attribute correlations with low-dimensional histograms to accurately approximate high-dimensional joint data distributions, effectively addressing the "dimensionality curse."
The paper proposes **Prefetching B+-Trees (pB+-Trees)**, which use prefetching to accelerate B+-Tree searches by creating wider nodes to reduce tree height and speed up range scans via leaf pointer arrays, achieving 1.21-1.5× faster searches and over 6× faster range scans.
The paper introduces **PREFER**, a system that efficiently answers preference queries—which optimize object selection based on weighted attributes—by leveraging pre-processed materialized views, outperforming prior methods in both preprocessing and execution time.
The paper presents a framework for efficiently maintaining materialized views by optimizing shared subexpressions, selecting materialized expressions and indices, and choosing between incremental or recomputation plans, significantly improving maintenance performance.
The paper presents algorithms for generating efficient query rewritings using views under the closed-world assumption, ensuring optimal performance by minimizing subgoals or intermediate relation sizes, and demonstrates their effectiveness through experiments.
The paper presents a parameterized adaptive caching algorithm that dynamically adjusts the precision of cached approximations (e.g., intervals for numeric values) to optimize performance, outperforming exact-caching algorithms when bounded imprecision is acceptable while matching their performance when exact caching is required.
The paper introduces the *Epsilon Grid Order* algorithm, an efficient and scalable method for computing similarity joins in large multidimensional datasets by using a grid-based sorting approach and external sorting to avoid high memory consumption.
The paper presents an efficient algorithm for answering approximate and exact multi-dimensional aggregate queries (e.g., SUM, COUNT) using selective traversal of a Multi-Resolution Aggregate (MRA) tree, providing confidence intervals and iterative refinement until error or time constraints are met.
The paper explores the performance of implementing XML containment queries in relational database systems versus inverted list engines, finding that with optimizations, RDBMSs can outperform specialized IR engines by improving join algorithms and cache utilization.
The paper presents a scalable monitoring system for processing large flows of XML and HTML documents, featuring a subscription language, efficient architecture, and a novel alert algorithm, implemented within the Xyleme project to handle millions of pages and subscriptions daily.
The paper proposes a novel sampling technique based on von Neumann's golden rule for accurate range query size estimation, outperforming Min-Skew histograms and wavelet methods in accuracy while being easily extendable to higher dimensions.
The paper presents scalable, communication-efficient, and robust distributed algorithms for mining association rules in large transactional databases partitioned across multiple machines, addressing the limitations of prior approaches like CD and FDM.
The paper *"A data-integration system provides access to a multitude of data sources through a single mediated schema"* introduces **LSD**, a machine-learning-based system that semi-automatically generates semantic mappings between source schemas and a mediated schema by training multiple learners on initial user-provided mappings and combining their predictions to improve accuracy, while incorporating domain constraints and XML structural information.
The paper introduces StorHouse/Relational Manager, a database system that directly executes SQL queries across an active storage hierarchy (disk, optical, tape) without file post-processing, proposes an Atomic Data Store for historic data, a Hub-and-Spoke Data Warehouse architecture, and demonstrates federation with other databases using logical partitioning techniques.
The paper proposes the CachePortal system, which enables dynamic content caching for database-driven e-commerce sites by intelligently invalidating cached web pages when underlying database content changes.
The paper presents a scalable and reliable data management system for distributed networks, inspired by Internet routing protocols, featuring a dynamic database index and optimized schema design, initially applied to postal logistics with potential for sensor networks.
The paper defines content integration for E-Business, distinguishing it from traditional data and application integration, explores its challenges and required services, and examines architectural solutions and XML's role.
The paper describes the development of the "Aviation Safety Data Mining Workbench," a tool designed to enhance aviation safety analysis by applying data mining techniques to improve the efficiency and depth of incident data processing.
The paper discusses the challenges and research issues in designing Nimble Technology's XML-based data integration product, which addresses the growing need for interoperable data exchange across diverse systems.
The paper demonstrates how combining orthogonal primitives for correlation removal and efficient GroupBy/outerjoin processing enables syntax-independent, optimized query execution for subqueries and aggregations, as evidenced by superior TPC-H benchmark performance in Microsoft SQL Server.
The paper introduces Oracle's **Fast-Start Fault Recovery**, a feature that minimizes database downtime by optimizing checkpointing for faster crash recovery and enabling concurrent transaction rollback with minimal performance impact.
The paper presents a perception-based image retrieval (PBIR) system that accurately measures image similarity by leveraging human perception and learning user query concepts through intelligent sampling, achieving effective results with minimal labeled examples.
The paper presents a spatial database integration for CAD applications, enabling interactive spatial queries on large product databases for digital mockup and haptic rendering.
The paper presents a scalable, hash-based IP traceback system that efficiently identifies the origin of individual IP packets with minimal storage overhead (0.5% of link capacity).
The paper proposes route-based distributed packet filtering (DPF) as a scalable and proactive defense against DDoS attacks, leveraging power-law Internet topology to effectively filter spoofed traffic and localize attack origins with minimal deployment on under 20% of AS sites.
The paper proposes *KeyGem*, a scalable group key management system using periodic batch rekeying and proactive FEC to improve performance and reliability in secure group communications.
The paper analyzes how the age of cached content affects performance in distributed web systems, evaluates the impact of cache hierarchies and inter-request times, and explores strategies like pre-term refreshes to reduce client misses.
The paper demonstrates that End System Multicast, when adapted for latency and bandwidth, effectively supports high-performance conferencing applications in dynamic, heterogeneous Internet environments.
The paper introduces IPNL, a NAT-extended IPv4 architecture that addresses address depletion while maintaining IPv4's core features, enabling site isolation and multi-homing without modifying existing routers or polluting routing tables.
The paper investigates how Differentiated Services (Diff-Serv) and Expedited Forwarding (EF) impact video streaming quality by experimentally measuring application-level performance using modified servers, clients, and a standardized video quality tool across controlled and real-world QoS-enabled networks.
The paper proposes scalable protocols (RPB and RBS) for reliable on-demand streaming media delivery, optimizing performance and efficiency based on client characteristics and media quality requirements.
The paper analyzes the statistical throughput of elastic document transfers under dynamic bandwidth sharing, demonstrating that mean throughput is insensitive to flow size and arrival distributions when sessions are Poisson, and highlights the impact of demand relative to capacity on performance.
The paper proposes the Adaptive Virtual Queue (AVQ) algorithm for Active Queue Management, analyzing its stability, queue control, and robustness, and demonstrates its effectiveness compared to other AQM schemes while offering a simple implementation.
The paper develops efficient algorithms for provisioning Virtual Private Networks (VPNs) in the hose model, optimizing bandwidth usage by constructing tree-based VPN structures and demonstrating significant improvements over Steiner tree approaches.
The paper introduces *Chord*, a scalable distributed lookup protocol that efficiently maps keys to nodes in a peer-to-peer network, adapting dynamically to node changes while maintaining logarithmic communication and state overhead.
The paper introduces the Content-Addressable Network (CAN), a scalable, fault-tolerant, and self-organizing distributed system that provides hash table-like functionality for large-scale networks.
The paper proposes and evaluates three techniques (Geo Track, GeoPing, and GeoCluster) for mapping IP addresses to geographic locations, highlighting their performance and inherent challenges in accurately determining host locations.
The paper introduces a multi-server fair queuing service discipline that approximates Generalized Processor Sharing (GPS), provides performance guarantees, and analyzes its delay and service discrepancy for applications like Ethernet link aggregation.
The paper introduces **Aggregated Bit Vector (ABV)**, a scalable packet classification algorithm that improves upon the bit vector search (BV) method by using recursive aggregation and filter rearrangement to achieve logarithmic time complexity, demonstrating significant performance gains over BV in simulations.
The paper introduces Smoothed Round Robin (SRR), a fair queueing scheme that improves burstiness and short-term fairness in scheduling by using a Weight Matrix and Weight Spread Sequence (WSS) while maintaining O(1) time complexity, making it suitable for high-speed networks and QoS provisioning.
The paper analyzes the stability of OSPF under steady and perturbed conditions by evaluating convergence times, routing load, and route flaps across different deployment scenarios (TE extensions, subsecond HELLO timers, and alternative refresh strategies) using simulations on a 292-node ISP network.
The paper presents a minimal-knowledge algorithm for discovering bridged Ethernet LAN topologies using SNMP MIBs, requiring only three shared host forwarding entries between bridges and access to one endpoint, enabling scalable and accurate network mapping.
The paper proposes HBH, a hop-by-hop multicast routing protocol that uses recursive unicast trees to support unicast clouds and address routing asymmetries, outperforming other protocols in delay and bandwidth efficiency.
The paper investigates the safety and performance of slowly-responsive, TCP-compatible congestion control algorithms under dynamic network conditions, examining fairness, utilization, and rate stability.
The paper introduces TFMCC, a TCP-friendly multicast congestion control protocol that extends TFRC to multicast by addressing scalable feedback, round-trip time measurement, and fairness while preventing feedback implosion.
The paper introduces TBIT, a tool for analyzing TCP behavior in web servers, and presents findings on server TCP implementations, including bugs and non-compliance with congestion control standards.
The paper proposes a sketch-based method using randomized techniques and intelligent domain partitioning to approximate complex aggregate SQL queries over data streams with limited memory, offering provable error guarantees and improved accuracy over histograms.
The paper proposes a cooperative best-effort cache synchronization policy between data sources and caches to minimize divergence with low overhead, demonstrating its effectiveness through simulations.
The paper presents a dynamic proxy caching technique that combines the benefits of proxy-based and back-end caching by enabling granular caching of both dynamic content and layout, significantly reducing bandwidth and response times in real-world web applications.
APEX is an adaptive XML path index that improves query performance by dynamically indexing frequently used paths and supporting incremental updates, outperforming traditional indexes by 2 to 54 times, especially for irregular XML data.
The paper demonstrates that the forward-and-backward index serves as a covering index for branching path queries in XML data, shows its optimality for full query coverage despite practical size limitations, and proposes smaller, specialized indexes for improved performance on restricted query classes.
The paper demonstrates that XML's ordered data model can be efficiently supported in relational databases by encoding order as data values, proposing three encoding methods and algorithms for translating ordered XPath to SQL, and validating their performance experimentally.
The paper introduces *Bellman*, a system that mines database structure to identify similar fields, join paths, and database structures, aiding in data preparation, schema mapping, and complexity management.
The paper proposes a scalable parallel hash ripple join algorithm that improves convergence speed through sampling and maintains performance during memory overflow, outperforming the original hash ripple join and matching traditional parallel hybrid hash join in efficiency.
The paper presents improved algorithms for minimizing tree pattern queries (TPQs) in XML and LDAP directories, achieving \( O(n^2) \) complexity for cases without integrity constraints and with required-child/descendant constraints, and \( O(n^4) \) for cases with subtype constraints, using graph simulation.
The paper proposes a framework for efficiently processing time-parameterized spatio-temporal queries—including window queries, k-nearest neighbors, and spatial joins—by reducing them to nearest neighbor searches and leveraging branch-and-bound techniques with R-tree indexing.
The paper proposes **Algorithm MPro**, an optimal method for minimizing expensive predicate evaluations in top-k queries by determining and executing only necessary probes, significantly improving efficiency over traditional approaches.
The paper proposes a method using Fast Fourier Transform (FFT) and prediction techniques to efficiently monitor and filter nearest or near-neighbor time series patterns in a streaming context, significantly improving response times.
The paper proposes the pCluster model for clustering objects based on coherent patterns across subsets of dimensions rather than traditional distance metrics, and presents an effective algorithm for detecting such clusters in large datasets.
The paper introduces **Probabilistic Wavelet Synopses**, a wavelet-based data reduction technique that uses probabilistic thresholding to provide accurate approximate query answers with guaranteed error bounds, addressing the limitations of deterministic wavelet methods.
The paper introduces *workload compression* to improve scalability in SQL workload-driven tasks like index selection and approximate query processing by reducing workload size while preserving essential characteristics.
The paper introduces a Patricia tree-based B-tree variant for OLTP systems, implemented in iAnywhere ASA Version 8.0, which offers improved space efficiency and performance over traditional indexes.
The paper describes how IBM Informix Extended Parallel Server (XPS) efficiently implements join operations in a star schema for data warehouses and data marts.
The paper compares the performance of Quadtree and R-tree index structures in Oracle Spatial for GIS data, evaluating their efficiency in various queries and operations.
The paper presents an automated method for optimizing physical database design in parallel databases by leveraging the query optimizer to recommend and evaluate data partitioning strategies for improved workload performance, with rank-based enumeration outperforming random-based approaches.
The paper introduces TPC-DS, a more realistic decision support benchmark for retail product suppliers, featuring complex snowflake schemas, skewed data, and a diverse query set.
The paper introduces TimesTen, an in-memory mid-tier data manager that enables applications to cache subsets of a disk-based database, ensuring low latency, high throughput, and synchronization between caches and the back-end database.
The paper proposes **XCache**, a semantic caching system for XML queries that uses tree automata and regular expression subtyping to enable efficient XQuery containment checking and rewriting, improving performance by reducing data transmission costs.
The paper *COUGAR: the network is the database* presents a distributed data management system for sensor networks that enables in-network processing and flexible querying without centralized computation, improving scalability and efficiency.
**Summary:** CubeExplorer is an integrated system for efficient and effective online exploration of large data cubes, combining iceberg cube computation, feature extraction, and gradient analysis techniques.
The paper introduces the concept of Software as a Service (SaaS) through Application Service Providers (ASPs) and ASP aggregation, explaining how hosted applications are delivered over the internet and how aggregation enables single access and sign-on for multiple services.
The paper reveals that BGP misconfigurations are frequent (200-1200 daily prefixes affected) but rarely disrupt end-user connectivity, with most caused by preventable router design flaws.
The paper analyzes routing anomalies unique to Internal BGP (IBGP), demonstrates that determining correct IBGP configuration is NP-hard, and provides sufficient conditions to ensure anomaly-free operation.
The paper presents *rtg*, an automated tool that generates realistic BGP routing tables and updates for small-scale test labs, enabling scalable and repeatable studies on BGP behavior, routing instability, and forwarding performance.
The paper proposes erasure-resilient encoding and efficient peer collaboration algorithms to optimize throughput in overlay networks for large transfers, leveraging digital fountain encoding and minimizing communication overhead.
The paper proposes **Secure Overlay Services (SOS)**, a proactive architecture combining secure overlay tunneling, consistent hashing, and filtering to significantly reduce the likelihood of successful DoS attacks by obscuring target paths and pushing attack mitigation to high-capacity network cores.
The paper proposes an overlay-based Internet Indirection Infrastructure (I3) that uses identifier-based rendezvous communication to decouple sending and receiving, enabling efficient support for multicast, anycast, and mobility services.
The paper introduces the eXplicit Control Protocol (XCP), a congestion control method that outperforms TCP by remaining efficient, fair, and stable at high bandwidth-delay products while decoupling utilization and fairness control.
The paper analyzes the TCP-friendliness of equation-based rate control under random losses, showing that conservativeness depends on the convexity of the rate function, loss variability, and correlation structure, with guidelines for when the control remains TCP-friendly.
The paper investigates whether selfish behavior by Internet endpoints threatens network stability, finding that while older TCP and drop-tail queueing systems remain efficient at Nash equilibria, modern TCP variants with RED or drop-tail queues lead to inefficiency, though a simple stateless mechanism like CHOKe can restore stability.
The paper presents efficient Internet mapping techniques that significantly reduce measurement overhead while accurately capturing router-level ISP topologies, and shares detailed maps of ten diverse ISPs, revealing key structural properties.
This paper argues that network generators based on power-law degree distributions better capture the Internet's large-scale hierarchical structure than traditional hierarchical generators like Transit-Stub and Tiers, despite initial assumptions favoring the latter.
The paper evaluates three existing techniques for estimating POP-to-POP traffic matrices, identifies their limitations, and proposes a new approach using choice models to improve accuracy by incorporating POP-specific data.
The paper demonstrates that the optimal P2P data replication strategy lies between uniform and proportional replication, outperforming both, and can be achieved through simple distributed algorithms.
The paper introduces Wave and Equation Based Rate Control (WEBRC), an equation-based multicast congestion control protocol that ensures TCP fairness with minimal rate fluctuations, utilizes a novel multicast round trip time (MRTT) for receiver rate adjustment, and employs periodic data transmission waves to optimize performance and reduce overhead.
The paper introduces a scalable, hierarchical application-layer multicast protocol for low-bandwidth streaming that reduces link stress by 25%, maintains low latency and control overhead, and performs robustly in simulations and wide-area tests with groups of 32-100 members.
The paper demonstrates that route flap damping, intended to stabilize BGP routing, can excessively delay convergence for stable routes due to its interaction with BGP path exploration, and proposes a modification to mitigate this issue.
The paper demonstrates that determining convergence in I-BGP with route reflection is NP-Complete, proposes a modified protocol ensuring guaranteed convergence and stability independent of message order.
The paper introduces the Single-Buffered (SB) router model to generalize router architectures beyond CIOQ, analyzes various SB architectures using Constraint Sets, and identifies the Distributed Shared Memory router as a promising alternative with potential performance and efficiency benefits.
The paper introduces ephemeral state processing (ESP), a router-based building block designed to provide IP-like generality, simplicity, and scalability for supporting end-to-end network services by allowing packets to create and manipulate temporary router state through predefined computations.
The paper proves that the computational complexity lower bound for packet scheduling algorithms to achieve tight GPS-relative and end-to-end delay bounds is Ω(log₂n), even when the delay bound is relaxed to O(nᵃ) for 0 < a ≤ 1, and extends these results to stronger computational models.
The paper introduces **SLoPS (Self-Loading Periodic Streams)**, an end-to-end method for measuring available bandwidth (avail-bw) by analyzing one-way packet delays, implemented in the non-intrusive tool **pathload**, which demonstrates that avail-bw variability increases in congested or low-capacity paths and shows a rough correlation with TCP throughput, though TCP measurement is more intrusive.
The paper analyzes Internet flow rate distributions, finding they are less skewed than flow sizes and correlated for large flows, and identifies network congestion and receiver window limits as primary causes using the T-RAT tool.
The paper introduces two memory-efficient algorithms, sample and hold and multistage filters, for accurately identifying large network flows above a threshold, significantly improving accuracy over traditional sampling methods while enabling scalable traffic measurement and threshold-based accounting.
The paper argues that applying end-to-end design principles to network storage, exemplified by the Internet Backplane Protocol and exNode in Logistical Networking, ensures scalability without contradicting the Internet's architectural foundations.
The paper argues that the Internet's technical architecture must evolve to accommodate conflicting stakeholder interests ("the tussle") and proposes design principles to address this challenge.
The paper proposes the TIX bulk-algebra to integrate information retrieval-style querying with database-style evaluation in XML databases, introducing efficient scoring methods like TermJoin for improved performance.
The paper presents **XRANK**, a system for efficient ranked keyword search over XML documents, addressing challenges like nested element retrieval, granular ranking, and hierarchical keyword proximity, while also generalizing to handle both XML and HTML documents.
The paper presents a distributed top-*k* monitoring approach that reduces communication costs by using arithmetic constraints at remote data sources to maintain approximate top-*k* results within a specified error tolerance, significantly lowering overhead compared to centralized methods.
The paper proposes SQL extensions for array-based calculations in OLAP environments, along with optimizations and execution models, to address the limitations of traditional SQL in handling complex numerical computations efficiently.
The paper introduces *winnowing*, a local document fingerprinting algorithm that efficiently detects partial copying and performs within 33% of a proven lower bound for such algorithms, with validation from real-world Web data and the MOSS plagiarism detection system.
The paper formalizes minimal information sharing across private databases and develops secure protocols for intersection, equijoin, and their sizes, enabling new applications while ensuring no extra data is revealed beyond query answers.
The paper proposes two models (interval and position) and methods for estimating containment join sizes in XML query optimization, demonstrating their effectiveness through performance studies.
The paper proposes efficient inverted-file-based join algorithms for set-valued attributes in object-oriented and object-relational databases, comparing them with signature-based methods for set containment, equality, and overlap joins.
The paper introduces **envelope transforms** to enhance Dynamic Time Warping (DTW) indexing, improving both retrieval precision and speed in Query by Humming systems by treating music as a time series and optimizing dimensionality reduction techniques.
The paper proposes a two-step schema matching technique using attribute correlations and graph matching to address opaque column names and data values, validated through experimental results.
The paper proposes a space-efficient algorithm using "2-level hash sketches" to estimate the cardinality of set expressions (e.g., union, intersection, difference) over continuous update streams with deletions, providing low-error, high-confidence results without rescanning past data.
The paper proposes AIG, a framework for integrating relational data into XML documents that conform to a DTD and satisfy constraints, using semantic attributes, SQL queries, and optimization techniques.
The paper proposes an efficient similarity search and classification method using rank aggregation, where multiple voters rank database elements based on random projections, and the best median rank is selected, achieving high accuracy with low computational cost.
The paper characterizes how inaccurate storage access cost parameters can lead to suboptimal query plans in relational query optimizers and demonstrates the importance of accurate cost information for optimal performance.
The paper presents a theory of redo recovery by introducing an installation graph to define recoverable system states and establishing an invariant between normal operation and recovery, explaining how common techniques achieve correct redo recovery.
The paper presents a formal operational semantics, type system, and effect analysis for an OQL-like query language with object creation and external method calls, proving soundness and enabling optimization and non-determinism detection.
The paper proposes a lazy deterministic pushdown automata (XPush Machine) to efficiently process large numbers of XPath queries with predicates on XML streams, achieving high throughput (0.5–4.5 MB/sec) through optimized space and time techniques.
The paper presents XSQ, a memory-efficient system for processing XPath 1.0 queries on streaming XML data using pushdown transducers with buffers, achieving high throughput while minimizing buffering.
The paper proposes a method for mobile clients to validate previous spatial query results using server-provided "validity regions," reducing query frequency for nearest neighbor and window queries with minimal overhead.
The paper proposes a hardware-accelerated approach using modern graphics hardware to efficiently perform spatial selections and joins by leveraging rendering and searching capabilities, reducing computational costs without requiring data pre-processing or storage changes.
The paper introduces **BBS (Branch-and-Bound Skyline)**, an **IO-optimal, progressive algorithm** for skyline queries that outperforms the existing **NN (Nearest Neighbors)** method by eliminating duplicates, reducing node accesses, and minimizing space overhead while efficiently returning initial results.
The paper presents the design of TinyDB, an acquisitional query processor for sensor networks that optimizes data collection by controlling where, when, and how often data is sampled, significantly reducing power consumption compared to traditional passive systems.
The paper presents a scalable system for executing XPATH queries on wide-area sensor databases by logically treating distributed sensor data as a single XML document, using dynamic caching and a novel query-evaluate-gather technique to optimize performance.
The paper presents an algorithm for composing XSL transformations with XML views of relational databases to optimize execution by eliminating the need for XSLT processing.
The paper proposes a dynamic sample selection method for approximate query processing that adaptively combines pre-constructed non-uniform samples to provide more accurate query-specific approximations than static sampling approaches.
The paper proposes a framework called *velocity density estimation* for diagnosing and visualizing trends (dissolution, coagulation, shift) in fast-evolving data streams using efficient online and batch methods with linear computational complexity.
The paper proposes an efficient similarity search method for voxelized CAD objects using sets of feature vectors, introducing a new distance measure and query acceleration technique that improves result relevance and speed.
The paper argues that efficient XML publishing requires relational engines to support relation-valued variables via a new GApply operator, which enhances performance beyond simple sorting and tagging.
This paper presents a method for translating XQuery to SQL using dynamic interval encoding to efficiently handle nested FLWR expressions, element constructors, and built-in functions while preserving document order and enabling optimal relational query plans.
The paper introduces **multi-dimensional clustering (MDC)** in DB2 Universal Database Version 8, a data layout scheme that organizes tables based on multiple orthogonal clustering attributes to improve query performance, particularly for OLAP and data warehousing, by reducing random I/O and enhancing range query efficiency.
The paper introduces *WinMagic*, a technique that uses window aggregation to eliminate subqueries and redundant table accesses, improving performance in correlated and non-correlated SQL queries, as demonstrated in IBM DB2 implementations.
The paper introduces **TIMBER**, a native XML database system that enables efficient set-at-a-time query processing with algebraic rewriting and cost-based optimization, addressing limitations of relational mappings for XML data.
The paper proposes a *Knowledge Plane* for the internet—a pervasive, AI-driven system that autonomously manages, diagnoses, and repairs networks using high-level cognitive models rather than traditional algorithms.
The paper proposes a shared routing underlay for overlay networks, advocating cost-aware, layered primitives to replace independent probing, and demonstrates its utility through basic operations and library services.
The paper explores energy-saving strategies for Internet networking devices by analyzing the feasibility of putting components to sleep and proposing protocol modifications to support such measures, emphasizing their potential global impact, especially in energy-scarce regions.
The paper proposes a delay-tolerant network architecture for challenged internets, using an overlay with asynchronous message forwarding, in-network storage, and interoperable naming to address intermittent connectivity and resource constraints.
The paper introduces a dynamic traffic-aware routing algorithm called PBR, which uses scalar potentials and steepest gradient search to route packets around congestion while maintaining loop-free paths, local computations, and low overhead, demonstrating improved delay and jitter over shortest-path routing in simulations.
The paper presents an algebraic framework for analyzing path vector protocols like BGP, showing that convergence and path optimality depend on the monotonicity and isotonicity of the path compositional operation, and applies this to both performance-oriented and policy-based routing scenarios.
The paper proposes design principles for policy languages in path-vector protocols to prevent routing anomalies while maintaining expressiveness, addressing risks from policy interactions in decentralized networks like BGP.
The paper demonstrates that low-rate TCP-targeted DoS attacks can throttle TCP flows by exploiting retransmission time-outs while evading detection, and explores randomized time-out mechanisms as a potential defense.
The paper proposes DELTA and SIGMA, a dynamic key-based solution to prevent unfair bandwidth allocation in multicast congestion control by securing group access against inflated subscriptions.
The paper proposes a framework for classifying DoS attacks as single- or multi-source using header analysis, ramp-up behavior, and spectral analysis, demonstrating its effectiveness through real-world detection and validation.
The paper investigates the causes of Internet path inflation, attributing it primarily to inter-domain routing policies and peering practices rather than intra-domain traffic engineering, and highlights the lack of BGP controls for optimizing cross-ISP paths.
The paper introduces **ARAM**, a model that predicts routing table structure by simulating address allocation and routing practices, validates it against historical data, and evaluates its implications for IP lookup scalability, suggesting multibit tries may outperform TCAMs as tables grow.
The paper presents *AutoFocus*, a method for automatically clustering network traffic into minimal, meaningful groups based on consumption patterns, dynamically identifying dominant and unusual usage without prior knowledge of traffic structure.
The paper demonstrates that selfish routing in Internet-like environments achieves near-optimal average latency but increases congestion on specific links and undermines traffic engineering predictability.
The paper presents an efficient algorithm for content-based forwarding in networks where message routing is determined by evaluating predicates against message content, optimizing speed and performance for high-volume, complex scenarios.
The paper introduces *pSearch*, a decentralized P2P information retrieval system that uses Latent Semantic Indexing to organize document indices semantically, reducing search costs while maintaining performance comparable to centralized systems.
The paper proposes scalable, high-throughput router architectures using optics to address issues like packet mis-sequencing, traffic patterns, rapid fabric configuration, and fault tolerance, enabling a practical 100Tb/s router design.
The paper introduces a Bloom filter-based algorithm for Longest Prefix Matching (LPM) in IP routing, offering better performance and scalability than TCAM-based approaches, with constant lookup speed for IPv4 and IPv6 using minimal memory.
The paper introduces **HyperCuts**, a packet classification algorithm that uses a decision tree with **k-dimensional hypercube nodes** instead of hyperplanes, achieving **significant improvements in memory efficiency (2–10× less than HiCuts) and search speed (50–500% faster worst-case performance)**, while enabling **full pipelining and fast updates**.
The paper describes the implementation and testing of the DARPA Quantum Network, the world's first operational quantum key distribution (QKD) system for secure end-to-end communication, including its physical layer, cryptographic protocols, and IPsec integration.
The paper introduces **Stratified Round Robin**, a low-complexity, hardware-friendly packet scheduler that ensures bandwidth fairness and provides a bounded delay independent of the number of flows, outperforming other schedulers of similar complexity.
The paper compares hard-state and soft-state signaling protocols, analyzing their trade-offs in consistency and overhead, and finds that soft-state with explicit removal and reliable signaling achieves comparable or better consistency than hard-state with lower overhead.
The paper empirically demonstrates that active queue management (AQM) schemes like PI and REM improve web response times only under high network loads (≥90% capacity) when combined with ECN, while ARED performs poorly, and without ECN, AQM offers little benefit over drop-tail FIFO.
The paper proposes a robust Active Queue Management (AQM) algorithm called Proportional Integral based series compensation and Position feedback compensation (PIP), which outperforms PI by reducing errors from model inaccuracies and enhancing responsiveness to dynamic network conditions.
The paper proposes *persistent dropping*, a novel SYN packet drop strategy that efficiently controls traffic aggregates during flash crowd events by modeling client persistence, ensuring quick convergence to control targets, minimizing client delay, and being easily implementable, with real-world tests showing up to 60% delay reduction.
The paper proposes an entropy-penalized regularization method for estimating traffic matrices from link measurements, ensuring consistency with data while minimizing information-theoretic divergence from an independent source/destination model, and demonstrates its effectiveness using real-world backbone data.
The paper demonstrates that robust intra-domain routing can achieve near-optimal network utilization with limited traffic demand knowledge, challenging the assumption that precise demand data is essential for effective traffic engineering.
The paper presents methods to estimate original flow size distributions from sampled packet data, including unsampled flows, using statistical inference and protocol details, with applications in network attack detection and analysis of sampling implementations.
The paper presents a high-level programming tool for anonymizing and transforming packet traces, enabling customizable policy scripts to handle both headers and payloads while ensuring sensitive data removal.
This paper analyzes the performance and reliability benefits of multihoming for stub networks, finding significant improvements in performance (with diminishing returns beyond four providers) and reliability, while offering guidelines for optimal ISP selection and usage strategies.
The paper proposes improved techniques for mapping traceroute hops to Autonomous Systems (ASes) using BGP tables, DNS lookups, and BGP updates to address inaccuracies in conventional methods, aiming to develop a more reliable AS-level traceroute tool.
The paper finds that the ring geometry in DHT routing provides the greatest flexibility, leading to superior resilience and proximity performance compared to more complex geometries like hypercubes, trees, or butterfly networks.
The paper proposes a peer-to-peer architecture (ODRI) based on optimal diameter de Bruijn graphs, demonstrating their superior routing efficiency and fault resilience compared to existing systems like Chord and CAN.
The paper proposes scalable modifications to Gnutella's P2P design by dynamically adapting overlay topology and search algorithms, achieving significant performance improvements.
The paper proposes a first-principles approach to modeling Internet topology by integrating statistical and graph-theoretic methods with practical constraints on router and link capabilities, aiming to resolve controversies and improve understanding of network structure.
Vivaldi is a distributed, efficient algorithm that assigns synthetic coordinates to hosts to predict network latencies accurately with minimal measurement overhead, achieving a median prediction error of 11%.
The paper presents a methodology for reverse engineering routing designs from router configurations, revealing that conventional interior/exterior gateway protocol models inadequately capture the diverse mechanisms used in operational networks, and highlights unusual designs and their trade-offs.
The paper introduces *Pathneck*, a tool that efficiently and accurately locates bottleneck links on Internet paths using Recursive Packet Train (RPT) probing without requiring destination access, achieving ~80% success in detection and aiding traffic optimization.
The paper proposes a scalable overlay network monitoring system that uses algebraic methods to estimate path loss rates and latency with only \( O(n \log n) \) measurements, efficiently adapts to topology changes, balances measurement load, and handles errors, achieving high accuracy and rapid adaptation.
CapProbe is a fast, accurate, and computationally efficient network capacity estimation technique that combines delay and dispersion measurements of packet pairs, outperforming pathchar and matching pathrate in accuracy while being faster and simpler.
The paper proposes and evaluates novel smart routing algorithms for multihomed users, demonstrating their effectiveness in optimizing cost and performance without negatively impacting other users.
The paper demonstrates that intelligent BGP route control combined with ISP multihoming can achieve performance and reliability close to overlay routing while leveraging the existing Internet routing infrastructure.
The paper investigates the feasibility of using application end-point architectures for large-scale live streaming, demonstrating through real-world data and design evaluations that such architectures can meet key requirements for resource availability, stability, and efficiency.
The paper analyzes packet loss in an urban 802.11b network, finding that loss rates are uniformly distributed, often stable but occasionally bursty, primarily caused by multi-path fading rather than distance or interference, with implications for protocol design.
The paper evaluates three link-quality metrics (ETX, per-hop RTT, and per-hop packet pair) against minimum hop count in a wireless ad hoc network, finding ETX best for stationary nodes but hop count superior for mobile senders due to self-interference issues with other metrics.
The paper formulates the delay-tolerant networking (DTN) routing problem, proposes a framework for evaluating routing algorithms in environments with time-varying connectivity and finite buffers, and demonstrates that efficient routing is achievable with limited, rather than complete, topological knowledge.
The paper proposes the "Postmanet," a system using postal-delivered digital storage media to bridge the digital divide by complementing traditional Internet connectivity with high-bandwidth, low-cost, and globally accessible data transfer.
Platypus is an authenticated source routing system using network capabilities to enable accountable, policy-compliant path selection, balancing user control over routes with ISP traffic policy enforcement.
The paper proposes the Secure Path Vector (SPV) protocol, an efficient symmetric-cryptography-based solution to secure BGP update messages, offering a 22x speed improvement over S-BGP while enabling incremental deployment.
The paper proposes *shields*—network filters deployed in end systems to block exploits for known vulnerabilities before patches are applied—as a more reliable and less disruptive first-line defense against worms than patching, demonstrating feasibility through a prototype framework with low false positives and minimal performance impact.
The paper presents a methodology to identify the autonomous system (AS) or AS session responsible for BGP routing instability by analyzing and correlating BGP updates from multiple observation points, validated through simulations and real-world data.
The paper proposes a Principal Component Analysis-based method to effectively detect, identify, and quantify network traffic anomalies using simple link measurements, demonstrating high accuracy and low false alarm rates in real backbone networks.
The paper presents an analytic model and methodology to evaluate and minimize hot-potato disruptions in ISP networks by analyzing interactions between interdomain and intradomain routing.
The paper proposes **Adaptive NetFlow**, a dynamic sampling solution that improves robustness and accuracy over traditional NetFlow by adjusting sampling rates adaptively and introducing a **Flow Counting Extension** for unbiased non-TCP flow estimation, while enabling scalable traffic reporting under congestion.
The paper demonstrates that distributed schedulers in buffered multistage interconnection networks can be provably work-conserving with speedups of 2 or more, analyzes their performance under lower speedups, and shows their robustness under extreme traffic conditions.
The paper presents an algorithm for exact GPS simulation with \( O(\log N) \) complexity, enabling WF²Q to achieve minimum deviation from GPS service with optimal efficiency.
The paper argues that the traditional buffer sizing rule \( B = \overline{RTT} \times C \) is outdated for backbone routers and proposes a new rule \( B = \frac{\overline{RTT} \times C}{\sqrt{n}} \) (where \( n \) is the number of flows), demonstrating that significantly smaller buffers can maintain throughput while enabling faster on-chip SRAM use.
The paper proposes a wavelet denoising-based technique to detect shared congestion between any pair of Internet paths without requiring common endpoints, drop-tail queues, or single congestion points, demonstrating improved accuracy, speed, and robustness in simulations and real-world experiments.
The paper demonstrates that max-min fair congestion control methods with a stable symmetric Jacobian remain stable under arbitrary feedback delays, proposes the Max-min Kelly Control (MKC) framework for robust performance, and shows its benefits for high-speed networks.
The paper reveals widespread DNS misconfigurations—lame delegation, diminished server redundancy, and cyclic dependency—that degrade performance and availability, highlighting the need for systematic error-checking mechanisms in large-scale systems.
The paper introduces CoDoNS, a peer-to-peer-based name service that improves upon DNS by offering faster lookups, resilience to attacks, and rapid updates through proactive caching, automatic load-balancing, and cryptographic delegation while maintaining backward compatibility.
The paper proposes a three-level name resolution architecture (user descriptors → service identifiers → endpoint identifiers → IP addresses) using flat names and DHTs to enhance service persistence, mobility, multi-homing, and middlebox integration in the Internet.
The paper introduces *Mercury*, a scalable protocol enabling efficient multi-attribute range-based searches with explicit load balancing through lightweight sampling, achieving logarithmic-hop routing and near-uniform load distribution while demonstrating its effectiveness in reducing messaging overhead for distributed game state maintenance.
The paper develops fluid models to analyze BitTorrent's scalability, performance, efficiency, and incentive mechanisms, supported by simulations and real-world data.
The paper presents a Scalable Distributed Information Management System (SDIMS) that aggregates network-wide data using DHT-based trees, offering scalability, flexibility, administrative isolation, and robustness through lazy reaggregation and spatial replication, significantly outperforming existing approaches.
The paper *Magnet* presents a general-purpose tool for navigating and refining searches in semi-structured data environments without requiring domain-specific interfaces or schema expertise.
The paper presents *Metarouting*, a declarative approach using the *Routing Algebra Meta-Language (RAML)* to flexibly define and automatically verify correctness of routing protocols, separating mechanisms from policy while ensuring convergence guarantees.
The paper proposes HLP, a hybrid link-state and path-vector protocol, as a scalable, efficient, and convergent alternative to BGP, demonstrating significant improvements in churn reduction, event isolation, and convergence time.
The paper demonstrates that ensuring stable interdomain routing under autonomous AS policies requires ranking routes based on AS-path lengths to prevent instability.
The paper proposes the Variable-structure congestion Control Protocol (VCP), which uses only two ECN bits for congestion feedback to achieve high efficiency, fairness, and low packet loss in high bandwidth-delay networks, though with slower fairness convergence than XCP.
The paper analyzes how the TCP-friendly equation, loss event rate estimation, and delay estimation in TFRC lead to throughput imbalance with TCP, exacerbated by differing sending rates and loss event rate disparities, causing extreme bandwidth allocation differences.
The paper analyzes the underutilization of Explicit Congestion Notification (ECN) in the Internet, identifies performance drawbacks in its current specification, demonstrates significant improvements by enabling ECN in TCP SYN-ACK packets, and provides practical recommendations to incentivize broader ECN adoption across networks, end-hosts, and web servers.
OpenDHT is a public, scalable distributed hash table service designed to address storage allocation control and interface flexibility for diverse applications while ensuring fair usage among untrusted clients.
The paper introduces *Meridian*, a lightweight, scalable framework for network-based node selection using multi-resolution rings, direct measurements, and gossip protocols, enabling efficient closest-node discovery, leader election, and latency-constrained node location without absolute coordinates, with proven scalability and accuracy in simulations and real-world deployment.
The paper presents Prefix Hash Trees, a DHT-based data structure for geographic range queries in Place Lab, demonstrating that DHTs can simplify deployment and management while maintaining acceptable performance.
The paper proposes a method to optimize message delivery in delay-tolerant networks (DTNs) under path failures by splitting, replicating, and erasure-coding message fragments across multiple paths, drawing on portfolio theory to maximize delivery probability in both Bernoulli and Gaussian path models.
The paper proposes *Idle Sense*, a novel adaptive MAC layer access method for wireless LANs that dynamically adjusts contention windows based on idle slot observations to optimize throughput, fairness, and error rate adaptation while maintaining low collision overhead and delay.
ExOR is an integrated routing and MAC protocol that improves throughput in multi-hop wireless networks by opportunistically selecting forwarders after transmissions, leveraging long but lossy links, and achieving significant throughput gains compared to traditional routing.
The paper classifies TCP throughput predictors into Formula-Based (FB) and History-Based (HB) methods, evaluates their accuracy, and identifies key factors affecting their performance, showing FB works best for non-saturated transfers while HB performs well with historical data but depends on path conditions.
The paper introduces **BADABING**, a novel packet loss measurement algorithm that improves accuracy over traditional Poisson-based probing while allowing a tunable trade-off between measurement precision and network impact.
The paper presents a data mining and information-theoretic methodology for analyzing Internet backbone traffic to identify common communication patterns and anomalies, validated using real-world core Internet data.
The paper introduces a novel hash table design that reduces worst-case lookup times by minimizing memory accesses, improving throughput for high-speed network applications without requiring additional buffering.
The paper proposes two algorithms, DIRPE for reducing TCAM range expansion and MUD for multi-match classification, both improving efficiency without modifying existing TCAM hardware.
The paper proposes Scalable Dynamic Pipelining (SDP), a novel IP-lookup scheme that addresses all five scalability challenges—routing-table size, throughput, cost, power dissipation, and update efficiency—by dynamically mapping trie nodes to pipeline stages based on node height, enabling optimal worst-case performance and incremental updates.
The paper proposes using entropy-based analysis of packet feature distributions in flow traces to detect and classify network anomalies automatically and effectively.
The paper presents a privacy-preserving, multilevel traffic classification method based on transport-layer host behavior patterns, achieving 80%-90% coverage and over 95% accuracy without relying on payload, port numbers, or additional data.
The paper presents TVA, a network architecture that effectively mitigates Denial of Service (DoS) attacks by using sender authorizations, addressing various attack vectors while maintaining performance and deployability on off-the-shelf hardware.
TeXCP is an online distributed Traffic Engineering protocol that dynamically balances network load in real-time by adaptively shifting traffic across paths to prevent congestion, outperforming offline TE methods with significantly reduced capacity requirements.
The paper presents a practical technique for rigorous protocol specification to enable specification-based testing, successfully applied to TCP, UDP, and the Sockets API, revealing implementation differences and demonstrating feasibility for complex protocols like TCP.
The paper introduces *re-feedback*, a novel mechanism ensuring truthful path metrics in data headers by aligning incentives for honest reporting without altering end-user pricing, with applications like TCP rate policing and incremental deployment around unmodified routers.
The paper introduces *declarative routing*, a method using a database query language to express routing protocols, balancing extensibility and robustness while maintaining performance and security.
The paper introduces **PIAS**, a scalable and deployable IP anycast architecture using a proxy overlay to address deployment, scalability, and load-balancing limitations while maintaining the benefits of traditional IP anycast.
The paper argues that the current Internet architecture can become evolvable—capable of gradual, ISP-led change—with minor modifications addressing both technical and economic incentives.
The paper proposes BATON*, a multi-way tree P2P search structure that reduces search cost to \(O(\log_m N)\) with a manageable linear update cost penalty in \(m\), while also extending BATON to support multi-attribute queries.
The paper presents a decentralized collaborative data sharing model where participants reconcile updates with provenance-based authority rankings, tolerating disagreements while maintaining individual data consistency.
The paper introduces the Stable Bloom Filter (SBF), a space-efficient data structure for approximately detecting duplicates in streaming data by evicting stale information to maintain recent elements, ensuring a bounded false positive rate while outperforming alternative methods in accuracy and time efficiency.
This paper explores the gap between theoretical research on aggregate functions and practical implementation of user-defined aggregates (UDAs) in databases, proposing a framework to integrate UDAs into query optimization, rewriting, and view maintenance.
The paper introduces **Network Datalog (NDlog)** as a declarative language for network specifications, presents optimized distributed query evaluation techniques, proves eventual consistency under dynamic updates, and demonstrates performance on a 100-machine testbed.
The paper introduces forensic analysis techniques, including "corruption diagrams" and progressively advanced algorithms (monochromatic, RGB, polychromatic), to determine who, when, and what in cases of database tampering detected via cryptographic hashing.
The paper introduces *CONCH* (constraint chaining), an energy-efficient continuous monitoring algorithm for sensor networks that combines temporal and spatial suppression by enforcing node and edge constraints, reducing communication costs while maintaining data accuracy and handling network failures.
The paper presents energy-efficient algorithms for monitoring extreme values (MAX or MIN) in sensor networks by using localized thresholds to minimize message traffic.
The paper proposes a formal utility metric for anonymized datasets and introduces a method to enhance utility in *k*-anonymous and *l*-diverse tables while preserving their privacy guarantees.
The paper introduces a *personalized anonymity* framework for privacy-preserving data publication that tailors generalization to individual needs, ensuring optimal protection while maximizing data utility, and theoretically and experimentally demonstrates its superiority over uniform approaches.
The paper introduces CLIDE, an interactive query interface that guides users in formulating feasible queries over web service-accessed sources by providing real-time, complete, and efficient suggestions to avoid infeasible queries.
The paper presents **GPUTeraSort**, a high-performance external sorting algorithm for large databases that leverages GPU parallelism and optimized memory interfaces to outperform CPU-based sorting methods in both speed and cost-efficiency.
The paper presents a technique for selectively recovering from flawed user transactions by removing only tainted data and dependent transactions, leveraging transaction-time states as an online backup to avoid costly point-in-time recovery.
The paper proposes an automated method for selecting and ordering the most influential attributes in query results to improve user understanding and efficiency, introducing a hybrid Split-Pane approach and validating its effectiveness through performance and user studies.
The paper presents a high-performance system for processing complex event queries over real-time RFID data streams, introducing an extended event language, efficient query execution with native operators and relational techniques, and optimizations for large-scale workloads, validated through performance analysis and comparisons.
The paper proposes a meta-data indexing approach (including the full meta-data index and inheritance meta-data index) to efficiently evaluate XPath queries with range constraints on ordered meta-data annotations in XML documents, demonstrating the superiority of the inheritance-based method in performance.
The paper presents MonetDB/XQuery, a fast and scalable XQuery processor built on a relational engine, utilizing range-based XML encoding, relational algebra compilation, optimized query strategies, and XML-to-relational update mapping, achieving high performance on the XMark benchmark.
The paper proposes a **compressed skycube** structure to efficiently support **frequent updates** and **concurrent subspace skyline queries** by balancing query and update costs.
The paper proposes the first efficient method for reverse k-nearest neighbor (RkNN) search in arbitrary metric spaces, using conservative and progressive distance approximations to filter results without significant storage overhead.
The paper presents **Directed Local Search (DLS)**, an efficient indexing technique for unstructured tetrahedral meshes that leverages mesh topology to improve query performance without complex preprocessing, reducing disk accesses by 26%–4x and execution time by 25%–4x compared to traditional methods.
The paper proposes a novel information retrieval ranking strategy for keyword search in relational databases, addressing challenges like assembling answers from joined tuples, scoring relevance, and leveraging database structure, and demonstrates its effectiveness through real-world experiments.
This paper explores the design space between database-centric and network-centric approaches in large-scale publish/subscribe systems, proposing hybrid solutions that leverage both database and network optimizations to reduce communication overhead and server load while maintaining scalability for stateful queries.
XPORT is a profile-driven, extensible distributed data dissemination system that supports customizable data types, profiles, and optimization metrics through a generic tree-based overlay network, enabling efficient and adaptable performance under dynamic conditions.
The paper introduces **DADA**, a data cube for **Dominant Relationship Analysis (DRA)**, enabling efficient querying of dominance relationships between products and customers to aid business decision-making.
The paper demonstrates that treating database workloads as ordered sequences rather than unordered sets can enhance physical design tuning effectiveness, proposes techniques to address associated challenges, and validates them experimentally on SQL Server.
The paper analyzes the performance of Documentum ECI self-repairing wrappers, which provide content integration and automatic maintenance, using data from commercial deployments between 2003 and 2005.
The paper summarizes AT&T Labs-Research's development and practical applications of SPIDER, a system for efficient flexible string matching in large databases, used for customer data matching and data quality analysis.
The paper introduces **PADS**, an end-to-end system that uses a declarative language to describe and process ad hoc data formats, enabling parsing, querying, and analysis while promoting reusability and tool interoperability.
The paper introduces three tools—HTDGen, HTTrace, and HTPar—for testing database applications by generating meaningful test databases, executing tests efficiently, and enabling parallel test execution.
The paper introduces the BEA AquaLogic Data Services Platform (ALDSP), a middleware tool for declaratively developing, testing, and optimizing data services in SOA environments, supporting diverse data sources and offering features like layered service definitions, query optimizations, and security.
The paper introduces *VisTrails*, a system that integrates visualization and data management to streamline the creation, comparison, and reproducibility of scientific visualizations, reducing time to insight and improving usability.
The paper demonstrates an efficient timetravel service using external databases with snapshot isolation to retrieve and identify historical snapshots.
The paper introduces **MAXENT**, a maximum entropy-based method for accurate selectivity estimation of conjunctive predicates in query optimization, outperforming ad hoc heuristics in IBM DB2 by leveraging all available multivariate statistics to avoid bias and improve query performance.
The paper introduces **MAXENT**, a maximum entropy-based method for consistent and unbiased selectivity estimation of conjunctive predicates in query optimization, outperforming ad hoc heuristics in IBM DB2 LUW by leveraging all available multivariate statistics.
The paper *InMAF: indexing music databases via multiple acoustic features* proposes a music retrieval method combining PCA and neural networks with human perception-based acoustic features to improve accuracy and robustness over traditional spectral-feature approaches.
The paper demonstrates that Quark, an open-source system, efficiently integrates structured and full-text search in XQuery with scoring and ranking capabilities over XML data.
The paper introduces the **CS Cache Engine**, a client-side caching system for location-based services that uses the **Complementary Space Caching (CS caching)** scheme to enhance data availability, reduce unnecessary server requests, and improve query response times by maintaining both cached objects and complementary regions representing uncached data.
"Avatar Semantic Search enhances traditional keyword search by leveraging structured annotations extracted from text, demonstrating superior performance over conventional engines on datasets like Enron emails and blogs."
This tutorial summarizes record linkage methodologies, algorithms, and tools for identifying approximate duplicate records, comparing techniques from databases, information retrieval, statistics, and machine learning while highlighting their similarities, differences, strengths, and limitations.
The paper introduces **VINI**, a virtual network infrastructure enabling realistic, controlled experimentation with real routing software, traffic loads, and customizable topologies on shared physical infrastructure, demonstrated via **PL-VINI** on PlanetLab.
The paper proposes a novel approach to IP multicast that challenges the pessimistic view of its complexity by offering a simpler, fully-general design.
The paper introduces the Datagram Congestion Control Protocol (DCCP), a congestion-controlled unreliable transport protocol designed to address the limitations of UDP for high-bandwidth applications while avoiding TCP's reliability overhead.
The paper presents *Jigsaw*, a system that uses over 150 radio monitors to provide a unified, cross-layer view of large-scale 802.11 network behavior, addressing challenges in scale and ambiguity while analyzing performance issues like interference and management inefficiencies in a university building.
The paper presents practical, measurement-based models for predicting packet reception and carrier sense with interference in static wireless networks, achieving higher accuracy than models ignoring interference by using real-network RSSI and packet count data from N trials in an N-node setup.
The paper proposes **Interference-aware Fair Rate Control (IFRC)**, a distributed congestion control mechanism for wireless sensor networks that dynamically allocates fair and efficient transmission rates by monitoring queue lengths, sharing congestion states with minimal overhead, and using an AIMD-based rate adaptation, achieving near-optimal fairness and preventing queue overflow.
The paper presents *Wit*, a non-intrusive tool that enhances wireless network monitoring by merging multi-monitor traces, inferring missing packets using formal language methods, and deriving performance metrics to improve 802.11 MAC-level analysis.
The paper proposes capacity overprovisioning (CO) as a QoS alternative to admission control (AC), analyzing its dimensioning under traffic fluctuations, shifts, and failures, and finds that CO and AC have comparable bandwidth needs when resilience is required.
COPE is a traffic engineering algorithm that optimizes for expected traffic scenarios while ensuring worst-case guarantees, achieving efficient resource utilization and congestion avoidance across diverse conditions.
Swing is a closed-loop, network-responsive traffic generator that accurately models and reproduces application traffic dynamics, including burstiness across timescales, by capturing user, application, and network behavior from observed data and simulating it in emulated environments.
The paper introduces a stochastic network calculus framework with maximum-backlog-centric stochastic arrival and service curves, deriving key results under (min, +) algebra and independent case analysis, and provides methods to determine these curves for flow and server characterization.
The paper introduces the *dK*-series, a systematic method to analyze and synthesize network topologies by capturing degree correlations in subgraphs of size *d*, enabling accurate reproduction of graph metrics and properties, with *d=2* being sufficient for most practical purposes and *d=3* nearly reconstructing real-world topologies like the Internet.
The paper analyzes node selection strategies in distributed systems, finding that uniform-random replacement reduces churn effectively, while preference-based strategies often increase churn, and demonstrates how adding randomization improves performance across various protocols.
The paper presents a scalable, loop-free routing system where end-systems use tags to guide packets along non-shortest paths, enabling path diversity without explicit source routing while maintaining compatibility with existing Internet infrastructure.
The paper presents **MIRO**, a flexible multi-path interdomain routing protocol that enhances BGP by allowing negotiated additional paths between domains while maintaining control for transit networks and ensuring scalability and backward compatibility.
The paper argues that the internet's lack of accountability hinders innovation and optimal routing, proposing that introducing robust monitoring systems aligned with contracts is essential to address these issues.
The paper proposes a more accurate Internet routing prediction model by addressing prior limitations—treating Autonomous Systems (ASes) as non-atomic structures with internal route diversity and avoiding oversimplified AS relationships—resulting in improved consistency with observed routes and better unobserved route predictions.
The paper analyzes 21 months of Japanese backbone traffic data, revealing that peer-to-peer traffic dominates residential usage, with a small fraction of heavy-hitters and fiber users driving most traffic, leading to significant impacts on network capacity planning.
The paper proposes **LEND**, a system that accurately diagnoses link-level network faults by identifying **minimal identifiable link sequences (MILSes)** from end-to-end measurements, enabling real-time, topology-adaptive, and statistically unbiased inference of loss rates.
The paper argues that Poisson-based probing in active measurements is not uniquely unbiased due to PASTA's limitations, ignores estimation variance, and often requires inversion, providing examples where Poisson probes are unsuitable and suggesting alternative approaches.
The paper proposes COPE, a wireless mesh network architecture that improves throughput by using network coding to mix packets from different sources during transmission, demonstrating significant gains in real-world testbed evaluations.
The paper proposes "Growth Codes," a novel data replication technique for sensor networks in disaster scenarios, which enhances data persistence and transmission efficiency despite sudden node failures.
The paper introduces *SybilGuard*, a protocol that limits sybil attacks in decentralized systems by leveraging trust relationships in social networks to bound the number of fake identities a malicious user can create.
The paper proposes a model to evaluate the adoptability of secure BGP routing protocols, finding that adoption depends on cost thresholds and path authentication benefits, with full path security improving adoptability under weaker attacker models.
The paper analyzes a 17-month spam dataset to reveal that most spam originates from a few IP ranges, primarily using short-lived bots and occasionally hijacked BGP routes, suggesting network-level filtering and routing security improvements could effectively combat spam.
"Speak-up defends against application-level DDoS attacks by encouraging clients to increase their traffic volume, leveraging the attackers' limited bandwidth to allow legitimate clients with spare capacity to dominate server resource allocation."
The paper proposes Approximate Concurrent State Machines (ACSMs) using Bloom filters and hashing to compactly track flow states with minimal errors, demonstrating applications in video congestion control and P2P traffic detection while significantly reducing memory usage.
The paper proposes *Split-Detect*, a scalable intrusion prevention method that avoids full packet reassembly by splitting signatures into fragments, forcing attackers to reveal evasion attempts through abnormal behavior or detectable signature pieces, reducing processing overhead while maintaining detection efficacy.
The paper introduces the Delayed Input DFA (D²FA), a memory-efficient representation for regular expressions that reduces transition storage by over 95% compared to traditional DFAs, enabling high-speed deep packet inspection at multi-gigabit rates.
"VRR is a robust, link-layer-based routing protocol combining traditional point-to-point and DHT routing, eliminating the need for network flooding or address translation, and outperforming existing wireless routing protocols in diverse environments."
The paper proposes ROFL, a routing algorithm that routes directly on flat host identities instead of network locations, showing that while imperfect, the approach is feasible and warrants further exploration.
The paper investigates how routing changes (failover and recovery events) cause end-to-end packet loss, forwarding loops, and performance degradation, revealing that ISP routing policies and iBGP configurations significantly impact path performance during such events.
The paper studies policy-based routing with non-strict preferences, establishing conditions for stable, optimal, and asynchronously convergent routing solutions in complex network scenarios.
The paper proposes a User Satisfaction Index (USI) model for VoIP services like Skype, using objective network metrics (bit rate, jitter, RTT) to quantify user satisfaction without requiring user surveys or voice signals.
The paper proposes multi-tree overlay multicast heuristics for bandwidth-scarce, heterogeneous environments, improving high contributors' performance by 10-240% and ensuring equitable bandwidth distribution, validated via PlanetLab experiments.
The paper analyzes Windows Update's patch distribution system using year-long data from 300 million computers, identifies key traffic and user behavior patterns, and evaluates alternative strategies like caching and peer-to-peer to improve efficiency in large-scale software update dissemination.
The paper explores how Akamai's CDN redirections can be leveraged to infer high-quality Internet paths without active probing, demonstrating their correlation with network latency and their utility in optimizing overlay routing.
The paper argues that database usability is as crucial as capability, identifies five pain points in current systems, and proposes a research agenda including a presentation data model, direct data manipulation, schema-later design, provenance, and consistency to improve usability.
The paper introduces SGL, a data-driven scripting language for scalable game AI, enabling efficient behavior customization for large numbers of non-player characters through query processing and indexing techniques.
The paper proposes indexing techniques for dataspaces that efficiently support hybrid queries combining keywords and structure by extending inverted lists to incorporate schema elements, relationships, hierarchies, and synonyms, demonstrating significant performance improvements and scalability.
The paper proposes improved approximation algorithms for achieving *k*-anonymity with an *O*(log *k*)-approximation ratio, outperforming prior methods in efficiency and adaptability.
The paper investigates how database systems retain deleted data in various components, posing privacy risks, and proposes transparency criteria and techniques for secure deletion to mitigate forensic analysis threats.
The paper introduces a double-heap algorithm and join-signature model to efficiently compute top-k queries with ad-hoc ranking functions using tree-structured indices, significantly outperforming baseline methods.
The paper *Spark: top-k keyword query in relational databases* proposes an effective and efficient ranking method based on *virtual documents* for keyword searches in relational databases, along with optimized query processing algorithms that minimize database accesses, demonstrating superior performance in retrieval effectiveness and efficiency compared to existing approaches.
The paper *FICSR: Feedback-based Inconsistency Resolution and Query Processing on Misaligned Data Sources* introduces a query-driven, feedback-based approach to resolving data conflicts by using ranked interpretations and bidirectional user-system feedback to improve alignment between user expectations and system outputs.
The paper statistically analyzes and empirically compares sketching techniques, concluding that Fast-AGMS sketches consistently outperform or match other methods, making them the preferred choice for practical applications.
The paper proposes efficient synopses and distinct-value estimators for multiset operations in a scalable "synopsis warehouse" architecture, enabling unbiased, accurate, and parallelizable estimation with theoretical guarantees.
The paper proposes an Update Management Service (UMS) and Key-based Timestamping Service (KTS) to ensure data currency in replicated DHTs by efficiently retrieving current replicas using distributed timestamping, demonstrating effectiveness through analysis, implementation, and simulation.
BLINKS introduces a bi-level indexing scheme for efficient top-*k* keyword searches on graphs, improving performance and scalability by partitioning the graph into blocks and leveraging summary and detailed indexing for accelerated query processing.
The paper presents **XSeek**, an XML keyword search engine that identifies meaningful return nodes by analyzing data structures and keyword match patterns to infer search semantics and distinguish between search predicates and return specifications.
This paper introduces new declarative similarity predicates for data quality based on probabilistic information retrieval (e.g., language models and hidden Markov models), classifies existing predicates, and evaluates their performance and accuracy for data cleaning tasks.
The paper introduces *JouleSort*, an energy-efficiency benchmark for evaluating computer systems' energy performance in sorting tasks, demonstrating a 3.5x improvement using a mobile CPU and laptop drives.
The paper proposes a method to predict storage workload generated by a database management system based on a given database workload and physical design, enabling better end-to-end storage and database configuration.
The paper proposes augmenting the relational model with queries as data values and extended join mechanisms to uniformly model and query both data and metadata, demonstrating its effectiveness through a prototyped system.
The paper introduces algorithms for efficiently handling insertions, deletions, and updates in a cracked database while maintaining its self-optimizing performance benefits under dynamic workloads.
The paper presents efficient log-based recovery methods for middleware servers, minimizing overhead through optimized logging and checkpointing while enabling parallel recovery of client states, with promising performance demonstrated in a prototype implementation.
The paper demonstrates that leveraging aggregate constraints in deduplication improves accuracy, despite semantic and computational challenges, by defining and optimally solving the problem within a restricted search space.
The paper introduces ILIADS, an algorithm that combines data matching and logical reasoning to improve ontology integration, demonstrating significant performance gains over existing tools FCA-merge and COMA++.
The paper presents a declarative mapping approach that compiles bidirectional views between application models and databases to enable seamless data transformation and query handling, implemented in a commercial product.
The paper proposes an adaptive indexing approach for efficiently processing dynamic location constraints in moving object environments, reducing processing time by 70% and storage accesses by 80% while adapting to changing workloads and movement patterns.
The paper introduces **TRACLUS**, a **partition-and-group framework** for trajectory clustering that first partitions trajectories into line segments using MDL and then groups similar segments via density-based clustering to discover common sub-trajectories.
The paper introduces **Keyword-Driven Analytical Processing (KDAP)**, a framework that enables intuitive keyword-based search combined with OLAP aggregation capabilities to efficiently explore and analyze complex data without requiring extensive pre-organization.
The paper proposes a random walk method to uniformly sample hidden databases via form-like interfaces, exploring fixed and random attribute orderings and using probabilistic rejection to enhance sample quality.
The paper proposes a two-step, preference-aware clustering approach for SQL-query-result navigation, which offline clusters query results by diverse user preferences and dynamically constructs a cost-based navigational tree to help users efficiently explore relevant subsets without explicit input.
The paper proposes a privacy-preserving protocol for record matching across data sources using a third party and vector space mapping to ensure data and schema confidentiality while efficiently identifying common information.
The paper introduces *m-invariance*, a generalization principle and algorithm for privacy-preserving re-publication of dynamic datasets with insertions and deletions, addressing limitations of existing *k*-anonymity and *l*-diversity approaches.
The DBO engine enables scalable approximate query processing by providing progressively accurate estimates with statistical bounds during execution, allowing users to stop early when satisfied with the result's precision.
The paper proposes a piggybacking optimization technique for efficient XML data dissemination in publish-subscribe systems by leveraging upstream subscription matchings to reduce downstream filtering workload.
The paper proposes a dynamic clustering algorithm to reduce maintenance overhead in topic-based publish-subscribe systems by grouping correlated topics into virtual clusters, thereby unifying their supporting structures and improving performance.
The paper introduces **Massively Multi-Query Join Processing** techniques to efficiently handle thousands of concurrent XML stream queries involving value joins, achieving significant performance improvements over naive approaches.
The paper introduces a lazy, adaptive RID-list intersection algorithm that improves efficiency and robustness in query processing by avoiding upfront RID-list formation and using adaptive set intersection instead of an AND-tree, reducing IO costs and mitigating cardinality mis-estimation issues.
The paper presents an optimal top-down join enumeration algorithm that matches the efficiency of bottom-up dynamic programming while retaining traditional optimizer architecture and enabling faster performance through techniques like branch-and-bound.
The paper presents a parallel version of progressive optimization (POP) for shared-nothing databases, introducing novel voting schemes, checkpoint operators, and intermediate result reuse to improve query performance by up to 22x with minimal overhead.
The paper argues that storing sparse data sets in a single wide table is optimal, provided RDBMSs support efficient storage, ad-hoc querying, and evaluation over numerous attributes, eliminating the need for complex schema design.
The paper introduces TRELLIS, a scalable disk-based suffix tree algorithm for indexing genome-scale DNA sequences efficiently, outperforming existing methods in both construction and querying times while handling large datasets like the human genome with limited memory.
The paper introduces **GRIPP**, a linear-time-and-space graph indexing structure for fast reachability queries, achieving sub-5ms response times on 5-million-node graphs and seamless integration with relational databases.
The paper proposes **FG-index**, a nested inverted-index based on frequent subgraphs, which enables verification-free query processing for frequent queries and significantly reduces verification costs for infrequent queries by using δ-Tolerance Closed Frequent Graphs to control index size.
InfiniteDB is a PC-cluster-based parallel DBMS designed for efficient storage and processing of massive databases, offering multi-level parallelism, adaptive optimization, heterogeneous data integration, and fault tolerance.
LIPTUS is a banking tool that integrates customer interactions (emails, calls) with structured account data, enabling enhanced analytics and customer intelligence by linking text-mined interaction metadata with existing profiles.
The paper introduces TPoX, an application-oriented XML database benchmark that evaluates comprehensive database functionality—including storage, indexing, and transaction processing—by simulating a multi-user financial workload using FIXML data.
The paper introduces *ReXSA*, a schema advisor tool for IBM DB2 9 that recommends hybrid relational-XML database schemas based on qualitative properties like reuse, evolution, and performance profiles to optimize data persistence.
The paper explores Microsoft SQL Server's subquery optimization techniques, detailing various execution strategies (e.g., forward/reverse lookup, set-based approaches) and demonstrating their performance trade-offs, emphasizing the need for cost-based optimization in modern query processing.
The paper presents *AllInOneNews*, the world's largest news metasearch engine, detailing its scalable architecture, novel features like semantic query matching, and a comparative evaluation against Google News and Mamma News using effectiveness, diversity, and time-sensitivity metrics, while introducing a new combined relevance-time-sensitivity measure.
The paper introduces **Map-Reduce-Merge**, an extension of the Map-Reduce model that adds a **Merge phase** to efficiently process multiple heterogeneous datasets, enabling relational operations like joins while maintaining scalability.
The paper introduces **BIwTL**, a Business Information Warehouse Toolkit and Language that simplifies and automates information warehouse management by using a high-level declarative language (GIWL) to handle structured and unstructured data while abstracting technical complexities.
"GPUQP is a relational query engine that co-processes queries using both CPUs and GPUs, demonstrating design considerations and performance benefits for database operations like tree indexes and joins."
The paper presents Shirako and NIMO, systems that automate on-demand provisioning of virtual machines for database applications by learning performance models and dynamically allocating resources to meet SLAs.
The paper presents **XTream**, a prototype platform for building data stream applications, demonstrated through VoIP-E-Mail integration and a Smart Home scenario, while discussing essential functionalities for data stream management systems.
OSIRIS-SE is a Java-based infrastructure enabling reliable, distributed data stream management on mobile devices through coordinated operator checkpointing, fine-grained reliability, and decentralized orchestration, demonstrated in a healthcare telemonitoring application.
AutoDomainMine is a graphical data mining system that integrates clustering and classification to optimize scientific processes by analyzing experimental input conditions and result graphs to provide ranked responses to user queries.
The paper *Mashup Feeds* presents a system for continuous web service queries using collection-based stream processing to monitor and extract evolving data from sources.
The paper presents TopX, a search engine framework for unified indexing, querying, and ranked retrieval of unstructured, semistructured, and structured data using efficient top-*k* algorithms, scoring models, and ontology-based query expansion.
The paper introduces a **relationship search operator** for querying heterogeneous data graphs, leveraging link structures to discover meaningful connections between objects and using ObjectRank for ranking, while also exploring summary graph constructions for efficient query processing and result presentation.
The paper introduces the QUEST system, which integrates and queries conflicting scientific taxonomies and data using a constraint-based model to resolve value and structural misalignments in context-driven research.
The paper examines system design challenges in sensor databases, focusing on architectural design, scheduling, routing, and medium access control to improve query processing efficiency in resource-limited wireless sensor networks.
Ethane is a centralized, flow-based enterprise network architecture that enforces fine-grained policies through simple switches and a controller while maintaining compatibility with existing infrastructure.
The paper introduces *Sherlock*, a system that uses *Inference Graphs* to model multi-level dependencies in enterprise networks, improving fault localization accuracy by 30% compared to two-level approaches.
The paper presents modeling techniques to automatically diagnose transient outages and performance issues in 802.11 wireless networks by analyzing delays related to media access and mobility management using measurement, inference, and modeling.
The paper proposes a framework combining a Chi-Square test to detect Skype's encrypted traffic fingerprint and a Naive Bayesian Classifier based on traffic features, validated by an off-line heuristic, to effectively identify Skype traffic in real time.
The paper proposes *BubbleStorm*, a probabilistic peer-to-peer search system built on random multigraphs that ensures flexible, reliable, and tunable exhaustive search while handling high churn and peer failures.
The paper proposes a Surveyor-based method using Kalman filters to detect malicious behavior in Internet Coordinate Systems by modeling normal node dynamics and leveraging trusted nodes to identify anomalies in systems like Vivaldi and NPS.
The paper proposes a malleable processor architecture with dynamic reconfiguration of cache capacity and thread count, along with a runtime optimization algorithm, to achieve both ease-of-programming and high performance in packet processing, outperforming commercial processors like Intel's IXP2800 in most deployments.
The paper explores integrating Network Processor (NP) subsystems with general-purpose servers in PlanetLab to significantly enhance performance, demonstrating an 80x improvement in packet processing rates and reduced latency while maintaining ease of application porting through a fast path/slow path structure.
The paper introduces **ProgME**, a programmable network measurement architecture that uses flexible **flowsets** and real-time adaptation to improve scalability and accuracy for diverse monitoring applications.
The paper introduces **SLA M**, a unified active measurement tool that improves accuracy in monitoring SLA compliance for packet loss, delay, delay variation, and network performance tomography while reducing bandwidth usage compared to existing methods.
The paper introduces the *lottery tree (lottree)* mechanism to incentivize participation and recruitment in peer-to-peer systems, ensuring fairness and scalability while analyzing optimal parameters through simulations.
The paper demonstrates that peer-assisted video-on-demand (VoD) can significantly reduce server bandwidth costs while maintaining user experience, especially with prefetching and ISP-localized traffic, as evidenced by analysis of MSN Video traces and a developed analytical model.
The paper investigates the granularity of routing policies in AS-level Internet models, finding that business relationships alone are insufficient for accurate path selection and introducing "next-hop atoms" to better capture per-neighbor path choices, hot-potato routing, and BGP tie-breaking.
The paper proposes a distributed mechanism to enforce route preference ordering only during policy disputes to balance BGP stability with autonomous system policy freedom.
The paper presents MORE, a MAC-independent opportunistic routing protocol that uses random packet mixing to improve throughput over ExOR by avoiding coordination overhead and enabling spatial reuse, achieving 22% higher median unicast throughput and 35-200% greater multicast performance.
The paper proposes the Data-Oriented Network Architecture (DONA), a clean-slate redesign of Internet naming and name resolution to better accommodate modern data retrieval and service access needs, replacing the original host-centric model.
The paper proposes NUTSS, a novel architecture combining overlay and data-path signaling to improve flow establishment in the Internet by integrating endpoint and network provider policies without modifying existing protocols.
The paper proposes **Complexity Oblivious Network Management (CONMan)**, a network architecture that simplifies management by minimizing protocol-specific details in the data-plane interface, enabling structured high-level policy enforcement.
The paper presents an axiomatic framework for modeling fundamental forwarding mechanisms in communication networks, enabling formal analysis and prototype implementation of diverse communication services through a universal forwarding engine.
The paper proposes REIN, a deployable interdomain service framework, along with robust algorithms, to enhance Internet reliability cost-effectively by improving network redundancy.
The paper proposes Failure-Carrying Packets (FCP), a new routing paradigm that eliminates convergence delays by allowing data packets to autonomously discover working paths using minimal router state, improving reliability and reducing overhead compared to traditional routing protocols.
The paper introduces EtherFuse, a backward-compatible device that improves Ethernet's resilience by accelerating spanning tree reconfiguration and suppressing packet duplication without requiring changes to existing hardware, software, or protocols.
The paper presents a study on prefix hijacking and interception in the Internet, demonstrating that interception is nearly as feasible as hijacking, with higher-tier ASes capable of significantly impacting traffic, while also highlighting detection challenges.
The paper proposes a lightweight, distributed, and real-time IP prefix hijacking detection method that leverages stable hop counts and path super-relationships from multiple vantage points on the data plane, achieving high accuracy with false positive and negative rates below 0.5%.
Portcullis mitigates Denial-of-Capability (DoC) attacks by ensuring per-computation fairness in bandwidth allocation for connection setup packets, guaranteeing legitimate senders can establish capabilities with high probability regardless of attacker strength.
The paper introduces **UDmap**, an automated algorithm that identifies dynamically assigned IP addresses using server logs, revealing their significant role in spam (42.2% of Hotmail's spam) and suggesting broader applications in phishing and botnet detection.
The paper proposes a method to distinguish real Internet topology changes from transient routing dynamics in BGP data, revealing distinct evolution trends for customer and provider networks to inform future routing architecture design.
The paper proposes methods to generate scalable, annotated Internet router topologies that closely mimic real-world network characteristics, improving simulation accuracy for network research.
The paper presents distributed rate limiters that enforce global rate limits across multiple sites for cloud-based services, ensuring efficient, scalable, and accurate traffic policing with minimal overhead.
The paper demonstrates that end-host-based congestion prediction is more accurate than previously thought but still uncertain, proposing Probabilistic Early Response TCP (PERT) to emulate AQM/ECN behavior, with analysis and simulations showing PERT/RED's improved stability and feasibility.
**Summary:** Structured Stream Transport (SST) introduces hierarchical child streams to efficiently handle mixed transaction sizes, avoid handshake delays, prevent head-of-line blocking, and unify datagram and stream semantics while maintaining TCP-friendly performance.
The paper presents **RAPID**, an intentional DTN routing protocol that optimizes specific routing metrics like delivery latency by treating routing as a resource allocation problem, and demonstrates its superior performance through real-world deployment and simulations.
The paper demonstrates that 802.11 networks are highly vulnerable to weak or narrow-band RF interference from devices like Zigbee and jammers, identifies key vulnerabilities in MAC and PHY layers, and proposes rapid channel hopping as an effective mitigation strategy.
The paper demonstrates that analog network coding, where routers forward interfering signals instead of packets, can double the capacity of a 2-way relay network and achieves higher throughput than traditional routing or digital network coding.
The paper introduces **Partial Packet Recovery (PPR)**, combining **SoftPHY** (confidence hints for bits) and a **postamble scheme**, along with **PP-ARQ** (a link-layer protocol for retransmitting only likely erroneous bits), demonstrating a **2x increase in end-to-end capacity** in wireless mesh networks compared to traditional retransmission methods.
The paper proposes visualization techniques (atomic, aggregated, and density plots) to enable interactive exploration and pattern discovery in billion-record datasets by addressing scalability challenges through coordinated aggregation and summary graphics.
The paper introduces *k*-degree anonymity for graphs to protect individual privacy by ensuring each node shares its degree with at least *k*-1 others, proposes efficient algorithms to achieve this with minimal modifications, and validates their effectiveness on synthetic and real datasets.
The paper proposes a cryptographic framework using Private Information Retrieval (PIR) to enable private location-based queries without requiring a trusted anonymizer, offering stronger privacy guarantees against both snapshot and correlation attacks while maintaining practical performance.
The paper presents two near-optimal approximation algorithms—**Greedy** (with an \(O(\log n + \log \alpha)\) approximation ratio) and **Harmonic** (with a \(2\beta\) ratio)—for efficiently evaluating shared filters in overlapping data stream queries, demonstrating their effectiveness in experiments.
The paper introduces a formal evaluation model and optimization framework for efficient pattern matching over event streams, addressing richer query languages and runtime challenges through sharing techniques that significantly improve performance.
The paper presents a scalable regular expression matching system that combines the efficiency of DFAs and NFAs, using caching and RE clustering to handle large rule sets efficiently on data streams while outperforming existing tools.
The paper proposes OCI, a parameter-free, efficient top-down clustering method that uses independent component analysis and the exponential power distribution to robustly identify clusters of varying shapes and distributions while effectively filtering outliers.
The paper proposes flexible dimensionality reduction techniques for the Earth Mover's Distance (EMD) to enable efficient, lossless similarity search in high-dimensional multimedia databases, significantly reducing computational costs while maintaining accuracy.
The paper proposes an angle-based space partitioning method using hyperspherical coordinates to improve parallel skyline computation by evenly distributing skyline points across partitions and reducing redundant processing, outperforming traditional grid-based approaches.
The paper introduces efficient techniques and data structures for maintaining categorical skylines in streaming data, addressing challenges in partial-order attribute domains and dominance queries through geometric arrangements and lightweight indexing.
The paper analyzes Cayley graphs as potential network structures for Peer-based Data Management Systems (PDMS) by evaluating their properties in relation to PDMS requirements, identifying key measures for suitability, and providing both theoretical and experimental insights to guide future development.
The paper introduces PISCES, a peer-to-peer system that selectively indexes data based on query and update frequencies, using a just-in-time approach to optimize efficiency and reduce maintenance costs while improving search performance.
The paper proposes an efficient path encoding and storage scheme for RFID data in supply chain management, enabling fast processing of tracking and path-oriented queries with significant performance improvements over existing methods.
The paper introduces a **relational-style XML query** method that automatically discovers and queries varied XML structures derived from relational data, leveraging functional dependencies for efficiency while enabling schema flexibility and integration.
The paper proposes a graph query language and access methods that treat graphs as fundamental units, extending relational algebra with graph pattern matching and composition operators, and demonstrates significant performance improvements over SQL-based approaches through optimized graph-specific techniques.
The paper introduces **LEAP**, a novel graph mining framework that efficiently identifies significant graph patterns by leveraging structural and significance similarity correlations, outperforming existing methods in speed and accuracy for pattern discovery and classification.
The paper proposes **Cohesive Subgraph Visualization (CSV)**, an approximate algorithm that maps nodes and edges to a multi-dimensional space to efficiently identify and visualize dense subgraphs with **O(|V|² log|V|)** complexity, aiding both graph exploration and scaling exact mining methods.
"Privacy-MaxEnt introduces a maximum entropy-based method to quantify privacy by integrating adversaries' background knowledge and published data constraints to estimate unbiased conditional probabilities of sensitive attributes given quasi-identifiers."
The paper introduces **DPhyp**, a dynamic programming algorithm that efficiently handles complex join predicates and non-inner joins by modeling queries as hypergraphs, outperforming existing methods in optimization time.
The paper presents a heuristic-based algorithm for selecting an effective sideways information passing strategy (SIPS) in a magic-sets transformation for a Datalog-to-SQL compiler, using abstract interpretation to estimate relation sizes, ensuring improved performance for many queries without degrading others.
The paper introduces two graph summarization operations, **SNAP** and **k-SNAP**, which enable controlled aggregation of nodes based on attributes and relationships, offering drill-down/roll-up functionality akin to OLAP, along with efficient algorithms and NP-completeness analysis for **k-SNAP**.
The paper proposes a disk-based suffix array arrangement combining a small trie and a blocked data structure to achieve faster pattern matching queries, improving speed by up to three times compared to existing methods while requiring minimal extra memory and construction overhead.
The paper proposes efficient algorithms—exact, sampling-based, and Poisson approximation—for answering probabilistic threshold top-*k* queries on uncertain data, where results must meet a user-specified probability threshold, and validates their effectiveness empirically.
The paper introduces *Lahar*, an event processing system for probabilistic event streams that improves recall and precision over deterministic methods and achieves efficient processing through novel static analysis and algorithms.
The paper presents a concurrency control algorithm that automatically detects and prevents snapshot isolation anomalies while maintaining the non-blocking benefits of snapshot isolation, providing serializable isolation without requiring application modifications.
The paper analyzes the disconnect between academic and industrial approaches to middleware-based database replication in terms of performance, availability, and administration, and proposes research agendas to bridge this gap within 5-10 years.
The paper introduces **SPEERTO**, a method for efficient top-*k* query processing in distributed peer-to-peer networks using skyline-based routing and thresholding to minimize data transfer while progressively delivering exact results.
The paper proposes a **Sampling Cube** framework for statistical OLAP over sampling data, enabling confidence interval calculations and efficient query processing by grouping similar segments and introducing a **Sampling Cube Shell** method to reduce storage for high-dimensional data.
The paper introduces **FunctionDB**, a database system that treats continuous functions (e.g., polynomials) as first-class citizens, enabling efficient and accurate algebraic querying without discretization, outperforming traditional approaches in accuracy (15-30%) and performance (10x-100x).
The paper proposes an efficient in-memory filter for approximate membership checking that quickly eliminates non-matching sub-strings without false negatives, outperforming previous methods in filtering power and speed.
The paper presents an interactive method and tool for systematically generating and refining multiple integrated schemas from source schemas using graph-based concept matching, adaptive enumeration, and user constraints to reduce alternatives and ensure information preservation.
The paper presents a self-configuring data integration system that automatically creates a probabilistic mediated schema and mappings from multiple data sources, enabling high-quality querying without initial human effort.
The paper proposes a method called "concept un-classification" to transform imperfectly integrated taxonomies from multiple sources into navigable OLAP hierarchies by minimizing imprecision while resolving concept intersections.
The paper proposes **EASE**, a unified keyword search method that efficiently indexes and queries unstructured, semi-structured, and structured data using graph summarization and an extended inverted index, achieving superior performance in both speed and accuracy compared to existing approaches.
The paper presents an efficient engine for enumerating nonredundant subtree answers in keyword proximity searches over data graphs, ensuring completeness and high correlation with ranking, along with a novel redundancy-aware ranking approach.
The paper introduces a virtualization design advisor that optimizes database performance by automatically configuring virtual machine resource allocations based on workload analysis and runtime feedback, validated using PostgreSQL and DB2 with DSS and OLTP workloads.
The paper demonstrates that column-stores significantly outperform row-stores in analytical workloads due to differences in both storage and query execution techniques, such as vectorized processing and compression, and argues that row-stores cannot fully match their performance without substantial modifications.
The paper examines how modern hardware advancements render traditional OLTP database architectures outdated and demonstrates through systematic optimizations of the Shore database that logging, latching, locking, B-tree operations, and buffer management collectively contribute to a 20x performance gap, with no single dominant bottleneck.
The paper introduces Oracle 11g's one-pass distinct sampling method for efficiently deriving table-level statistics from partition-level statistics, improving performance and scalability compared to Oracle 10g's two-pass approach.
The paper presents a heuristic and cost-based optimization approach in IBM DB2 pureXML to selectively group and order XPath expressions for concurrent evaluation, improving query performance and reducing optimization complexity.
The paper describes enhancements to .NET CLR integration in SQL Server 2008, including improved extensibility for user-defined types, aggregates, and spatial data, along with LINQ-based productivity improvements and performance optimizations.
The paper introduces **Pig Latin**, a high-level data processing language that bridges the gap between SQL's declarative style and MapReduce's procedural approach, enabling efficient large-scale data analysis on Hadoop while improving productivity and maintainability.
**Summary:** The paper introduces *Spade*, a declarative stream processing engine for IBM's *System S*, which enables rapid development of scalable, optimized distributed data-flow applications through a flexible intermediate language, built-in operators, and automatic performance tuning.
The paper proposes query-aware data stream partitioning to efficiently distribute processing load across multiple nodes in a Data Stream Management System (DSMS), optimizing performance for high-speed network traffic monitoring.
"Damia is a web-based data integration platform designed to enable business users to easily create data mashups by combining diverse sources into feeds for situational applications, integrated with IBM's Mashup Hub for enterprise use."
The paper presents an efficient RDF data management system over DB2, featuring optimized storage, ontology reasoning, SPARQL-to-SQL translation, and performance evaluation against other RDF stores.
The paper introduces SGL, a scalable declarative language for data-driven game development, demonstrating its use in building and modifying games with relational operations visualized during runtime.
The DBO database system provides scalable analytic processing over large data archives while continuously offering statistically meaningful approximate query results, enabling users to abort queries early if the approximation suffices.
"*Stretch n Shrink* is a query design framework that adjusts queries based on user feedback to achieve desired answer sizes, requiring minimal database engine modifications."
SEMMO is a scalable MMO engine with a distributed consistency protocol that enables local client computations while the central server only serializes game actions, demonstrated through the game *Manhattan Pals*.
The paper introduces Orion 2.0, an advanced uncertain database management system that natively supports probabilistic data with attribute and tuple uncertainty, arbitrary correlations, and compatibility with discrete and continuous probability distributions while adhering to Possible Worlds Semantics.
HERMES is a trajectory database engine prototype that enables aggregative Location-Based Services (LBS) by leveraging advanced spatio-temporal query processing within an Object-Relational DBMS.
"SchemaScope is a system for inferring, visualizing, cleaning, and refining XML schemas from sample documents, demonstrated through diverse use cases."
The paper presents an XML Index Advisor for DB2 that recommends optimal indexes for XML workloads by leveraging the query optimizer, demonstrating its implementation and effectiveness.
The Spicy system automates schema mapping selection by integrating schema matching, mapping generation, and verification modules, using structural analysis to improve mapping quality, demonstrating scalability and high precision in experiments.
XArch is a system for efficiently archiving and querying hierarchical data versions using a nested merge approach, with extensions for sorting large datasets and a declarative query language for version retrieval and history tracking.
Borealis-R is a replication-transparent stream processing system that ensures low latency, high availability, and consistent results in wide-area networks by deploying uncoordinated operator replicas at strategic locations while efficiently managing resources.
The Demaq system enables declarative development of distributed applications by transforming rule-based XML message logic into optimized execution plans using a specialized runtime engine.
The paper demonstrates Cascadia, an RFID-based system for extracting and managing high-level events from raw data, through a digital diary application that automatically populates a calendar with meaningful events using a building-wide RFID deployment.
The paper introduces **X.QUI.SITE**, a scalable recommendation system for social tagging sites that integrates diverse user behaviors to suggest items, people, and topics while providing explanatory insights for user decisions.
RMFinder enhances topic-based P2P pub-sub systems by dynamically clustering related topics using message similarities, enabling users to automatically receive relevant content beyond their subscriptions with minimal overhead.
"BibNetMiner is a tool for mining and analyzing large bibliographic networks like DBLP, offering functions such as clustering, ranking, and profiling of authors and conferences with a user-friendly interface."
The paper compares Hibernate and Microsoft's Entity Data Model (EDM) as ORM frameworks that enable object-oriented systems to manage relational database data safely and efficiently, particularly in multithreaded web applications.
The paper proposes a novel approach to answering why-not questions in databases by refining the original query to include missing tuples, offering an alternative to existing methods that modify the database or identify culprit operators.
The paper proposes a hierarchical browsing approach using order-based representative skylines and clustering to efficiently elicit and refine user preferences in database queries.
The paper proposes a novel, efficient, and correct criterion for determining spatial domination between multi-dimensional rectangles to enhance pruning effectiveness in query processing, outperforming existing methods.
The paper proposes new similarity measures and efficient algorithms for sampling dirty relational data to identify overlapping string-valued attributes, demonstrating improved accuracy over existing methods but highlighting a tradeoff between accuracy and speed, which motivates a two-stage filtering approach.
The paper introduces **Recsplorer**, a suite of recommendation algorithms that leverage temporal precedence patterns in user behavior to improve personalized suggestions, demonstrating effectiveness through real-world course planning data and user evaluations.
The paper introduces a tree-based index framework using directed maximal weighted spanning trees and sampling to efficiently answer label-constraint reachability queries in edge-labeled graphs.
The paper introduces **Pregel**, a scalable, fault-tolerant system for large-scale graph processing using a vertex-centric computational model designed for efficient execution on distributed clusters.
The paper introduces **PODS**, a novel system for processing uncertain data streams using continuous random variables, enabling efficient and accurate relational operations (aggregates and joins) with superior performance over sampling methods, as demonstrated in a tornado detection case study.
The paper proposes an algorithmic approach to event summarization using a hidden Markov model to capture system dynamics, achieving concise and interpretable summaries with minimal description length.
The paper introduces a novel indexing technique and road network partitioning scheme to efficiently process continuous proximity queries among moving objects in road networks, achieving a 95% reduction in overhead and a 20x speedup over baseline methods.
The paper proposes an efficient algorithm (IKNN) for finding the *k* best-connected trajectories from a database given a small set of query locations, optimizing both spatial proximity and order constraints while ensuring computational efficiency.
The paper introduces TACO, an in-network outlier detection framework for wireless sensor networks that uses locality-sensitive hashing with boosting, load balancing, and pruning to efficiently identify outliers while balancing accuracy and bandwidth usage.
The paper proposes the "all-possible-repair" semantics for inconsistent probabilistic databases, converting consistent query answering (CQA) into a problem over "repaired possible worlds," and introduces efficient pruning methods to handle the computational challenges of CQA for range, join, and top-k queries.
The paper proposes **page-differential logging**, a DBMS-independent storage method for flash memory that improves I/O performance by writing only the difference between original and updated pages, outperforming existing page-based and log-based approaches in most scenarios.
The paper introduces Ternary Locality Sensitive Hashing (TLSH), a method leveraging Ternary Content Addressable Memories (TCAMs) to achieve O(1) query time and near-linear storage for approximate nearest neighbor search in high-dimensional Euclidean space, enabling high-speed similarity search at scale.
The paper presents an automatic schema clustering and query classification approach for multi-domain pay-as-you-go data integration systems, enabling domain-agnostic integration without human intervention.
The paper proposes a keyword-based structured query language that blends keyword search with structured query processing to enable expressive and flexible access to large, heterogeneous web-extracted knowledge bases, addressing disambiguation through query rewriting and ranking.
The paper proposes a novel method for fusing multiple features (text, visual, user data) in social media using a Feature Interaction Graph and Markov Random Field for similarity evaluation, retrieval, and recommendation, demonstrating superior performance on Flickr data.
The paper proposes *k-isomorphism*, a method for anonymizing social networks by creating *k* pairwise isomorphic subgraphs to protect against structural attacks on node and link information, demonstrating its effectiveness and efficiency on real datasets.
The paper proposes **UQDT**, a privacy-preserving distributed index structure using Query Dissemination Trees to efficiently route queries to relevant publishers while enforcing *k*-anonymity and load balancing to prevent bottlenecks.
The paper proposes an efficient 3-stage MapReduce approach for parallel set-similarity joins, optimizing workload balancing, memory usage, and replication, with experimental validation on real datasets.
ParaTimer is a progress indicator for MapReduce DAGs that estimates time remaining for parallel queries by using a critical-path-based approach and handling runtime challenges like failures and skew, providing multiple scenario-based estimates.
The paper introduces **DataPath**, a data-centric database system that pushes data to processors for efficient, low-latency analytic processing, outperforming traditional compute-centric approaches.
The paper proposes **Positional Delta Trees (PDTs)**, a positional update tracking method for column stores that enhances online update efficiency while minimizing read performance overhead compared to value-based differential approaches.
The FORWARD framework simplifies Ajax-based page development by treating pages as declaratively defined, incrementally rendered views using extended SQL and page units, reducing code complexity while maintaining performance comparable to hand-coded solutions.
The paper evaluates diverse cloud-based transaction processing architectures, revealing significant variations in cost and performance across major vendors' services.
The paper proposes RT-CAN, a scalable multi-dimensional indexing scheme for cloud systems that combines CAN-based routing and R-tree indexing to efficiently process queries while maintaining low overhead.
The paper compares lightweight locking and speculative concurrency control for partitioned main-memory databases, showing that speculative concurrency outperforms locking when aborts or multi-round distributed transactions are rare, achieving up to 2× higher throughput in TPC-C benchmarks.
ExSPAN is a scalable, declarative framework for efficiently querying and maintaining distributed network provenance, enabling forensic analysis and accountability with optimized bandwidth usage.
The paper proposes a **Skew-Tolerant Histogram (STHistogram)** for geographic data that improves accuracy by detecting and leveraging hotspots in skewed distributions, outperforming existing methods.
The paper challenges the traditional histogram paradigm by introducing diverse and heterogeneous bucket types, demonstrating that they significantly improve estimation accuracy with lower space requirements compared to homogeneous histograms.
The paper evaluates probabilistic threshold queries in MCDB, a system for managing uncertain data, addressing both system and statistical challenges in answering queries that filter objects based on user-specified probability thresholds.
The paper introduces **Ad-hoc kNN (AKNN)** and **Range kNN (RKNN)** queries for fuzzy objects, proposes optimized algorithms with improved distance approximations and pruning rules, and validates their efficiency through experiments.
The paper presents an optimal reachability labeling scheme for workflow provenance, using skeleton labels derived from the workflow specification to efficiently compute compact, logarithmic-length labels for runs, enabling constant-time reachability queries.
The paper proposes a non-homogeneous generalization approach for privacy-preserving data publishing that reduces anonymization error while maintaining *k*-anonymity, introduces safeguards against new attack types, and demonstrates improved data utility through experimental validation.
The paper introduces an unsupervised method for identifying and scoring structured annotations in web queries by mapping them to relevant structured data tables and attributes, using a probabilistic model to filter out irrelevant annotations.
The paper presents scalable, quality-guaranteed active learning algorithms for record matching, specifically designed to address limitations of previous approaches by leveraging problem-specific characteristics.
The paper *I4E: Interactive Investigation of Iterative Information Extraction* presents an interactive approach for explaining, diagnosing, and repairing unexpected outputs in iterative information extraction (IIE) systems, enabling effective post-extraction investigation through algorithmic support and user interaction.
The paper proposes efficient, unbiased techniques for estimating the size and other aggregates of hidden web databases using a small number of queries, supported by theoretical analysis and experiments.
The paper proposes an index structure and lookup algorithm for efficiently querying error-tolerant set containment with string transformations, addressing the asymmetric Jaccard containment similarity for the first time.
This paper proposes a query language and storage framework for efficiently querying and managing semiring-based data provenance to support diverse applications like trust assessment and debugging.
The paper explores three approaches for tightly and efficiently integrating Hadoop with Teradata's parallel DBMS to enable optimized large-scale data analysis across both systems.
The paper introduces Ricardo, a scalable analytics platform that integrates R and Hadoop to enable sophisticated statistical analysis on large datasets by efficiently partitioning tasks between R for computation and Hadoop for data management.
The paper introduces MySpace's "People You May Know" (PYMK) friend recommendation algorithm, discussing its design, quality, and effectiveness in enhancing social connections.
The paper proposes a method for forecasting high-dimensional data by explicitly forecasting a subset of attribute combinations and dynamically predicting others using correlation models, with evaluations showing that capturing high-dimensional correlations improves accuracy, and a variant has been implemented in Yahoo!'s advertising system.
The paper describes Facebook's scalable data warehousing and analytics infrastructure, built using open-source tools like Scribe, Hadoop, and Hive, which processes over 60TB of daily data and supports diverse use cases from business intelligence to site features.
The paper describes Microsoft SQL Azure's implementation as an Internet-scale relational database service that supports full SQL and ACID transactions within consistency domains while enabling relaxed consistency across large clusters of thousands of nodes.
The paper proposes a "pay-as-you-go" architecture that creates an adaptive XML text index over LOB columns, enabling full context-aware text search via XQuery without requiring physical data migration.
The paper introduces Sedna, a native XML database management system featuring a schema-based clustering storage strategy and novel memory management for efficient querying and updating of large XML datasets.
Google Fusion Tables is a web-centric data management system designed for broader accessibility, emphasizing ease of use, collaboration, and web integration to enable non-traditional users and applications.
The paper proposes an order-independent stream processing approach using parallel sub-stream queries and deferred result consolidation to handle discontinuous data streams, addressing challenges like late data, parallelism, and fault tolerance in the Truviso Continuous Analytics system.
The paper introduces **SIE-OBI**, a streaming information extraction platform designed for operational business intelligence, enabling real-time situational awareness by extracting and correlating facts from multiple structured and unstructured data sources through a declarative interface and optimized execution framework.
HadoopDB, a hybrid of MapReduce and DBMS, demonstrates its versatility and flexible architecture in real-world applications like protein sequence analysis and TPC-H data warehousing while maintaining performance, scalability, and fault tolerance.
The paper introduces *Surfer*, a cloud-based graph processing engine that combines *MapReduce* and *propagation* primitives for efficient and flexible large-scale graph computations, offering high-level building blocks, a GUI for task creation, and execution optimizations.
The paper presents **DCUBE**, a system that discovers hidden discrimination in databases by extracting and analyzing classification rules within an Oracle database, enabling users to perform complex discrimination queries.
The paper introduces **K*SQL**, a unifying engine that supports expressive sequence pattern queries with generalized Kleene-closure and nested word models, enabling applications in relational data, XML, software traces, and genomics.
"TwitterMonitor is a real-time system that detects and analyzes emerging trends on Twitter, providing users with meaningful descriptions and interactive trend ordering options."
"Glacier is a query-to-hardware compiler that transforms streaming queries into FPGA-based digital circuits using a library of modular stream processing operators."
The paper presents a system for efficient, complete, and user-friendly keyword search on complex data graphs like DBLP and Mondial, featuring effective ranking, redundancy handling, and compact visualization of multi-node answers.
The paper presents a system that combines keyword search with OLAP-based multidimensional analysis to enable interactive exploration of aggregated corpus data, allowing users to refine searches through dynamic filtering and visualization of results.
DeepPeep is a system for discovering, organizing, and analyzing web forms to explore hidden-Web content, enabling domain-specific form collections, search engines, and visualization tools.
The Midas project develops a system to extract, integrate, and aggregate unstructured and semi-structured financial data from SEC and FDIC filings into a repository of entities, events, and relationships for applications like data mashups and risk analysis.
PIQL is a query language designed for distributed key-value stores that enables complex queries while guaranteeing predictable performance by strictly bounding I/O operations.
The paper introduces DoCQS, a prototype system that enables fine-grained data querying within document content using a relational model and a flexible Content Query Language (CQL).
QRelX is a framework that automatically generates meaningful, close-to-original database queries while ensuring desired result cardinality, using query space transformation, proximity-based search, and incremental cardinality estimation.
GDR is a guided data repair system that combines automated techniques with user feedback, using decision theory and active learning to prioritize and display repairs efficiently, improving data quality with minimal user input.
iTuned is a tool that recommends optimal database parameter settings and visualizes their impact using Adaptive Sampling, on-line experimentation with minimal overhead, and cross-platform portability.
FlexPref is an extensible framework integrated into a DBMS query processor that enables efficient preference query processing by supporting various preference evaluation methods through a simple three-function registration system.
The paper reviews database system techniques for large-scale data mining, comparing SQL and MapReduce, and highlights solved challenges and open research questions.
The paper argues that the relational model remains a vital and underappreciated foundation for database systems, critiques deviations from it in current practices, and highlights the enduring relevance and potential of *The Third Manifesto* as a blueprint for robust database design.
The paper advocates for self-tuning database technology based on feedback control loops and mathematical models, suggesting simpler, componentized architectures for scalable, autonomic E-services.
The paper introduces **REFEREE**, an open framework for developing and evaluating recommender systems using **ResearchIndex**, a large digital library of computer science papers, enabling realistic testing with rich content, user behavior, and citation data while emphasizing practical performance trade-offs over purely accuracy-based metrics.
The paper proposes a robust adaptive similarity search technique that incorporates non-relevant feedback by constructing a decision surface to separate relevant and non-relevant regions, improving search accuracy over existing methods.
The paper proposes *structural function inlining*, a technique that improves typing and optimization of structurally recursive XML queries by inlining recursive functions using type information, yielding more precise result types and efficient execution plans, with experiments showing significant performance gains over existing methods.
The paper demonstrates that XPath queries can be processed efficiently in polynomial time, contrary to existing exponential-time systems, and introduces linear-time algorithms for specific XPath fragments.
The paper proposes integrating XSL processing into database engines by translating XSL stylesheets into algebraic expressions and optimizing template rule selection and execution.
The paper presents ROLEX, a system that optimizes relational database queries to deliver efficient, consistent, and navigable XML views via a virtual DOM interface by predicting and minimizing navigation costs.
The paper proposes **C-SDA (Chip-Secured Data Access)**, a client-based security solution using smartcards to enforce data confidentiality and access control on untrusted servers by mediating access to encrypted databases.
The paper proposes a robust watermarking technique for relational databases that embeds a detectable, key-dependent bit pattern in specific attribute values to deter piracy without requiring the original data or watermark for detection.
The paper presents parametric query optimization (PQO) solutions for linear and piecewise-linear cost functions, enabling efficient query plan selection across varying parameter values by leveraging existing optimizers with minimal modifications.
PLASTIC improves query optimization efficiency by clustering similar queries, using representative plans for each cluster to reduce optimization time while maintaining near-optimal performance.
The paper proposes a generic technique to create accurate cost functions for database operations by modeling memory access patterns across hierarchical memory systems, validated through experiments in the Monet DBMS prototype.
The paper introduces **PSoup**, a system that enables both ad-hoc and continuous queries over streaming data by symmetrically processing data and queries, supporting intermittent connectivity and adaptive query processing.
The paper introduces *Aurora*, a new DBMS designed for monitoring applications that process continuous data streams from sensors, differing from traditional business data processing by emphasizing real-time operation, stream-oriented operators, and specialized optimization.
The paper introduces the XML Stream Machine (XSM), a transducer-based system that efficiently processes XQuery on XML streams by compiling queries into optimized, buffer-minimizing transducers and generating C programs for execution.
The paper proposes efficient update algorithms for structure indexes in semistructured/XML databases, focusing on bisimulation-based indexes, and demonstrates that these algorithms significantly outperform rebuilding the index from scratch.
The paper introduces the **RE-Tree**, an efficient index structure for large databases of regular expressions (REs) that accelerates matching by pruning the search space using novel measures for comparing infinite regular languages and optimized tree operations.
The paper proposes efficient structural join algorithms for XML queries using B+-trees and R-trees, with B+-tree-based methods—enhanced by sibling pointers—demonstrating superior robustness and performance over existing approaches.
The paper presents an online algorithm for skyline queries that immediately returns initial results, continuously produces more, and allows users to dynamically influence the preference order of results during execution.
The paper introduces *Progressive Merge Join (PMJ)*, a non-blocking sort-based join algorithm that produces early results during external mergesort, reducing latency while maintaining efficiency compared to traditional blocking methods.
The paper proposes a strict partial order semantics for modeling complex preferences in database systems, enabling cooperative query answering through preference constructors and algebra, with applications in personalization and e-services.
The paper proposes an efficient on-line method for multi-dimensional regression analysis of time-series data streams by using compressed regression measures, a partially materialized data cube model with a tilt time frame, and an exception-guided drilling approach to minimize storage and computational costs.
The paper presents efficient, memory-friendly algorithms for approximating frequency counts above a user-defined threshold in data streams, with bounded error guarantees, applicable to both singleton items (e.g., IP monitoring) and variable-sized sets (e.g., market basket transactions).
The paper proposes an augmented XSKETCH synopsis model that combines structural and value-distribution summaries to improve selectivity estimation for XML path queries with value predicates, along with efficient construction algorithms and experimental validation.
The paper proposes a **compressed accessibility map (CAM)** for efficient access control in XML data, leveraging structural locality to balance space and time complexity, with lookup time proportional to the item depth and log CAM size.
The paper proposes optimization techniques for securely evaluating XML twig queries by eliminating or simplifying recursive security checks based on DTD information, improving performance without violating access control policies.
The paper proposes and evaluates three sampling-based download policies that efficiently detect more changes in large-scale data environments compared to existing methods, significantly improving change detection with fixed resources.
The paper proposes an efficient invalidation framework for dynamic content caching in multitiered architectures, distinguishing invalidation from view maintenance and reducing update management costs for applications that tolerate temporary inconsistency.
The paper presents a two-phase framework, implemented in Clio, for translating between XML and relational schemas by converting high-level user mappings into semantically meaningful queries that preserve source data relationships and target constraints.
The paper introduces **COMA**, a flexible schema matching system that combines multiple matchers, including a novel reuse-oriented approach, to improve accuracy and reduce user effort in finding semantic correspondences between schemas.
The paper introduces **ProTDB**, a probabilistic XML database system designed to manage uncertain data, addressing structural and granularity challenges in XML while demonstrating efficient query implementation and application in bioinformatics and web information extraction.
The paper introduces SIMPL, a fast and accurate text classification algorithm that uses multiple linear discriminant projections and decision trees to achieve near-linear training time, outperforming naive Bayes in accuracy and rivaling SVMs in accuracy while being significantly faster.
The paper *DISCOVER: Keyword Search in Relational Databases* presents a system that enables keyword queries over relational databases without requiring schema or SQL knowledge, efficiently finding and joining relevant tuples through candidate network generation and optimized execution planning.
The paper proposes SP, an almost-serial protocol for main-memory database systems that uses timestamps and mutexes to reduce locking overhead, achieving significant throughput and response time improvements over strict two-phase locking.
The paper proposes MVCC-DW, a multi-version concurrency control mechanism for MOLAP servers that enables non-blocking, lock-free online data warehouse maintenance while ensuring serializability.
The paper introduces FAS, a freshness-aware coordination middleware for OLAP clusters that dynamically balances data freshness and query performance by scheduling updates and queries to meet client-specified freshness limits while ensuring high throughput and consistent transactions.
The paper introduces and provides efficient approximation algorithms with error guarantees for three types of reverse nearest neighbor aggregate queries (Max-RNNA, List-RNNA, Opt-RNNA) over data streams of client locations, where exact computation is infeasible.
The paper presents an efficient tree-pattern aggregation algorithm for scalable XML data dissemination, minimizing precision loss while meeting space constraints by leveraging document-distribution statistics and novel tree-pattern containment and minimization techniques.
The paper introduces **attribute translation grammars (ATGs)**, a framework for efficiently publishing relational data as DTD-conformant XML documents by extending DTDs with SQL-based semantic rules, ensuring correctness and enabling static analysis.
The paper introduces IBM's SMART project, which aims to enhance DB2 with autonomic capabilities to reduce administrative tasks and costs through features like automated deployment, maintenance, and recovery.
GnatDb is a lightweight, secure embedded database system that ensures data integrity through atomic updates and cryptographic techniques while maintaining a minimal memory footprint of under 11 KB.
The gRNA is a programmable, modular infrastructure designed to facilitate rapid development and deployment of customizable genomics-centric applications for life sciences research.
The paper presents an efficient method for performing record deletions and updates via index scans in DB2 V7, reducing unnecessary lock calls, page traversals, and I/O operations by improving synergy between query planning and data management.
The paper introduces **XMark**, a benchmark framework for evaluating XML database performance across diverse query types, providing a scalable document database and a comprehensive query set to compare XML query processors.
The paper introduces the Denodo data integration platform, a mediator system designed to address challenges in integrating heterogeneous, scattered, and weakly structured data from diverse sources, and shares insights from real-world applications.
The paper proposes a two-step query expansion method using enhanced K-means clustering on web logs to improve search results in a yellow page service by recommending similar and related categories.
The paper proposes an automated system for personalizing web portals, focusing on news feed categorization, management, and targeted advertising using domain knowledge and user data, with features for news alerts and a case study implementation.
"RTMonitor is a real-time traffic navigation system that uses mobile agents and a two-level traffic graph with an Adaptive PUSH OR PULL (APoP) scheme to efficiently maintain temporal data consistency while minimizing communication and processing overheads."
The paper demonstrates how extensibility in an object-relational database management system (ORDBMS) enables the integration of UML statecharts for dynamic integrity constraints and active behavior through a StateMachine Module.
ALIAS is an active learning-based interactive deduplication system that minimizes manual effort by requiring only simple domain-specific similarity functions and a small set of labeled record pairs to efficiently train and apply a deduplication function on large datasets.
The paper compresses the extensive and varied history of histograms—spanning their development, applications, successes, failures, and future prospects—into a concise summary while prioritizing key events and biased perspectives, with minimal loss of informational fidelity.
XSEarch is an efficient, scalable semantic search engine for XML that uses a simple query language, ranks results with IR techniques, and returns high-quality, semantically related document fragments.
The paper proposes a resilient and efficient content distribution network for streaming data that maintains coherency despite failures, with low overhead and improved fidelity through filtering and dissemination techniques.
The paper proposes a two-phase framework for clustering evolving data streams, combining online microclustering with pyramidal time frames for efficient offline analysis of dynamic data patterns over varying time horizons.
The paper demonstrates that XML tree structures can be efficiently compressed using subtree-sharing techniques from symbolic model checking and queried directly with path languages like XPath, enabling scalable in-memory processing of large XML documents.
The paper presents an efficient algorithm for phrase matching in XML documents that dynamically handles interleaved markup by leveraging inverted indices on words and tags, outperforming a nested-loop approach.
The MARS system efficiently publishes XML data from mixed (relational+XML) and redundant storage by rewriting client XQueries against a public schema into optimized queries against a proprietary schema using LAV- and GAV-style views while respecting integrity constraints.
The paper proposes an XML projection technique using static analysis to reduce memory usage in XQuery processors by loading only necessary document parts, achieving significant memory savings and performance improvements.
The paper proposes a native XML query processing system that combines tree-based navigation and information retrieval-style pattern matching for efficient query execution, demonstrating performance benefits through a cost model and experimental validation.
The paper proposes and evaluates three approaches for efficiently processing high-volume XML message brokering by leveraging a shared path matching engine to customize output and optimize post-processing.
The paper proposes an efficient index-based holistic twig join algorithm for XML documents that outperforms existing methods, particularly when join selectivities vary, while maintaining optimal worst-case I/O and CPU costs.
The paper proposes and evaluates new execution strategies for shared window joins in continuous query systems, identifying a performance flaw in existing approaches and demonstrating that the MQT strategy outperforms others across various workloads.
The paper introduces PIER, a scalable, distributed query engine using overlay networks to enable relational query processing across thousands of machines on the Internet, balancing traditional database goals with scalability needs.
The paper proposes and evaluates various tuple routing policies for adaptive optimization in distributed stream query plans, analyzing their impact on performance metrics like response time and throughput using a queuing model and discrete event simulation.
The paper introduces **AQuery**, a query language and algebra designed to inherently support ordered data, enabling efficient optimization and execution of order-dependent queries, with experiments showing significant performance improvements over SQL:1999.
The paper introduces **SASH**, a self-adaptive histogram set that dynamically adjusts to changing workloads by using query feedback to optimize and restructure histograms for accurate selectivity estimation in RDBMSs.
The paper proposes VIPAS, a novel algorithm that enhances authority search in the web by incorporating virtual links derived from user browsing behavior alongside traditional link analysis to improve result relevance and adaptability.
"Data Morphing is an adaptive, cache-conscious storage technique that dynamically reorganizes data attributes based on query workload analysis to optimize cache utilization and outperform both N-ary and PAX storage models."
The paper proposes a shrinking-based approach for multi-dimensional data analysis, which reorganizes data using a gravity-inspired technique, detects clusters through connected dense components, and selects optimal clusters via multi-scale evaluation, demonstrating effectiveness in both low- and high-dimensional spaces.
The paper introduces "data bubbles," a distance-based summarization method for non-vector metric spaces, enabling efficient hierarchical clustering with minimal runtime overhead and negligible quality loss.
The paper introduces deterministic and randomized algorithms for efficiently identifying Hierarchical Heavy Hitters (HHHs) in data streams by incorporating hierarchical structure, improving accuracy over naive approaches.
The paper introduces "coarse-grained optimization," a lightweight heuristic technique that rewrites correlated SQL statement sequences generated by OLAP tools to improve performance by exploiting inter-statement relationships.
The paper proposes and evaluates incremental multi-way join algorithms (nested loop and hash joins) with join ordering heuristics for sliding window queries over in-memory data streams, showing hash joins outperform nested loops and expiration strategies impact cost.
The paper introduces the *staircase join*, a relational database operator that enhances XML/XPath processing by leveraging tree-aware properties like subtree relationships through simple integer operations, improving performance in a main-memory DBMS.
The paper proposes a continuous data quality monitoring system called PAC-Man, which uses probabilistic, approximate constraints (PACs) to detect and address subtle, evolving data quality issues in network traffic databases for ISPs.
The paper introduces **AWSOM (Arbitrary Window Stream mOdeling Method)**, a lightweight, adaptive stream mining technique that efficiently discovers patterns and trends in sensor data with minimal resources, outperforming traditional forecasting methods in accuracy and efficiency.
The paper introduces **τXQuery**, a temporal extension of XQuery that supports valid-time queries by mapping them to conventional XQuery through optimized time-slicing techniques, including four approaches to reduce evaluation periods.
The paper generalizes the Hierarchical Pre-Grouping optimization technique for OLAP aggregate queries by providing an algebraic definition, sufficient conditions for its application using functional and inclusion dependencies, and introduces the novel Surrogate-Join transformation.
The paper introduces a measure of data clusteredness to accurately estimate the output cardinality of partial preaggregation, aiding query optimizers in deciding its effectiveness.
The BHUNT scheme automatically discovers fuzzy algebraic constraints between relational data columns using catalog information, data sampling, and statistical techniques to improve query optimization, achieving significant performance gains.
The paper proposes a MEMS-based storage data placement scheme for relational data that optimizes both row-wise and column-wise access, improving I/O utilization and cache performance.
The paper proposes a RAM-efficient query execution model with iteration filters to optimize memory usage in constrained devices, enabling co-design rules for hardware and application adaptation.
Lachesis is a storage manager that leverages device-specific performance characteristics to improve I/O efficiency, boosting DSS performance by up to 3× under concurrent OLTP workloads while simplifying configuration.
The paper identifies essential primitives for summarizing SQL workloads, highlights the limitations of current SQL in expressing these primitives, and proposes extensions to SQL and relational engines to efficiently support workload summarization.
The paper introduces a new rank-join algorithm and two nonblocking physical operators for efficiently processing top-K join queries in relational databases by progressively ranking results during the join operation.
The paper proposes AniPQO, a minimally intrusive heuristic for parametric query optimization that effectively handles nonlinear cost functions with up to four parameters while requiring only minor modifications to existing query optimizers.
The paper introduces multiscale histograms for efficiently summarizing topological relations (contains, contained, overlap, disjoint) in large spatial datasets, offering exact and approximate algorithms with constant-time querying and improved accuracy while optimizing storage.
The paper introduces a formalism for reasoning about order and grouping properties in query processing, along with a plan refinement algorithm that exploits these properties to avoid unnecessary sorting and grouping operations, reducing query evaluation costs with minimal optimization overhead.
The paper presents requirements and an algorithm for merging two formal models based on given correspondences, addressing key applications like view integration and ontology merging.
The paper proposes a cryptographic framework for enforcing access control policies on published XML documents by partially encrypting data and distributing keys to users, supported by a declarative policy language and logical optimizations.
The paper presents *AnMol*, a platform that converts biomolecular structures into graph-based vector representations to enable efficient structural queries (e.g., similarity and substructure searches) through spatial operations and inexact matching, with refinement for improved accuracy.
The paper presents a data management system for integrated mobile services, emphasizing georeferenced content linked to transportation infrastructure, covering modeling, querying, updates, and applications.
The paper presents **XISS/R**, a system that implements XML indexing and storage using a relational database, leveraging an extended preorder numbering scheme for structure-independent querying and a web interface for XPath-to-SQL translation.
The paper introduces *NexusScout*, a location-based application leveraging the *Nexus Platform* to enable mobile users to access spatial data, register events, exchange geographical messages, and locate friends through dynamic integration of distributed spatial information providers.
The paper introduces **StatMiner**, a system for efficiently estimating and managing coverage and overlap statistics in data integration, demonstrated through **BibFinder**, a mediator for computer science bibliographies that leverages these statistics to optimize query processing.
The paper introduces SQL Spreadsheet, an extension to SQL that enables efficient and scalable array and spreadsheet-like calculations for business modeling, overcoming the limitations of standard SQL and alternative solutions like spreadsheets and MOLAP engines.
"IrisNet is a scalable architecture for internet-wide sensing services, leveraging smart nodes with computing power to efficiently process and query sensor data, demonstrated through applications like parking space finding and infrastructure monitoring."
The paper summarizes methods, tools, and techniques for constructing and integrating data-centric web applications, emphasizing process modeling and web service integration.
The paper revisits the 1995 query language *W3QL*—which enabled declarative, database-inspired web searches with form navigation and optimization—and evaluates its relevance in modern search contexts like enterprise systems and the semantic web, suggesting that its principles could still enhance information retrieval.
The paper explores three thread-based techniques—running independent operations in separate threads, multi-threaded operator implementations, and work-ahead sets for cache preloading—to enhance database performance on SMT processors, achieving 30%–70% throughput improvements over single-threaded approaches.
The paper proposes using speculative threads to enable optimistic intra-transaction parallelism in database systems, minimizing DBMS modifications and programmer effort while improving performance by 36-74% for some TPC-C transactions on a 4-processor chip-multiprocessor.
The paper introduces a formal notion of *schema embedding* between XML DTD schemas to ensure information preservation during integration, proves its NP-completeness, and provides heuristic algorithms for finding embeddings.
The paper proposes a framework for designing information-preserving XML-to-relational mapping schemes by applying equivalence-preserving transformations to a known lossless and validating scheme, demonstrating its effectiveness through the LILO algorithm, which improves performance over Edge++ while maintaining practical constraint enforcement.
The paper investigates the complexity and algorithms for rewriting XPath queries using materialized views, showing coNP-hardness and Σ₃ᴾ complexity for a practical XPath fragment while providing polynomial-time solutions for three subclasses.
The paper demonstrates that using path summaries with optimization strategies in an XML query optimizer achieves performance improvements comparable to more costly index-based methods.
The paper proposes a parameter-free probabilistic method called feature-pivot clustering to detect bursty events in text streams by identifying sets of bursty features without requiring predefined parameters or positive examples.
The paper *FiST: scalable XML document filtering by sequencing twig patterns* introduces **FiST**, a scalable XML filtering system that transforms XPath twig patterns and documents into sequences using Prüfer's method, enabling efficient holistic matching via a dynamic hash-based index.
The paper proposes efficient Bottom-Up and Top-Down algorithms for computing the SKYCUBE (all possible subset skylines) by leveraging computation sharing strategies, outperforming naive approaches.
The paper introduces a semantic approach to skyline analysis by defining *skyline groups* and their *decisive subspaces*, enabling efficient computation and multidimensional exploration of subspace skylines through the *Skyey* algorithm.
The paper presents a framework for efficiently evaluating XQuery over streaming XML data by introducing optimizations for single-pass execution, a methodology to determine single-pass feasibility, and a code generation approach for handling user-defined aggregates, demonstrating significant performance improvements over existing systems.
The paper proposes *pq*-grams as an efficient method for approximate hierarchical data matching, particularly for addresses, by defining a *pq*-gram distance that approximates tree edit distance and demonstrating its effectiveness and scalability.
The TEXTURE benchmark evaluates the efficiency of text and relational query processing in RDBMSs, focusing on relevance ranking and mixed workloads, while introducing a synthetic text generator for controlled performance studies.
The paper proposes the **n-gram/2L index**, a two-level n-gram inverted index structure that significantly reduces storage space and improves query performance while maintaining the language-neutral and error-tolerant advantages of traditional n-gram indexes.
The paper proposes novel XML scoring methods (Twig and Path scoring) that integrate both structure and content, inspired by tf*idf, to improve ranked query processing efficiency and precision in heterogeneous XML repositories.
The paper proposes efficient indexing schemes based on the Relational Interval Tree to optimize combined queries on interval and simple-valued attributes in relational databases, demonstrating superior performance over existing methods.
The paper introduces **SEPIA**, a selectivity estimation technique for fuzzy string predicates that clusters strings, builds local and global histograms, and leverages edit distance to accurately predict the fraction of records matching similarity conditions in large datasets.
The paper presents efficient one-pass wavelet thresholding algorithms for minimizing maximum-error metrics in static and streaming data environments, achieving near-optimal accuracy with low space and time complexity.
The paper presents the first view matching algorithm for SPOJG views (including outer joins), handling SQL bag semantics and constraints without relying on bottom-up syntactic matching.
This paper proposes a data quality-aware cache model focusing on presence, consistency, completeness, and currency, enabling adaptive cache management and fine-grained quality guarantees through SQL extensions and dynamic query optimization.
The paper proposes a method for efficiently maintaining a semantic cache of materialized XPath views in XML databases, utilizing relational tables for storage and optimizing cache lookup and view selection based on query answerability and workload analysis.
The paper proposes optimizing nested query execution in SQL by leveraging parameter sort orders and state retention techniques to improve performance, with implementation and validation in PostgreSQL.
The paper proposes stack-based algorithms for efficiently processing path, twig, and DAG pattern queries on directed acyclic graphs without precomputing transitive closures or path indexes, achieving optimal quadratic runtime complexity relative to query variable bindings.
The paper analyzes optimal structures and synergistic benefits of interconnected spam farms (alliances) that manipulate search engine rankings and provides insights for detecting and combating link spam.
The paper extends prior work on efficient database application testing by enabling parallel execution of test runs while minimizing interference and reset operations to optimize resource usage and achieve linear speed-up.
The paper introduces **TopX**, an efficient query engine for ranked retrieval of XML documents that combines threshold-based top-k processing with optimized sequential and selective random index accesses to handle content and path conditions while supporting approximate results with minimal precision loss.
The paper proves the NP-Hardness of client-repository assignment in content dissemination networks with dynamic data and coherence requirements, proposes tailored max-flow min-cost and stable-marriage solutions, and demonstrates that dynamically adjusting delivered coherence can improve fidelity under intensive updates.
The paper proposes a decentralized, parallel algorithm for efficiently constructing structured overlay networks from scratch to support data-oriented applications like peer-to-peer databases and information retrieval, ensuring low latency and load-balancing for skewed key distributions.
SPIRIT is an efficient, single-pass streaming method that incrementally discovers correlations, hidden variables, and key trends in multiple time-series data, enabling anomaly detection, forecasting, and simplified data processing.
The paper proposes two greedy algorithms, RPglobal and RPlocal, to compress frequent-pattern sets by clustering patterns and selecting representative ones, achieving significant reduction in output size while maintaining efficiency.
The paper presents a message-oriented architecture for summarizing large databases by reducing table precision through rewriting and generalization, enabling efficient data mining and scalability for million-record datasets while supporting XML databases and incremental maintenance.
The paper introduces algorithms for online estimation of subset-based SQL queries (e.g., those with EXISTS, IN, NOT EXISTS, or NOT IN clauses) and addresses the challenge of providing probabilistic accuracy guarantees during query execution.
The paper introduces **content-based routing (CBR)**, a query processing technique that dynamically routes data subsets through different execution plans based on their statistical properties, improving efficiency over traditional single-plan optimizers.
The paper proposes a multidimensional indexing method for approximate retrieval of mixed string and numeric attributes using compressed tries for edit distance and numeric distance, demonstrating its effectiveness over alternative strategies.
The paper *Inspector Joins* introduces a hash-based join algorithm that optimizes performance by analyzing data during I/O partitioning to create specialized indexes and select the best join algorithm, achieving 1.1-1.4X speedups over state-of-the-art methods on multi-core processors.
The paper introduces **Spatio-temporal Pattern Queries (STP)**, which involve sequences of spatially and temporally constrained predicates, and proposes specialized evaluation algorithms and an index structure to efficiently process them.
The paper proposes protocols to measure and mitigate privacy leakage in distributed information sharing systems against semi-honest and malicious adversaries, classifying malicious threats as weakly or strongly malicious.
The paper demonstrates that high dimensionality severely undermines the effectiveness of *k*-anonymity in privacy-preserving data mining by making precise inference attacks feasible and forcing a trade-off between excessive data suppression and insufficient anonymity.
The paper investigates the computational complexity of checking whether a set of views violates *k*-anonymity when publishing a private relational table, identifies tractable subcases, and provides an efficient conservative algorithm for detecting violations.
The paper proposes an efficient algorithm, *TopInfluential-Sites*, to compute the top-*t* most influential spatial sites within a query region *Q* by introducing novel pruning techniques and a *minExistDNN* metric, significantly outperforming the brute-force approach.
The paper presents a general query framework and efficient algorithms for large-scale multi-structural databases, enabling real-time analysis of complex data across multiple dimensions, demonstrated on billions of web documents.
The paper introduces **eTuner**, an automated method to optimize schema matching systems by tuning their components using synthetic schemas, improving accuracy without user effort while allowing optional human input for further refinement.
The paper *"Mapping maintenance for data integration systems"* introduces **MAVERIC**, an automated system that uses lightweight **sensors** to detect broken semantic mappings in dynamic data integration environments, employing **perturbation**, **multi-source training**, and **filtering** to enhance accuracy and reduce false alarms, with experiments validating its effectiveness over real-world sources.
The paper proposes an optimization method for refreshing materialized views (MVs) in data warehouses by leveraging partition-level dependencies and query rewrite techniques to minimize refresh overhead and improve efficiency.
The paper discusses the challenges and solutions in developing the Integrated Microbial Genomes (IMG) system for managing and analyzing microbial genome data.
The paper introduces a punctuation-carrying heartbeat mechanism in Gigascope, a high-performance streaming database, to unblock operators, detect failures, and optimize performance while maintaining multi-Gigabit line speeds in industrial deployment.
The paper proposes a fuzzy Classification Query Language combining relational databases and fuzzy logic to enhance customer relationship management by improving customer equity, loyalty programs, mass customization, and marketing campaigns.
The paper introduces a flexible, scalable, and user-friendly framework for generating synthetic database distributions to address the lack of reproducible and adaptable data for evaluating database techniques.
The paper discusses MySQL Cluster 5.1's recovery principles, including an efficient synchronization algorithm for large datasets, a no-steal buffer manager choice, and data modification algorithms.
The paper describes Oracle's implementation of Support Vector Machines (SVM) in Oracle Database 10g, focusing on ease of use, scalability, and integration to overcome adoption barriers while maintaining high accuracy.
The paper discusses evolving publication practices in database research, analyzing recent changes and historical data from the *VLDB Journal* and *ACM Transactions on Database Systems*.
The paper introduces BP-QL, a query language designed for querying and analyzing business processes operating in distributed, cross-organizational environments.
MINERVA is a P2P Web search engine prototype that uses a DHT-based overlay network and distributed metadata to efficiently route queries to relevant peers, demonstrated on standard notebooks with easy USB installation.
The paper proposes database change notifications as a mechanism to efficiently cache query results by automatically invalidating cached data when underlying database changes occur.
The paper *CMS-ToPSS: efficient dissemination of RSS documents* discusses the growing popularity of user-friendly web publishing tools like blogs and content management systems, highlighting their collaborative nature and widespread adoption, and introduces an efficient method for distributing RSS content.
The paper presents a prototype integrated with Microsoft Visual Studio 2005 that interactively translates schemas between metamodels (e.g., OO to relational), generates instance-level mappings for data round-tripping, supports incremental editing, handles inheritance hierarchies flexibly, and uses high-level rules for schema transformation.
The paper introduces **AReNA**, an adaptive distributed catalog infrastructure based on relevance networks, designed to support data- and compute-intensive wide-area applications by efficiently managing large-scale server federations and client interactions.
The paper introduces **WmXML**, a system for watermarking XML documents by leveraging semantic attributes and query rewriting to ensure robustness against reorganization and alteration attacks while preserving data usability.
The paper proposes using relational query processors to evaluate XQuery by encoding XML data and queries into a relational format, leveraging existing RDBMS infrastructure for efficient processing.
The paper presents a dynamically adaptive distributed system designed to balance workload and optimize resource usage for processing complex continuous queries over data streams in a shared-nothing cluster environment.
The paper introduces **RankSQL**, a system that efficiently integrates ranking (top-k) queries into relational database management systems by extending SQL to support user-specified ranking functions and optimizing query execution for performance.
ConQuer is a system that efficiently rewrites SQL queries to retrieve only data consistent with user-specified key constraints, enabling scalable querying over inconsistent databases.
The paper proposes a QoS-based approach for optimizing data access and placement in federated systems to transparently integrate heterogeneous data sources under a unified schema while addressing performance and quality-of-service requirements.
The paper proposes a scalable framework for processing large numbers of continuous queries by dynamically clustering query predicates into hotspots and applying optimized group-processing strategies, significantly improving throughput.
The paper proposes two methods for continuous *k*-nearest neighbor monitoring in road networks, handling dynamic object and query movements and edge weight changes by either processing only critical updates or using shared execution to improve efficiency.
The paper introduces *nested mappings*, a novel schema mapping formalism that enhances existing approaches by enabling nested, correlated mappings and declarative grouping/merging semantics, improving data accuracy, reducing redundancy, and optimizing execution costs in data exchange.
The paper presents a linear-time algorithm for optimally partitioning XML document trees into storage units by allowing sibling nodes to share partitions, introduces two efficient heuristics, and demonstrates significant improvements in partition reduction and query performance compared to traditional subtree-only approaches.
The paper introduces **DiscoverXFD**, an efficient system for discovering XML functional dependencies to detect and address data redundancies in casually designed XML databases using novel data structures and partition-based algorithms.
The paper introduces algorithms (iDTD and CRX) to infer concise DTDs (SOREs and CHAREs) from XML data, demonstrating superior accuracy, conciseness, and speed compared to existing methods, with CRX specifically optimized for small datasets.
The paper proposes a secure and efficient method for querying encrypted XML databases by introducing security constraints, proving the NP-hardness of optimal encryption, and demonstrating that metadata indices enable fast query processing without compromising security against attackers with domain knowledge.
The paper proposes efficient query auditing algorithms for max and min queries in statistical databases to protect individual privacy while maintaining utility, particularly showing positive results for large databases to minimize query denials.
SMURF is an adaptive, declarative smoothing filter for RFID data streams that uses statistical sampling theory to dynamically adjust window sizes for accurate data cleaning.
The paper introduces a deferred cleansing method for RFID data analytics, where applications specify anomaly detection and correction via declarative rules, and queries are automatically rewritten to clean data efficiently at query time while leveraging SQL/OLAP functionality for scalable performance.
The paper introduces a type-based XML projection method that improves precision, reduces pruning overhead, and handles backward axes and multiple queries, with proven soundness and completeness for relevant query and DTD classes.
The paper introduces **Twig 2 Stack**, a bottom-up algorithm for processing **Generalized-Tree-Pattern (GTP) queries** over XML documents, which eliminates the need for costly post-processing operations like path-joining, sorting, duplicate elimination, and grouping while improving performance over existing methods.
The paper proposes an algebraic query model with novel semantics and efficient filters to retrieve meaningful XML fragments from nonschematic documents using keyword-based searches.
The paper introduces BP-QL, a visual query language for business processes based on BPEL, enabling distributed querying with formal foundations, efficient implementation, and compliance with industry standards.
The paper introduces the iMeMex Data Model (iDM), a unified and versatile data model capable of representing heterogeneous personal data—including structured, semi-structured, and unstructured data—within a single framework, enabling seamless queries across a user's entire dataspace.
The paper introduces *spam mass* as a measure of link spamming's impact on page rankings and demonstrates its effectiveness in detecting large-scale link spam on the Yahoo! web graph.
The paper proposes a "ranking cube" model to efficiently answer top-k queries with multi-dimensional selections by using rank-aware measures and progressive data retrieval, addressing the curse of dimensionality with ranking fragments.
The paper introduces Variable-Accuracy Operators (VAOs), a new interface for adaptive query execution that optimizes expensive modeling functions by dynamically adjusting their accuracy to meet query needs, demonstrated through financial queries on bond market data.
AFilter is an adaptable XML filtering system that leverages prefix-caching and suffix-clustering to efficiently match filter patterns in streaming XML data, improving scalability and performance over existing methods.
The paper studies maximal contained rewriting for tree pattern queries (a core subset of XPath) using views, providing algorithms and complexity results for both schema-free and schema-based cases, including recursive schemas.
The paper introduces "random pairing" (RP), a novel algorithm for maintaining uniform random samples in evolving datasets with stable sizes, and addresses resizing methods for growing datasets, proving base-data access is unavoidable for resizing.
The paper proposes a biased reservoir sampling method using temporal bias functions to maintain relevant recent data in evolving streams, proving theoretical properties for efficient implementation and demonstrating its advantages for query estimation and broader data mining tasks.
The paper proposes the *k-n-match* and *frequent k-n-match* problems to improve similarity search by dynamically matching objects in *n* dimensions, addressing limitations of traditional nearest neighbor approaches, and demonstrates their effectiveness and efficiency through experiments.
The paper proposes a progressive algorithm to efficiently compute the exact optimal location for a new site within a region \( Q \) that minimizes the average weighted distance from objects to their nearest site, using pruning techniques and lower-bound estimators to refine results iteratively.
The paper introduces *bellwether analysis*, a method to predict global query-based labels for large or evolving datasets by learning from cost-effective local subsets (*bellwethers*) when full labeling is impractical.
The paper proposes a method for automatically generating consistent and meaningful labels for attributes in integrated query interfaces across multiple web databases, ensuring user comprehension and validating the approach with experiments across seven domains.
HARBOR is an integrated recovery and high-availability approach for distributed data warehouses that leverages data replication to revive crashed sites by querying remote nodes for missing updates, avoiding stable logs and system quiescing while offering low runtime overhead and efficient recovery.
The paper proposes **strong session snapshot isolation**, an efficient method to prevent transaction inversions in lazy replicated databases while leveraging local concurrency controls.
The paper introduces *Seaweed*, a scalable query infrastructure for highly distributed datasets that trades query delay for completeness by incrementally returning results as endsystems become available, leveraging compact metadata and a DHT for resilient query dissemination and aggregation.
The paper presents *Casper*, a privacy-preserving framework for location-based services that anonymizes user locations into cloaked regions and processes queries without revealing exact positions, ensuring both service quality and user anonymity.
The paper proposes algorithms for resilient operator placement in distributed stream processing systems to withstand load variations without dynamic operator movement, addressing both linear and non-linear operator load characteristics.
The paper proposes a feedback control-based load shedding framework for stream databases that dynamically adjusts data dropping to maintain desired processing delay with minimal QoS violations and computational overhead.
The paper proposes a "Window Drop" operator for load shedding in data stream management systems, which preserves window integrity by probabilistically dropping entire windows in aggregation queries while minimizing result quality degradation.
The paper proposes *Flowcube*, a method to construct a multi-dimensional warehouse of RFID-based commodity flowgraphs that captures movement trends and deviations at various aggregation levels, enabling efficient analysis through path abstraction, pruning, and compression.
The paper proposes an approximate bitmap encoding scheme using multiple hashing that enables direct access and efficient query processing over compressed bitmaps, significantly improving speed (1-3 orders of magnitude faster than WAH) while controlling accuracy (90%-100%) and avoiding false misses.
The paper introduces *LB_Keogh*, a method that enables exact and efficient rotation-invariant shape matching across various representations and distance measures, significantly speeding up existing approaches without sacrificing accuracy.
The paper proposes an efficient distance signature index for computing and querying long-distance network distances on road networks by discretizing and encoding distances into optimized categories, demonstrating robustness across various conditions.
The paper analyzes two existing dynamic programming algorithms for generating optimal bushy join trees, showing their complementary strengths for chain and clique queries but poor performance on star queries, and introduces a new adaptive algorithm that outperforms both by dynamically adjusting to the query graph's search space.
The paper introduces ULDBs, an extension of relational databases that integrates uncertainty and lineage to enable consistent representation, efficient query processing, and new probabilistic query approaches.
The paper proposes a probabilistic database model that combines row- and column-level uncertainty to better approximate extraction distributions from imperfect information extraction tools, along with efficient algorithms for parameter optimization.
The paper proposes a secure and efficient scheme for creating a trustworthy inverted index on WORM storage to enable reliable keyword searches while ensuring regulatory compliance and maintaining good query performance.
The paper describes the query processing architecture, compiler, runtime system, distributed techniques, SQL translation, and lineage features of BEA's AquaLogic Data Services Platform (ALDSP), which uses XML, XQuery, and extensions to enable SOA-based data integration.
The paper describes the development and key features of TPC-DS, a new decision support benchmark designed to reflect modern industry trends and technologies while maintaining ease of use.
The paper describes the SAP NetWeaver BI accelerator's architecture and demonstrates its capability for efficient online data mining, particularly association rule mining, using in-memory aggregation and achieving fast query times on unprocessed business data.
The paper introduces DB2's Self-Tuning Memory Manager (STMM), which uses cost-benefit analysis and control theory to dynamically optimize memory allocation across diverse database components, achieving performance comparable to expert manual tuning.
The paper introduces **NUITS**, a prototype system for efficient and user-friendly keyword search over relational databases, supporting both simple and advanced queries with optimized processing and result display.
IPAC is an interactive framework for specifying XML access constraints and selecting security views, featuring a declarative language, global strategy configuration, and automated view generation to help DBAs implement optimal access control plans efficiently.
The paper introduces SIREN, a similarity retrieval engine that extends SQL syntax to enable similarity queries for complex data in relational DBMS.
"PARAgrab is a scalable web image management system supporting multimodal querying, archival, retrieval, and annotation with advanced visualization techniques."
The paper presents an interactive schema matching tool that incrementally suggests and ranks candidate matches for selected elements, improving usability by reducing false positives and enabling efficient navigation, integrated with Microsoft BizTalk Mapper.
OntoQuest is a user-friendly system that simplifies ontological data exploration through context-aware query guidance, extended OWL-to-database mappings, and bulk instance management, without requiring knowledge of ontology query languages.
HISA is a novel system that bridges the semantic gap in large image databases by combining ontological knowledge and visual features, enabling efficient retrieval via keywords, image examples, or both.
GMine is a scalable, interactive graph visualization and mining system that addresses challenges of large graphs through multi-resolution exploration using a G-Tree hierarchy and summarization via subgraph extraction.
The paper demonstrates the feasibility and benefits of building entire distributed sensor network systems declaratively using SQL and Datalog variants, highlighting rapid development, flexibility, efficiency, and uncovering similarities between sensor networks and database concepts.
R-SOX is a system for runtime semantic query optimization over XML streams that leverages dynamic schema knowledge to enhance query performance through annotation, incremental maintenance, and dynamic optimization techniques.
The paper introduces *MyPortal*, a tool for robustly extracting and aggregating web content blocks, while addressing related research challenges.
Crimson is a specialized tree storage system designed to efficiently manage and query large phylogenetic trees for evaluating tree reconstruction algorithms, supporting structure-based queries and benchmarking within the CIPRes project.
The paper "HUX: Handling Updates in XML" presents a system that reliably and efficiently translates XML view updates by first filtering untranslatable updates (U-Filter) and then finding optimal translations (U-Translator) for valid ones.
"InteMon is an intelligent, real-time cluster monitoring system that uses SNMP, MySQL, and stream mining to detect anomalies, identify their causes, and provide interactive visualization via a JSP web interface."
The paper introduces *LGeDBMS*, a lightweight database management system designed for embedded systems with flash memory, addressing its unique read/write/erase characteristics and demonstrating its use in a mobile PIM application.
The paper introduces randomized sampling algorithms for efficiently analyzing large-scale matrix and tensor data, covering theoretical foundations and applications in biological and internet datasets.
The paper proposes three authenticated join algorithms (AISM, AIM, ASM) for outsourced databases, which verify query result correctness using data owner signatures and outperform existing methods while shifting computational load to the service provider.
The paper introduces **Privacy Integrated Queries (PINQ)**, an extensible platform that enables privacy-preserving data analysis through a SQL-like interface while providing formal differential privacy guarantees without requiring trust in the analyst's expertise.
The paper proposes a declarative approach using the hlog language to enable users to directly provide and automatically process feedback in information extraction and integration programs, improving efficiency and accuracy.
The paper introduces *skip-and-prune (SnP)*, an efficient top-*k* query processing algorithm for non-monotonic cosine-based scoring in context-sensitive document retrieval, outperforming naive top-*k*, accumulator-based inverted files, and full-scan in both speed and precision.
The paper demonstrates a privacy attack exploiting exchangeability and de Finetti’s theorem, showing that data sanitization schemes like Anatomy—and others relying on random worlds, i.i.d., or tuple-independent models—are vulnerable even with minimal background knowledge, requiring only one individual’s nonsensitive attributes and a machine learning model trained on sanitized data.
The paper proposes techniques to protect sensitive aggregate information in hidden databases from being inferred through query responses, addressing a privacy concern opposite to traditional approaches that safeguard individual tuples while allowing aggregate access.
The paper proposes a grammar-based declarative framework for data representation and cleaning, combining generative grammars, database querying, and compiler-like actions to normalize and minimize cleansing needs, with empirical validation on real-world scenarios.
SELinks is a secure multi-tier web programming language that enforces customizable, object-level security policies through a novel type system (Fable) and database-resident policy functions, improving both security and performance.
The paper presents **Delex**, a system that extends **Cyclex** by optimizing complex information extraction (IE) workflows over evolving text data, efficiently recycling previous IE results to reduce processing time across multiple IE blackboxes.
The paper proposes a probabilistic tree-edit model to robustly adapt web wrappers to evolving HTML structures by learning edit operations from historical snapshots, enabling efficient wrapper maintenance.
The paper proposes efficient algorithms for retrieving the top-k tuples with the smallest expected ranks from distributed probabilistic data while minimizing communication costs.
The paper proposes efficient algorithms to compute typical top-*k* answer vectors that sample the score distribution of uncertain data, addressing ambiguity in query semantics and handling score ties.
The paper proposes a query simplification framework that gracefully degrades join-order optimization by safely reducing the join graph complexity when exact optimization is infeasible, ensuring robust performance across various query types and structures.
The paper proposes a hierarchical indexing structure based on junction trees and tree partitioning to efficiently manage and query large-scale correlated probabilistic databases, significantly improving performance for inference and aggregation queries while supporting dynamic updates.
The paper proposes a dynamic indexing technique for skyline points, integrated into sort-based skyline algorithms, to improve computational performance by reducing dominance checks and enabling efficient bitwise operations, scaling well with input size and dimensionality.
The paper introduces *Query By Output (QBO)*, a data-driven approach that enhances database usability by generating instance-equivalent alternative queries from a given query's output using a novel classification-based technique and optimization strategies.
The paper proposes polynomial-time algorithms to detect and minimally correct unsound workflow views—which distort dataflow and provenance analysis—by splitting composite tasks, proving the problem NP-hard and demonstrating the effectiveness of the solutions.
The paper proposes ROX, a run-time XQuery optimizer that uses sampling and materialization of partial results to overcome limitations of traditional static optimization, ensuring robust performance with controlled overhead.
The paper proposes an automatic top-k ranking algorithm for schema integration that uses directed and weighted correspondences to generate the best candidate schemas efficiently, reducing reliance on manual input.
The paper introduces efficient algorithms for generating executable scripts that compute core solutions in data exchange, bridging the gap between practical mapping tools and theoretical data exchange research while demonstrating superior performance over existing methods.
The paper introduces **FlexRecs**, a declarative framework for defining and combining customizable recommendation workflows over structured data, enabling flexible experimentation and implementation with extensible performance.
The paper proposes an efficient algorithm for approximate dictionary matching with edit distance constraints to improve named entity recognition by addressing typographical errors and outperforming existing methods by up to an order of magnitude.
GAMPS is a polynomial-time approximation framework that compresses multi-sensor data by dynamically grouping correlated signals and scaling their amplitudes to minimize storage while ensuring bounded reconstruction error and efficient querying.
The paper introduces **3-HOP**, a high-compression indexing scheme for reachability queries on dense directed acyclic graphs (DAGs), combining chain structures and hops to minimize index size while maintaining efficient query performance compared to existing methods like 2-hop and path-tree.
The paper introduces **Everest**, Yahoo!'s SQL-compliant, columnar, massively parallel data warehousing engine built on commodity hardware to efficiently manage petabyte-scale data for business insights at lower costs.
The paper summarizes advancements in flash memory SSD technology, demonstrating its superiority over traditional disk drives in transaction throughput, cost, and energy efficiency for enterprise databases.
The paper introduces the Volt optimizer framework in the ParAccel Analytic Database, which efficiently handles large join queries by partially ordering them before applying dynamic programming, demonstrating improved performance.
HoTMaN is a tool developed by MySpace and USC to automate and simplify the management of large-scale database storage, enabling rapid failover to hot-standby servers and reducing human error in operations.
The paper proposes a **Business Transaction framework** in Oracle RDBMS that enables long-lived, discontinuous transactions with **compensation-aware data types** to support shared updates while ensuring recoverability without cascading aborts.
The paper describes the access control mechanisms in the AquaLogic Data Services Platform (ALDSP), including its securable resource hierarchy, fine-grained schema security, XQuery-based policies, user identity mapping, and implementation techniques to ensure efficient query processing and caching.
The paper proposes a QoX-driven ETL design approach that integrates quality metrics to automate and optimize ETL processes, reducing costs and improving design outcomes.
The paper explores how Social Web technologies like Wikipedia and social tagging systems can enhance group cognition by improving memory, reasoning, and collaboration through mechanisms such as conflict resolution, reduced interaction costs, and computational organization of user-generated content.
The paper introduces Dryad and DryadLINQ, systems that enable distributed data-parallel computing by generalizing SQL and MapReduce, integrating strongly typed .NET objects, and supporting imperative and declarative operations within a high-level programming language while automating distributed execution.
The paper discusses techniques and challenges in managing large-scale uncertain data, integrating methods from databases, optimization, and statistical learning to improve decision-making under uncertainty.
The paper explores the potential integration of FPGAs as specialized processing units in database systems, highlighting their benefits, challenges, and development considerations to optimize performance and energy efficiency.
The paper provides a comprehensive overview of state-of-the-art techniques, applications, and future challenges in keyword search for structured and semi-structured data across various models.
The paper presents a unified framework for data anonymization techniques, viewing them through the lens of uncertainty as generating possible worlds, and connects privacy guarantees and query evaluation to uncertain data models while identifying new research directions.
PRIMA is a system that archives historical data under their original schemas while allowing users to query them using the current schema, automatically rewriting queries across schema versions and supporting temporal queries on both data and metadata.
DejaVu is a declarative event processing system that enables uniform pattern matching over live and archived event streams, demonstrated through smart library and financial market applications with interactive visualization tools.
**Summary:** *StreamShield* introduces a stream-centric security model using embedded "security punctuations" to enforce dynamic, flexible access control in Data Stream Management Systems (DSMSs), improving efficiency and adaptability.
The paper introduces *Grapevine*, a system that performs real-time large-scale analysis of social media content to track and explore trending entities and stories across demographics, enabling interactive discovery of user-specified topics.
The paper introduces *Perm*, a comprehensive provenance management system that computes, stores, and queries relational data provenance using query rewriting, enabling provenance handling with standard database techniques.
The paper introduces **Ferry**, a language and environment that allows developers to write database operations using programming language syntax, which is then compiled into efficient SQL for execution close to the data.
The paper presents the **SearchSpaceAnalyzer System**, a tool for visualizing and comparing the SQL Anywhere query optimizer's search spaces to diagnose performance issues related to suboptimal or inconsistent execution plans.
The paper demonstrates that OLAP exploration can be efficiently performed within a modern DBMS using SQL and UDFs, comparing three methods—plain SQL, lattice-based UDFs, and star cube UDFs—without requiring external servers or data export.
The paper presents **AIDE**, an ad-hoc intents detection engine that identifies hidden user query intents by analyzing frequent co-asked phrases in query logs, enabling real-time intent detection over large-scale search data.
**Summary:** Schemr is a schema search engine that enables keyword- and example-based schema retrieval using text search and schema matching, with a GUI for visualizing and exploring results.
XSearcher is an associative memory-based desktop search system that leverages semantic links from user activities to personalize search results by relevance and importance, mimicking human memory processes.
"ExQueX is an interactive system for exploring and querying XML documents through search, ranking, and filtering, enabling users to discover relationships and construct queries using exploration results."
The paper demonstrates how SystemT's information extraction technology, integrated with InfoSphere MashupHub, enables enterprise mashups by converting unstructured text feeds into structured data for join and aggregation operations.
The paper presents *Hermes*, an infrastructure for expressive keyword search on the data web, translating user queries into structured queries over integrated heterogeneous sources using schema-level mappings for distributed processing.
The paper analyzes a 12-week trace of a local-area wireless network, revealing distinct user sub-communities with varied movement and usage patterns, dominant web and chat activities, and traffic asymmetry during peak throughput, offering insights for similar environments.
The paper introduces a time-indexed SMDP model (TISMDP) to derive optimal dynamic power management policies for portable systems, achieving significant power savings compared to default algorithms for devices like SmartBadge, laptop hard disks, and WLAN cards.
The paper summarizes the development and deployment of the GUIDE system, a context-aware tourist guide that integrates wireless communication, information modeling, and distributed application design, offering a practical blueprint for future context-aware systems.
The paper proposes an approach where mobile hosts in disconnected ad-hoc networks actively modify their trajectories to ensure minimal-time message delivery, with algorithms developed for both known and unknown node movement scenarios.
The paper proposes a two-tier packet scheduling model for multihop wireless networks that ensures fair minimum bandwidth allocation while maximizing spatial reuse, along with an ideal centralized algorithm and a practical distributed CSMA/CA-based approximation.
The paper proposes a general analytical framework for achieving system-wide MAC layer fairness in wireless networks by designing local contention resolution algorithms that optimize per-flow utility functions without requiring global coordination.
The paper proposes an intersystem location tracking mechanism using boundary location areas (BLA) and boundary location registers (BLR) to reduce costs, call loss rates, and paging delays in multitier wireless networks.
The paper proposes a hybrid distributed-centralized tracking method for mobile users with uncertain parameters, combining distance-based updates and load-sensitive timers to reduce wireless costs while bounding worst-case tracking expenses.
The paper introduces **GLS (Geographic Location Service)**, a scalable, decentralized location service for mobile ad hoc networks that efficiently tracks node positions using a predefined hierarchy and identifier ordering, enabling robust geographic routing with low overhead and graceful degradation under node failures.
The paper presents a unified framework for header compression using a platform-independent description language and a code generation tool to simplify implementation across various protocols without significant performance loss.
The paper presents and evaluates six cell switching techniques (Late, Early, Strong, and three variations) to optimize when a mobile node should reattach to a wired network in a wireless computing environment.
The paper presents an end-to-end host mobility architecture using DNS updates and TCP connection migration, offering a secure and efficient alternative to Mobile IP without requiring changes to the IP substrate.
The paper presents a fully distributed fair scheduling algorithm for wireless LANs, derived from IEEE 802.11's DCF, which allocates bandwidth proportionally to flow weights without centralized control and requires minimal modifications to the standard.
The paper proposes and evaluates downlink scheduling algorithms for CDMA packet data networks, demonstrating that algorithms leveraging request sizes and channel conditions improve throughput, while optimal multiplexing (time or combined time-code) balances user satisfaction and network performance, though discrete bandwidth conditions may degrade user experience without significantly affecting network metrics.
The paper proposes a dynamic SINR adaptation scheme for CDMA networks that optimizes network-level QoS and energy efficiency by adjusting target SINR requirements based on channel conditions and QoS needs, outperforming static SINR approaches.
The paper proposes a scalable, low-latency cache invalidation strategy for mobile environments that reduces query latency and bandwidth usage by improving upon traditional invalidation report (IR)-based methods.
The paper proposes a semantic caching scheme for location-dependent data in mobile computing, demonstrating its flexibility and effectiveness over page caching through a mobility model, query processing strategies, and performance evaluation using the FAR replacement strategy.
The paper introduces a clients-providers-servers model for limited-bandwidth systems, demonstrates that a random file order performs well for diverse client access patterns, and provides a deterministic polynomial-time algorithm to achieve similar performance bounds.
The paper analyzes the impact of cache structure, capacity, and timeout strategies in on-demand routing protocols for wireless ad hoc networks, using simulations of the Dynamic Source Routing (DSR) protocol across various mobility scenarios and introducing new mobility metrics to evaluate routing performance.
The paper proposes *watchdog* and *pathrater* mechanisms to detect and avoid misbehaving nodes in mobile ad hoc networks, improving throughput by up to 27% despite high misbehavior rates, albeit with increased routing overhead.
The paper proposes a new application model for pervasive computing by redefining the roles of devices, applications, and user environments, outlining key attributes and challenges to achieve this vision.
The paper examines vulnerabilities in wireless ad-hoc networks, highlights the limitations of existing intrusion detection methods, and proposes new detection and response mechanisms tailored for such networks.
The paper introduces *W-mail*, an adaptive email system for wearable computing environments that extends traditional email formats and architectures by embedding context-aware commands (e.g., time, location, sensor status) to enable hands-free, always-on functionality tailored to daily life.
The paper outlines the social relevance and scientific potential of Environmental Observation and Forecasting Systems (EOFS) and highlights key challenges in wireless networking, such as media protocols, ad-hoc flooding algorithms, and mobile computing, required to advance these systems.
The paper presents the **Cricket compass**, a system that uses ultrasonic sensors and fixed beacons to accurately determine a mobile device's orientation within a few degrees, enabling enhanced context-aware applications like navigation and augmented reality.
The paper proposes a location-aware hoarding mechanism for mobile information systems to improve data access efficiency by prefetching location-dependent information, achieving higher hit ratios than traditional caching.
The paper proposes VIA, an application-level protocol that enables distributed, self-organizing data sharing across isolated service discovery domains by linking directories into hierarchical clusters for efficient querying and robust operation.
The paper proposes a proactive routing enhancement for on-demand ad-hoc networks by predicting and preemptively repairing likely path breaks based on received signal power, reducing broken paths and latency with minimal overhead.
The paper introduces MERIT, a framework for evaluating routing protocols in mobile ad hoc networks by comparing actual route costs to optimal mobile path costs using the concept of shortest mobile paths, with an efficient algorithm for practical implementation.
The paper demonstrates that the capacity of wireless ad hoc networks is constrained by traffic patterns and network size, with per-node capacity scaling poorly unless average source-destination distances remain small as the network grows.
The paper introduces the **Geographical Adaptive Fidelity (GAF)** algorithm, which reduces energy consumption in ad hoc wireless networks by selectively turning off redundant nodes while maintaining routing fidelity, achieving **40–60% energy savings** and extending network lifetime proportionally with node density.
**Summary:** Span is a distributed, energy-efficient algorithm for ad hoc wireless networks that dynamically selects coordinator nodes to maintain network connectivity and capacity while allowing non-coordinator nodes to sleep, significantly improving system lifetime without degrading performance.
The paper proposes and evaluates online power-aware routing algorithms, including max-min zPmin and zone-based routing, to optimize network lifetime in wireless ad-hoc networks, demonstrating their effectiveness despite the lack of a constant competitive ratio compared to offline optimal solutions.
The paper proposes the Appliance Data Services (ADS) architecture, which simplifies appliance computing by enabling seamless data movement across devices through principles like minimizing device features and leveraging network infrastructure, making devices easier to use and more functional.
The paper introduces *m-Links*, a middleware proxy system that enables small internet devices (like wireless phones) to navigate and access web content efficiently by restructuring link displays, using data detectors, and supporting extensible network-based services.
The paper proposes a "Smart Kindergarten" system using sensor-enhanced, wirelessly-networked toys and objects to create adaptive, individualized early childhood learning environments with real-time monitoring and evaluation capabilities.
The paper defines exposure in wireless ad-hoc sensor networks, develops an efficient algorithm for calculating minimal exposure paths to assess worst-case coverage, and evaluates its performance through extensive experiments.
The paper discusses the potential of biomedical smart sensors and highlights the unique research challenges in designing wireless networks for human-embedded sensors, emphasizing the need for bio-compatible, energy-efficient, ultra-safe, and application-specific solutions.
The paper presents AHLoS, a system that enables ad-hoc sensor networks to dynamically and accurately determine node locations through distributed iterative algorithms, achieving centimeter-level precision in prototype tests.
The paper reveals critical security flaws in the 802.11 WEP protocol due to cryptographic missteps, enabling practical attacks that undermine its intended protection.
The paper "SPINS: Security Protocols for Sensor Networks" introduces SNEP for secure data communication and μTESLA for authenticated broadcast in resource-constrained sensor networks, demonstrating their practicality and efficiency.
The paper proposes two distributed mechanisms—distributed priority scheduling and multi-hop coordination—to approximate ideal QoS scheduling in multi-hop wireless networks by leveraging priority tagging and adaptive priority adjustments, supported by analytical modeling and simulations.
The paper presents three deterministic, collision-free channel access protocols for ad hoc networks that ensure fairness and maximize bandwidth utilization by resolving contention based on neighbor identifiers.
The paper proposes an adaptive rate control mechanism for sensor networks that ensures energy efficiency and fair bandwidth allocation across multihop nodes under varying traffic conditions.
The paper introduces the Receiver-Based AutoRate (RBAR) protocol, a rate-adaptive MAC protocol for multi-hop wireless networks that improves throughput by dynamically adjusting data rates based on receiver-side channel quality estimation, outperforming sender-based approaches like WaveLAN II.
The paper demonstrates that dynamic voltage and frequency scaling in a low-power microprocessor significantly reduces energy consumption per instruction at lower speeds, enabling power-aware applications to adjust performance efficiently, as shown by improved performance in an H.263 video decoder compared to static and interval-based approaches.
The paper presents a Linux kernel-based software mechanism that automatically controls dynamic voltage scaling to optimize energy use (up to 75% savings) while maintaining user-specified interactive performance, without requiring modifications to user programs or workload constraints.
The paper proposes a physical layer-driven approach to designing energy-efficient protocols, algorithms, and applications for wireless sensor networks to maximize system lifetime by leveraging hardware-aware optimizations.
TCP Westwood (TCPW) enhances TCP performance in wireless and wired networks by using end-to-end bandwidth estimation to adjust congestion window and slow-start threshold more accurately after packet loss, avoiding unnecessary reductions caused by wireless channel errors while maintaining fairness and compatibility with TCP Reno.
The paper introduces four agile yet stable network capacity estimation filters, with a statistical process control-based filter outperforming traditional EWMA filters by balancing quick adaptation to persistent changes and resilience to transient noise.
The paper proposes and analyzes a novel **multiple-burst admission-spatial dimension (MBA-SD)** algorithm for efficient burst admission control in **cdma2000** systems, formulating it as an **integer programming problem** and demonstrating significant improvements in **capacity, coverage, and admission/outage probabilities** compared to existing methods.
The paper describes the mobile connectivity protocols and throughput performance of the Ricochet MCDN system, a frequency-hopping microcellular mesh network enabling seamless handoffs for users moving up to 70 mph, along with a predictive model for throughput based on velocity.
The paper proposes IP paging as a network-layer service to efficiently locate mobile hosts across diverse wireless interfaces, reducing power consumption and eliminating redundant application-layer implementations.
The paper proposes a cost-efficient Mobility Application Part (MAP) signaling protocol for IMT-2000 systems that reduces overhead, latency, and call loss by updating location information before crossing network boundaries.
The paper proposes **Zero-Interaction Authentication (ZIA)**, a system where a wearable token provides short-range wireless decryption authority to a laptop, ensuring secure file access only when the user is nearby while minimizing performance overhead.
The paper presents *Ariadne*, a secure on-demand routing protocol for ad hoc networks that ensures path authenticity and integrity through efficient cryptographic mechanisms.
The paper introduces the Opportunistic Auto Rate (OAR) protocol, which enhances throughput in multirate ad hoc networks by opportunistically transmitting multiple back-to-back packets during high-quality channel conditions while maintaining fairness over longer timescales.
The paper proposes an improved power control MAC protocol for ad hoc networks that avoids throughput degradation and reduces energy consumption compared to existing IEEE 802.11-based power control schemes.
The paper proposes ROMA, a distributed receiver-oriented channel access scheduling protocol for ad hoc networks with directional antennas, which improves throughput and delay by leveraging multi-beam capabilities and two-hop topology information, outperforming random access and static scheduling schemes.
The paper proposes a directional antenna-based MAC protocol for ad hoc networks that improves spatial reuse and reduces hop counts by using multi-hop RTS and single-hop CTS/DATA/ACK transmissions, outperforming IEEE 802.11 in certain scenarios.
The paper analyzes TCP/IP performance over 3G wireless links with rate and delay variation, proposes a model to predict throughput, and introduces an **Ack Regulator** to improve TCP performance by up to 40% by mitigating variability effects.
The paper proposes **pTCP**, a transport layer protocol that effectively aggregates bandwidth across multiple wireless interfaces on multi-homed mobile hosts, outperforming link-layer and application-layer striping approaches.
The paper presents **rocks** and **racks**, two user-level systems that enable transparent network connection mobility by detecting failures, preserving suspended connections, and automatically reconnecting with correct data recovery, while maintaining interoperability with standard sockets and introducing minimal overhead.
The paper analyzes an 11-week trace of a large campus-wide wireless network, revealing that residential traffic dominates, wireless laptops are primary devices, web protocols lead traffic volume but file sharing is significant, and excessive roaming—especially cross-subnet—disrupts connections, highlighting the need for better roaming solutions.
The paper introduces the Bounded-Slowdown (BSD) protocol, a dynamic power-saving method for wireless web access that optimally reduces energy consumption while limiting latency increase, improving web page retrieval times by 5–64% and cutting energy use by 1–14% compared to static power-saving modes.
The paper proposes a link-layer-assisted Mobile IP fast handoff method over wireless LANs, using access points and a MAC bridge to reduce packet loss and latency without modifying Mobile IP standards.
The paper explores the interdependence of routing and data compression in multi-hop sensor networks, demonstrating conditions for efficient data transmission with prescribed distortion and proposing stochastic models where joint compression reduces the overall rate/distortion function.
The paper proposes **TTDD (Two-Tier Data Dissemination)**, a scalable and energy-efficient data delivery model for large-scale wireless sensor networks with mobile sinks, where stationary sensor nodes proactively form a grid structure to minimize overhead while enabling continuous data updates to moving sinks.
The paper introduces a "wake-on-wireless" energy-saving strategy that extends battery life in PDA-based phones by up to 115% by powering down the device and wireless card during idle periods and activating them only upon receiving an incoming call.
The paper proves the NP-completeness of the minimum-energy broadcast problem in all-wireless networks for both general and geometric cases and proposes a distributed heuristic called Embedded Wireless Multicast Advantage (EWMA) that performs competitively with existing solutions.
The paper analyzes the upper bounds on network lifetime extension using cooperative cell-based energy conservation techniques in stationary ad hoc networks, showing significant lifetime improvements even at low node densities.
The paper presents a microeconomics-based framework for decentralized resource control in CDMA networks, optimizing transmission rate and signal quality to maximize user utility while addressing congestion and battery power costs.
The paper analyzes the uplink capacity of power-controlled CDMA networks with best-effort applications, showing that slowing transmission rates in a single-cell homogeneous system increases capacity and identifies a limit capacity where blocking probability can be minimized by rate control, extending these findings to heterogeneous and multi-cell scenarios.
The paper proposes efficient approximation algorithms for partitioning multi-hop wireless networks into QoS-constrained clusters with minimal access points and introduces an adaptive delivery mechanism to maximize throughput while meeting bandwidth and delay requirements.
The paper presents a robotics-inspired system that uses probabilistic inference on measured IEEE 802.11b signal strengths to achieve accurate indoor location tracking with off-the-shelf hardware.
The paper proposes an optimal tradeoff method between transcoding overhead and storage by selectively pre-adapting content variants and dynamically generating the rest, with proven algorithmic bounds and validated through simulation and a PDF adaptation trial.
"Rajicon enables remote PC GUI control via mobile devices through image-based navigation, mouse/keyboard operations, and specialized functions, addressing the limitations of existing solutions for on-the-go access."
The paper argues that pervasive computing must address environmental impacts like waste and energy consumption by developing sustainable architectures, design methods, and software solutions across the device lifecycle.
The paper explores the challenges and potential solutions for integrating mobile wireless devices into the computational grid, proposing a proxy-based clustered architecture to overcome limitations like low performance and unreliable connectivity while leveraging their widespread availability.
The paper proposes "recombinant computing" and the Speakeasy approach, enabling interoperability among devices, applications, and services with minimal prior knowledge by using mobile code and common interaction patterns for ad hoc configurations.
The paper proposes RCP (Reception Control Protocol), a receiver-centric transport protocol for mobile hosts with heterogeneous wireless interfaces, offering improved congestion control, loss recovery, power management, seamless handoffs, and bandwidth aggregation compared to sender-centric approaches.
The paper proposes **Neighborhood RED (NRED)**, a network-level scheme that extends Random Early Detection (RED) to distributed neighborhood queues in ad hoc wireless networks, significantly improving TCP fairness without requiring MAC-layer modifications.
The paper compares five Mobile IP(v6) handoff mechanisms—Hierarchical Mobile IPv6, Hierarchical Mobile IPv6 with Fast-handover, Mobile IPv6 with Fast-handover, Simultaneous Bindings, and S-MIP—showing that S-MIP outperforms others with minimal latency and packet loss, and proposes an optimization to prevent out-of-order packets.
The paper develops tight conditions and efficient approximation schemes for achievable rates in multi-hop wireless networks by jointly optimizing routing and scheduling, guaranteeing solutions within 67% of optimal in the worst case and typically closer to 80%.
The paper demonstrates that random ad hoc networks with infrastructure support achieve a per-node throughput capacity of \( \Theta \left( \frac{W}{\log N} \right) \), a significant improvement over purely ad hoc networks, under bounded node-to-access-point ratios and connected topology assumptions.
The paper analyzes the impact of wireless interference on multi-hop network throughput by modeling it with a conflict graph, presents methods to compute performance bounds, and demonstrates through simulations that interference-aware routing can improve throughput over traditional shortest-path routing.
The paper presents APIT, a range-free localization algorithm for large-scale sensor networks that performs well under irregular radio patterns and random node placement, with minimal impact on routing and tracking when localization error is below 0.4 times the communication radius.
The paper proposes a scalable coordinate-based routing algorithm that operates without location information, enabling efficient routing in ad hoc and sensornet environments where GPS or other location data is unavailable.
The paper presents polynomial-time approximation algorithms for optimizing location area planning in personal communication systems to minimize bandwidth usage while ensuring QoS, with provable performance guarantees in planar and general graphs, and an optimal solution for line graphs.
The paper presents polynomial-time algorithms for finding minimum-energy node- and link-disjoint paths in wireless ad-hoc networks, showing that link-disjoint paths consume less energy than node-disjoint ones and that additional paths can be obtained at lower energy due to wireless broadcast advantages.
The paper introduces the **Expected Transmission Count (ETX)** metric, which improves multi-hop wireless routing by selecting high-throughput paths based on link loss ratios, asymmetry, and interference, outperforming the minimum hop-count metric, as demonstrated in an 802.11b test-bed.
The paper develops a framework to quantify reactive routing overhead in unreliable stationary wireless networks (e.g., sensor networks) and demonstrates that infinitely scalable protocols are achievable through optimized traffic engineering.
The paper proposes **self-tuning power management (STPM)**, a dynamic approach that adapts to application needs and network conditions, significantly improving energy efficiency (e.g., 21% reduction) and performance (e.g., 80% lower latency) compared to default 802.11b power management.
The paper introduces **MFS (Model-based Frame Scheduling)**, a scheme that enhances IEEE 802.11 WLAN capacity by estimating network status to compute optimal transmission delays, reducing collisions and improving throughput by up to 20% (with RTS/CTS) and 150% (without RTS/CTS) in large networks.
The paper demonstrates that speed decay is inevitable in common mobility models, provides a framework to analyze it, and proposes a transformation to create stationary models for reliable simulations.
The paper proposes more realistic mobility models for mobile ad hoc networks by incorporating obstacles and Voronoi-based pathways, demonstrating their significant impact on protocol performance.
DIRAC is a software-based wireless router system with a distributed architecture (Router Core and Router Agents) that enables adaptive, link-layer-aware network services like fast handover, channel-adaptive scheduling, and policing.
The paper introduces **Ad hoc-VCG**, a truthful and cost-efficient routing protocol for selfish agents in mobile ad hoc networks, based on a VCG-inspired mechanism that ensures truthful cost reporting and minimizes overpayments while outperforming alternative approaches.
"Trajectory-based forwarding (TBF) is a scalable ad hoc network routing method that directs packets along predefined curves using node positions, enabling diverse routing services while decoupling path naming from actual paths."
The paper introduces *manycast*, a network-layer group communication scheme bridging anycast and multicast, enabling efficient discovery of multiple distributed service nodes in ad hoc networks, and demonstrates its advantages over application-layer implementations through analysis and simulations.
The paper proposes topology control strategies for two-tiered wireless sensor networks to maximize network lifetime by optimizing base-station placement and inter-node relaying based on computational geometry and mission-critical requirements.
This paper presents approximation algorithms for minimizing power consumption in wireless multi-hop networks while ensuring k-vertex (or k-edge) connectivity, achieving O(k) approximation for general graphs and practical distributed solutions for geometric graphs, outperforming existing heuristics.
The paper presents distributed algorithms for a sensor network to guide a target safely to a goal while avoiding dynamically modeled danger areas using artificial potential fields, with theoretical analysis and hardware validation on Mote sensors.
The paper proposes a game-theoretic admission and rate control framework (ARC) for CDMA data networks that optimizes provider revenue and user QoS by modeling competition between providers and customers, ensuring Nash equilibrium and differentiated services.
The paper analyzes wireless downlink data channels, deriving analytical relationships between user performance (blocking probability and throughput), cell size, and traffic density to explore trade-offs in coverage, capacity, scheduling, and admission control.
The paper proposes UCAN, a hybrid 3G and ad-hoc network architecture that improves cell throughput and fairness by routing data through proxy clients with better channel conditions via IEEE 802.11 peer-to-peer links.
The paper demonstrates that software-based IP routing lookups with longest matching prefix can achieve gigabit speeds using compact forwarding tables that fit in processor cache, enabling millions of lookups per second on general-purpose CPUs without specialized hardware.
The paper evaluates IP Switching's performance across various network environments, finding that it enables high switching rates (over 75% of datagrams) while analyzing the impact of flow classifiers and timer settings on efficiency and VC space requirements.
The paper proposes a scalable IP address lookup algorithm using binary search on hash tables organized by prefix lengths, achieving worst-case efficiency of log₂(address bits) hash lookups (5 for IPv4, 7 for IPv6) and optimizations reducing average lookups to under 2 for IPv4.
The paper analyzes Floor Acquisition Multiple Access (FAMA) protocols in networks with hidden terminals, demonstrating that carrier-sensing FAMA ensures collision-free transmissions and outperforms ALOHA and CSMA in such environments.
The paper introduces *trace modulation*, a technique that accurately and reproducibly emulates real wireless network conditions for mobile computing systems while remaining transparent to applications.
The paper proposes a wireless fair scheduling algorithm that adapts fluid fair queueing to handle location-dependent channel errors, providing theoretical bounds and demonstrating its effectiveness through simulations.
The paper presents a fault-tolerant real-time communication scheme using pre-established backup channels and resource sharing to ensure guaranteed timeliness with minimal overhead in multi-hop networks.
The paper introduces *Skyscraper Broadcasting* (SB), a multicast technique for video-on-demand that achieves low access latency like *Pyramid Broadcasting* (PB) while using only 20% of the buffer space required by *Permutation-Based Pyramid Broadcasting* (PPB).
The paper presents the design, implementation, and performance of an **Active Bridge**, a programmable network element that supports dynamic reconfiguration via loadable modules (switchlets), enabling protocol upgrades and error recovery while maintaining safety and security through static type checking in Caml, achieving 16 Mbps throughput and 1800 frames per second in a prototype setup.
The paper analyzes BGP routing instability at major U.S. Internet exchange points, revealing excessive redundant updates, unexpected anomalies, and systemic pathologies that impact network performance and reliability.
The paper proposes Fair Random Early Drop (FRED), an improved version of RED that ensures fair bandwidth allocation across non-adaptive, fragile, and robust traffic types by using per-active-flow accounting to adjust loss rates based on buffer usage, while maintaining efficiency with minimal state overhead.
The paper summarizes findings from a large-scale study of Internet packet dynamics, revealing asymmetries in network paths, prevalence of out-of-order delivery and corruption, a robust bandwidth estimation method, non-independent packet loss patterns with infinite variance, and congestion periods varying widely in duration.
The paper investigates the performance benefits of HTTP/1.1 features—persistent connections, pipelining, and compression—demonstrating significant improvements over HTTP/1.0 in packet reduction and elapsed time, while also highlighting the impact of web content optimizations like CSS and PNG.
The paper introduces *tcpanaly*, a tool for analyzing TCP implementation behaviors from packet traces, highlights challenges in achieving generality, reveals significant performance-impacting differences among eight TCP implementations, and advocates for standardized testing and reference implementations.
The paper proposes delta encoding and data compression extensions to HTTP to reduce response size and delay by transmitting only differences between cached and current resources, demonstrating significant benefits for certain content types.
The paper applies IP Multicast, Lightweight Sessions, and Application Level Framing principles to design a multicast-based shared editor, exploring loose consistency for improved performance amid network failures and losses.
The paper introduces Consistent Overhead Byte Stuffing (COBS), a byte-stuffing algorithm that tightly bounds worst-case overhead to at most one byte per 254 bytes while maintaining computational efficiency and competitive average overhead.
The paper proposes a flow-based security protocol for datagrams that achieves zero-message keying and soft-state efficiency while preserving connectionless semantics, with an implementation and performance evaluation in the 4.4BSD kernel.
The paper analyzes the impact of estimation errors, flow dynamics, and memory size on Measurement-based Admission Control (MBAC) performance, showing that certainty equivalence assumptions degrade QoS and providing design guidelines for robust MBAC schemes.
The paper introduces the **Fair Service Curve link-sharing model** and the **Hierarchical Fair Service Curve (H-FSC) scheduling algorithm**, which balance real-time service guarantees and hierarchical link-sharing fairness while exposing inherent tradeoffs between these goals, and validates their performance through implementation and experiments.
The paper proves and characterizes effective bandwidths for non-Markovian fluid source models with multiple rates and arbitrary duration distributions, showing they generalize Markovian models, reduce state-space complexity, and may have bandwidth requirements below peak rates under specific conditions.
The paper proposes *Iolus*, a scalable framework for securing multicast communications by addressing its fundamental differences from unicast security and overcoming scalability challenges.
The paper explores combining FEC and ARQ for scalable reliable multicast, showing improved efficiency and scalability when FEC is layered below ARQ or integrated with it, especially under varied loss conditions.
The paper introduces *virtual receivers* to optimize multicast traffic scheduling in WDM networks, balancing bandwidth consumption and channel utilization, proves the problem's NP-completeness, and proposes four heuristic solutions.
The paper compares reservation-capable and best-effort Internet architectures, finding that reservation-capable networks may retain significant advantages in certain scenarios despite bandwidth costs, with diverging performance gaps and bounded maximum benefits.
The paper analyzes the costs of QoS routing—protocol overhead and computational complexity—and proposes solutions to achieve efficient routing performance with reduced update traffic and processing costs.
The paper proposes a scalable buffer management scheme for FIFO queues to provide rate guarantees and fair excess bandwidth allocation while maintaining low complexity, integrating it with WFQ for QoS provisioning.
The paper proposes a cascade-based model to explain the multifractal scaling behavior of WAN traffic, reconciling it with self-similarity and attributing local irregularities to network mechanisms like TCP flow control, supported by empirical validation using wavelet analysis.
The paper introduces *digital fountain*, a scalable multicast protocol using efficient erasure codes to reliably distribute bulk data to heterogeneous clients without feedback channels.
The paper presents a scalable group key management solution using key graphs, with three rekeying strategies and protocols for secure joins/leaves, demonstrating logarithmic scalability in processing time relative to group size.
The paper proposes a window-based Random Listening Algorithm (RLA) for multicast congestion control, proves its essential fairness to TCP and other multicast sessions under specific conditions, and validates its performance through theoretical analysis and simulations.
The paper proposes an inter-domain multicast routing architecture using the MASC protocol for hierarchical address allocation and the BGMP protocol for constructing shared trees, enabling scalable global multicast in the Internet.
The paper examines the scaling limitations of using multicast session directories for address allocation in the Mbone and proposes a new approach to address these challenges.
The paper proposes **Core-Stateless Fair Queueing (CSFQ)**, a simplified architecture where edge routers label packets with flow rate estimates and core routers use FIFO scheduling with probabilistic dropping based on labels, achieving approximate fairness without per-flow state in the core.
The paper compares uniform and priority dropping for layered video transmission, finding that priority dropping offers smaller performance benefits than expected while uniform dropping has worse incentive properties than previously believed.
QoSMIC is a flexible, scalable, and resource-efficient QoS-sensitive multicast protocol that dynamically adapts to network conditions, minimizes pre-configuration reliance, and seamlessly transitions between shared and source-based trees while ensuring loop-free operation.
The paper proposes predictive and adaptive bandwidth reservation and admission control schemes for cellular networks to minimize hand-off drops while maintaining efficient bandwidth usage under varying traffic and mobility conditions.
The paper proposes and evaluates query control mechanisms (QD1/QD2, LT, ET, SBC) for the Zone Routing Protocol (ZRP) in ad-hoc networks, demonstrating their effectiveness in reducing control traffic and improving response time compared to proactive and reactive routing schemes.
The paper proposes "active services" as a more practical alternative to active networks by enabling user-defined computation at the application layer without altering Internet routing or forwarding, demonstrated through the implementation and deployment of the Media Gateway (MeGa) service.
The paper presents two high-speed algorithms—grid-of-tries and cross-producting—for least-cost matching filter in Layer Four switching, combining their advantages to optimize forwarding performance with minimal memory accesses.
The paper presents efficient packet classification algorithms that can handle thousands of filtering rules across multiple header fields at high speeds, ensuring worst-case performance for differentiated services in network routers.
Windmill is a passive network measurement tool that reconstructs application-level protocols and exposes underlying events through dynamically compiled filters, abstract modules, and an extensible engine, demonstrated in experiments analyzing routing instability and network utilization.
The paper presents a modular, high-performance router architecture in NetBSD that supports dynamic plugin integration for individual flows, achieving only an 8% overhead increase compared to monolithic kernels.
The paper proposes an end-to-end approach to enhance Web performance by using server-generated, proxy-customized piggybacked information to reduce latency, improve caching, and enable prefetching, without requiring changes to HTTP 1.1.
The paper proposes "Summary Cache," a low-overhead protocol for web proxy cache sharing that reduces inter-cache messages, bandwidth, and CPU usage while maintaining high hit ratios compared to existing methods.
The paper analyzes factors influencing the runtime complexity of the SRMM/p model for SONET mesh networks and proposes a state aggregation method using 2-phase hypoexponential distributions to improve computational efficiency while maintaining accuracy.
The paper proposes SHARQFEC, a scalable reliable multicast protocol that localizes traffic using administrative scoping and forward error correction to reduce session and repair traffic, outperforming existing methods like SRM.
The paper presents a retransmission-based error control technique for interactive video applications that prevents error propagation by correcting reference frame errors and masking repair delays through rearranged temporal dependencies, combined with layered coding to maintain quality under heavy packet loss.
The paper presents an analytic model for predicting steady-state TCP throughput that incorporates both fast retransmit and timeout mechanisms, demonstrating greater accuracy across a wider range of loss rates compared to prior models.
The paper presents an auto-tuning TCP implementation that dynamically adjusts socket buffers to maximize throughput across connections with vastly differing bandwidths, significantly improving performance and memory efficiency without manual configuration.
SEATTLE is a scalable, plug-and-play network architecture combining Ethernet's simplicity with IP's scalability through flat addressing, shortest-path routing, and efficient host resolution, reducing control overhead and state requirements significantly compared to traditional Ethernet bridging.
The paper introduces Approximate Link-state (XL), a routing algorithm that reduces overhead by selectively suppressing updates while maintaining soundness, completeness, and near-optimal path lengths, and demonstrates its superior efficiency over traditional link-state and distance-vector protocols.
Path splicing enables flexible, reliable routing by combining multiple routing trees ("slices") over a single topology, allowing traffic to switch paths dynamically with minimal header overhead while maintaining low latency and high reliability in both intra- and interdomain scenarios.
This paper demonstrates that Route Redistribution (RR) is widely used, complex, and critical for achieving network design objectives, highlighting the need for improved safety and flexibility in routing protocol interactions.
The paper proposes **PLayer**, a policy-aware switching layer for data centers that efficiently routes traffic through required middleboxes without overloading path selection, ensuring flexibility, resource efficiency, and correct traversal under network changes.
The paper proposes a cost-effective, high-bandwidth data center network architecture using commodity Ethernet switches to support full aggregate bandwidth without modifying existing hardware or software.
The paper presents **DCell**, a scalable, fault-tolerant, and high-capacity recursive network structure for data centers that outperforms traditional tree-based designs.
The paper presents **eXpose**, a novel statistical rule mining technique that identifies significant communication patterns in network traffic by analyzing packet timing and flow relationships, enabling improved network monitoring, diagnosis, and intrusion detection.
WISE is a tool that predicts CDN service response-time distributions for hypothetical deployment scenarios by learning causal relationships from existing traces, generating representative datasets, and providing an intuitive specification language, demonstrating accuracy in both controlled and real-world CDN deployments.
The paper introduces *shadow configurations*, a novel approach enabling pre-deployment evaluation of network configurations to reduce errors and disruptions with minimal overhead.
The paper advocates for a new enterprise network architecture where hosts actively participate in network decisions through *network exception handlers*—conditions and actions based on exposed network state—to improve resource efficiency and performance.
The paper demonstrates that dynamically adjusting channel width in wireless systems improves throughput, range, and power efficiency, and introduces SampleWidth, an algorithm that effectively identifies the optimal channel width for varying conditions.
SWIFT is a wideband system that coexists with narrowband devices by dynamically aggregating non-contiguous unused frequency bands, achieving higher throughput and range while maintaining narrowband-friendly operation.
ZigZag is an 802.11 receiver design that decodes colliding packets from hidden terminals by exploiting interference-free segments in successive retransmissions, achieving near-zero packet loss without MAC modifications or overhead.
The paper presents **AutoRE**, a spam signature generation framework that detects botnet-based spam emails and botnet membership by analyzing spam payload and server traffic, identifying **7,721 campaigns** and **340,050 botnet IPs** with low false positives.
The paper presents a Time Machine (TM) system that efficiently archives and retrieves network traffic by leveraging flow characteristics, integrates with intrusion detection for forensic analysis, and demonstrates effectiveness in high-volume environments.
The paper presents StopIt, a filter-based DoS defense system with a closed-control, open-service architecture that effectively blocks attack traffic from millions of bots within minutes while outperforming existing filter-based and capability-based systems in most scenarios, though capability-based systems are better for link-congestion attacks.
The paper proposes an augmented DFA model with auxiliary variables and optimization techniques to mitigate state-space explosion, significantly reducing memory, runtime, and per-flow state while maintaining high-speed performance for deep packet inspection.
The paper proposes universal deployment of packet-level redundant content elimination on Internet routers, along with redundancy-aware routing algorithms, to significantly reduce link loads and improve traffic engineering, demonstrating potential intra- and inter-domain load reductions of 10-50%.
The paper proposes VROOM, a network-management primitive that simplifies tasks like maintenance and energy reduction by enabling virtual routers to migrate between physical nodes without altering the logical topology, and demonstrates its effectiveness with hardware and software data planes.
The paper proposes an auction-based model to analyze BitTorrent's incentive mechanisms, demonstrating that proportional-share bandwidth allocation improves fairness and download speeds while revealing strategic behaviors like under-reporting file pieces.
The paper proposes **Rate-Delay (RD) services**, a performance-based network differentiation approach where users choose between higher transmission rates or lower queuing delay, supported by dual-queue routers with dynamic buffer sizing for incremental Internet deployment.
The paper demonstrates that ensuring truthful BGP path announcements by autonomous systems (ASes) requires not only protocols like S-BGP but also unrealistic policy restrictions, highlighting the high cost of preventing deceptive routing behavior motivated by traffic and revenue incentives.
The paper introduces a novel Google-based profiling tool that accurately classifies and analyzes Internet endpoint behavior using publicly available web data, outperforming traditional methods in various scenarios and revealing regional usage trends globally.
The paper presents a passive measurement-based methodology for inferring Internet topology by clustering traffic sources with shared paths, accurately recovering missing information, and minimizing active probes, validated through simulations and empirical data.
DisCarte improves Internet topology discovery by aligning and cross-validating record route (RR) and traceroute data using disjunctive logic programming, resulting in more accurate and complete router-level topologies.
SatelliteLab is a two-tier planetary-scale testbed that improves network heterogeneity by using well-provisioned core nodes (planets) for code execution and lightweight edge nodes (satellites) for traffic forwarding, enabling realistic evaluation of distributed systems under diverse Internet conditions.
The paper presents **iSPY**, a real-time, accurate, lightweight, and deployable IP prefix hijacking detection system that uses prefix-owner-based active probing to distinguish hijacks from network failures with low false positives (0.17%) and negatives (0.45%).
The paper introduces AIP (Accountable Internet Protocol), a network architecture that ensures accountability through self-certifying address hierarchies derived from public keys, addressing issues like spoofing, DoS, and route hijacking while supporting scalability, key management, and traffic engineering.
The paper proposes **P4P**, a cooperative architecture between P2P applications and network providers, which improves or maintains application performance while significantly reducing network costs compared to traditional P2P approaches.
This paper presents a lightweight, network-view-based approach to reduce costly cross-ISP P2P traffic while improving performance, demonstrating significant reductions in inter-AS traffic and latency, along with substantial download-rate improvements in both bandwidth-constrained and high-bandwidth environments.
The paper discusses the challenges and architectural design of P2P-VoD systems, presents insights from PPLive's real-world deployment, and analyzes user behavior and performance metrics to guide future P2P-VoD development.
Donnybrook enables large-scale, fast-paced multiplayer games (up to 900 players) without dedicated servers by reducing bandwidth via attention-based state updates and a tailored multicast system.
MIXIT enhances wireless mesh network throughput by forwarding likely correct bits from corrupted packets using physical layer hints and a novel network code, achieving significant throughput gains over existing schemes.
The paper presents a novel model-based approach to optimize fairness and throughput in IEEE 802.11 multi-hop wireless networks by accurately predicting flow performance and mitigating interference and MAC-induced dependencies, achieving significant improvements in both metrics.
The paper introduces **ViFi**, a protocol that improves vehicular WiFi connectivity by enabling simultaneous communication with multiple basestations, reducing disruptions and enhancing performance for applications like VoIP and web browsing compared to traditional handoff methods.
SoftRate is a wireless bit rate adaptation protocol that leverages physical-layer confidence information to estimate channel bit error rates, enabling rapid and accurate rate selection for 2X higher throughput than existing protocols.
The paper argues that exploiting simultaneous transmissions in wireless networks, rather than avoiding them, can significantly improve the efficiency and reliability of multicast and group communication primitives like anycast and leader election, as demonstrated through hardware prototypes and spectrum sensing techniques.
The paper presents **Net7**, the first Wi-Fi-like system for UHF white spaces, addressing spatial and temporal spectrum variations and fragmentation through adaptive spectrum assignment, a low-overhead protocol, and **SIFT**, a time-domain signal detection technique.
The paper proposes **PortLand**, a scalable, fault-tolerant layer 2 protocol for data center networks, addressing limitations of traditional Ethernet/IP in large-scale virtualized environments by leveraging known topology structures for efficient management and VM migration.
VL2 is a scalable, cost-effective data center network architecture that enables dynamic server allocation, uniform high capacity, performance isolation, and Ethernet simplicity using flat addressing, Valiant Load Balancing, and end-system address resolution, achieving near-optimal performance in prototype testing.
BCube is a server-centric, fault-tolerant network architecture for modular data centers that enhances bandwidth-intensive applications by leveraging multi-port servers and COTS mini-switches for efficient traffic handling and graceful degradation under failures.
HostTracker is a system that reliably tracks host-IP bindings using application-level data to attribute malicious activities to specific hosts despite dynamic IPs, proxies, and NATs, enabling host analysis, forensics, and dynamic blacklisting.
The paper *"Application-independent Redundancy Elimination (RE)"* introduces **SmartRE**, a practical and efficient architecture for network-wide redundancy elimination that improves resource utilization and enhances performance without requiring routing policy modifications.
ROMA is a distributed channel assignment and routing protocol for dual-radio mesh networks that minimizes intra- and inter-path interference to improve multi-hop throughput to gateway nodes.
The paper introduces **pathlet routing**, a flexible protocol where networks advertise path fragments (pathlets) that sources combine into end-to-end routes, enabling scalable, policy-rich routing while supporting diverse routing styles like BGP and source routing.
The paper demonstrates that distributed systems can achieve significant economic savings by leveraging geographic and temporal variations in electricity prices for data center operations.
"Persona is a privacy-focused online social network that uses attribute-based encryption to let users control access to their data, offering fine-grained policy enforcement and acceptable performance while replicating existing OSN functionality."
The paper demonstrates through theoretical modeling and experimental validation that carrier sense in wireless MAC protocols performs near-optimally with adaptive bitrate radios, with hidden and exposed terminals causing only modest throughput reductions, and a fixed sense threshold often suffices due to the noise floor's stabilizing effect.
The paper introduces **Interference Alignment and Cancellation (IAC)**, a novel MIMO signal processing technique that combines interference alignment and cancellation to nearly double LAN throughput, as demonstrated through theoretical analysis and GNU-Radio experiments.
DIRC is a wireless network design using phased array antennas and an optimization algorithm to enhance spatial reuse and maximize capacity in dense indoor environments, demonstrated to improve throughput while adapting to mobility and traffic changes.
The paper proposes a simple modification to the BGP decision process that prevents iBGP oscillation while maintaining routing flexibility and stability.
The paper proposes a novel, energy-efficient multicast forwarding fabric for scalable topic-based publish/subscribe systems, demonstrating its potential through simulations and implementations while advocating for a shift toward information-centric architectures.
The paper proposes PLUG, a flexible yet efficient pipelined lookup module for routers and switches, enabling rapid deployment of new network protocols without sacrificing performance compared to custom hardware solutions.
The paper presents a formal model and tool for analyzing Class of Service (CoS) policies in VPNs, enabling troubleshooting, auditing, and visualization by extracting and interpreting complex router configurations from 150 enterprise networks.
The paper presents *Giza*, a scalable diagnosis tool for detecting and troubleshooting performance issues in large hierarchical IPTV networks using multi-resolution data analysis and statistical data mining techniques.
The paper presents *NetMedic*, a system that enables detailed fault diagnosis in small enterprise networks by analyzing fine-grained component interactions and correctly identifies faulty components in 80% of cases.
The paper proposes a Lossy Difference Aggregator (LDA), a hash-based router primitive for high-precision latency and loss measurement at microsecond scales, offering superior accuracy and efficiency over existing methods like SNMP or active probing.
The paper introduces a spatio-temporal compressive sensing framework combining Sparsity Regularized Matrix Factorization (SRMF) and local interpolation to accurately reconstruct missing values in traffic matrices, achieving high performance in applications like network tomography and anomaly detection.
The paper introduces the Measurement Manager Protocol (MGRP), an in-kernel service that optimizes active network measurements by piggybacking application data on probe padding, reducing overhead while maintaining accuracy and improving performance, as demonstrated with MediaNet.
The paper introduces ROAR, a distributed algorithm that dynamically adjusts partitioning and replication levels in search engines to optimize performance, handle failures, and balance load efficiently.
The paper proposes using microsecond-granularity TCP timeouts to mitigate the TCP incast problem in datacenter networks, preventing throughput collapse and reducing latency for synchronized workloads.
Htrae is a low-latency prediction system for game matchmaking that combines geolocation with network coordinates to achieve faster convergence and lower prediction errors than existing methods.
The paper introduces error estimating codes (EEC), a novel coding scheme that allows receivers to estimate a packet's bit error rate without correcting errors, demonstrating its effectiveness in Wi-Fi rate adaptation and video streaming with low overhead.
The paper introduces *approximate communication*, a PHY-layer technique that leverages the structured nature of wireless symbol errors to provide unequal error protection without extra resource overhead, and demonstrates its effectiveness in media delivery through *Apex*, an SDR-based system that improves video quality by 5–20 dB PSNR under varying wireless conditions.
The paper proposes **Reference Latency Interpolation (RLI)**, a scalable per-flow latency measurement architecture that exploits delay similarity among closely spaced packets to achieve high accuracy (12% median error) with low overhead, addressing the limitations of prior aggregate-based solutions like LDA.
DAC is an automatic data center address configuration system that maps device-to-logical IDs using graph isomorphism and detects malfunctions, efficiently scaling to millions of devices.
The paper explores the benefits of allowing applications to implement custom routing protocols in CamCube's 3D torus network, demonstrating improved efficiency without significant overhead and proposing an extended routing service to support this flexibility.
The paper proposes **DCTCP**, a TCP variant for data centers that uses **ECN-based multi-bit feedback** to reduce buffer usage by **90%**, improve latency for short flows, and handle high throughput without harming foreground traffic or causing incast problems.
The paper analyzes over 200 Exabytes of inter-domain Internet traffic from 110 diverse networks over two years, revealing a shift toward direct traffic between content providers, CDNs, and consumer networks, a decline in P2P usage, a rise in video traffic, and estimates of global inter-domain traffic volume and growth.
The paper demonstrates that even advanced BGP security protocols like S-BGP and data-plane verification are insufficient to prevent traffic-attraction attacks, as attackers can exploit counterintuitive strategies (e.g., announcing longer paths or restrictive export policies) to maximize harm, highlighting the need for mechanisms that police export policies.
The paper analyzes Internet address usage through ICMP probing and clustering, revealing that adjacent addresses often form consistent blocks, and explores underutilization, dynamic allocation trends, and last-hop bitrate issues in network management.
OneSwarm is a P2P data-sharing protocol that offers better privacy than BitTorrent and better performance than Tor or Freenet by allowing configurable trust levels and novel lookup/transfer techniques.
The paper explores the feasibility of applying differential privacy to network trace analysis, demonstrating that while some approximations are needed to maintain accuracy, it shows promise for diverse analyses despite inherent randomization and declarative language constraints.
The paper argues that recent advancements in cryptographic algorithms enable general-purpose processors to encrypt internet traffic at line rates, making widespread end-to-end encryption feasible, and presents a high-performance TLS 1.2 implementation to support this claim.
The paper introduces FICA, a fine-grained channel access method that divides the wireless channel into subchannels to improve efficiency in high data rate WLANs by enabling simultaneous contention and usage through OFDM-based PHY architecture and frequency-domain contention.
The paper demonstrates that wireless packet delivery can be accurately predicted for commodity 802.11 NICs using Channel State Information (CSI) and an effective SNR-based OFDM receiver model, enabling precise rate, power, and antenna selection without relying on unreliable RSSI.
SourceSync is a distributed architecture that enables synchronized, concurrent transmissions from multiple senders in wireless networks, improving throughput and reducing errors in opportunistic routing and 802.11 last-hop diversity protocols.
SwitchBlade is a programmable hardware platform enabling rapid deployment of custom network protocols with modular, reconfigurable pipeline design, software exception handling, and parallel protocol operation at wire speed.
PacketShader is a GPU-accelerated software router framework that significantly outperforms CPU-only solutions, achieving 39 Gbps throughput for 64B IPv4 packets by leveraging GPU parallelism for high-performance packet processing.
EffiCuts reduces memory overhead in packet classification by using separable trees, selective merging, equi-dense cuts, and node co-location, achieving significantly lower memory usage and power consumption compared to HiCuts, HyperCuts, and TCAM.
The paper proposes a new theory for safely and expressively connecting multiple routing protocol instances without modifying existing protocols, addressing current vulnerabilities and operational limitations.
DONAR is a distributed system that efficiently directs client requests to optimal service replicas by balancing performance, load, and cost through a stable, decentralized coordination algorithm.
The paper presents a model for optimizing hybrid cloud migration of enterprise services by considering performance, cost, and security constraints, demonstrating its effectiveness through real-world applications and Azure deployments.
NetFence is a scalable DoS-resistant network architecture that uses secure congestion policing feedback to ensure fair resource allocation and suppress unwanted traffic without requiring per-host state at congested routers.
The paper proposes a computationally simple method to detect correlated anomalous flows by exploiting the equilibrium property of multiplexed flows on non-saturated links, identifying a distinct class of anomalies compared to existing techniques.
NetShield is a high-speed, accurate Network Intrusion Detection/Prevention System that efficiently matches vulnerability signatures using a candidate selection algorithm and lightweight parsing state machine, achieving multi-gigabit throughput.
The paper proposes **Resilient Routing Reconfiguration (R3)**, a congestion-free, efficient, and flexible routing protection scheme that outperforms existing methods by at least 50% under various failures while ensuring robustness to topology and traffic changes.
The paper introduces **MERCURY**, a scalable infrastructure that detects and analyzes the impact of network upgrades on performance using statistical rule mining and configuration data, validated in a tier-1 ISP network.
The paper proposes a method to analyze network component failures by mining existing low-quality data sources like router configurations, syslogs, and email logs, and applies it to study over five years of failures in a large regional network.
The paper proposes HyPaC, a hybrid packet and circuit-switched data center network architecture that combines traditional packet switches with an optical circuit-switched network to enhance bandwidth efficiency while maintaining simplicity, and demonstrates its benefits through a prototype called c-Through.
Helios is a hybrid electrical/optical switch architecture that reduces switching elements, cabling, cost, and power consumption while maintaining scalable bandwidth in large data centers.
DIFANE is a scalable, efficient solution that maintains traffic in the data plane by distributing rules across switches, reducing reliance on centralized controllers while supporting fine-grained policies with commodity hardware.
The paper demonstrates that existing Sybil defense schemes rely on detecting local communities around trusted nodes, revealing their fundamental limitations and suggesting opportunities to leverage general community detection algorithms for improved defenses.
The paper introduces **SPAR**, a middleware that optimizes scalability in Online Social Networks by partitioning and selectively replicating data based on social graph structure, ensuring data locality for direct neighbors while minimizing overhead and enabling high throughput without modifying application logic.
The paper presents **Crowdsourcing Event Monitoring (CEM)**, a framework that leverages end-user applications to detect and assess the impact of silent network outages, demonstrated effectively using BitTorrent data and ISP-confirmed events.
Bassoon is a backpressure-based SIP overload control mechanism that combines optimal load balancing and end-to-end traffic throttling to prevent congestion collapse, improving goodput, fairness, and responsiveness.
The paper proposes an unbiased sampling method for directed social graphs (USDSG) based on the Metropolis-Hasting Random Walk algorithm, demonstrating its effectiveness with less than 10% error compared to uniform sampling.
The paper presents *Contrabass*, a practical PHY and MAC protocol for MIMO concurrent transmissions that eliminates coordination overhead, enabling high throughput, low latency, and scalability without prior control message exchanges.
The paper proposes and implements a dynamic backpressure routing approach for Delay Tolerant Networks (DTNs), where per-packet routing decisions are made using queue backlogs and scheduling, demonstrating advantages over traditional methods.
The paper proposes a fair client-AP association scheme for WiFi networks that leverages hybrid WLAN architecture, demonstrating superior performance over RSSI-based methods while maintaining practicality and scalability.
The paper proposes a stochastic approximation-based algorithm that maximizes throughput in WLANs, outperforming standard IEEE 802.11 and other protocols in both connected networks and scenarios with hidden terminals.
The paper proposes a structured routing overlay for large Autonomous Systems (ASes) that separates control and forwarding planes, using a distributed entity to pre-compute BGP routes and improve scalability and stability compared to traditional iBGP approaches.
The paper presents a Passive IP Traceback (PIT) mechanism that reconstructs attack paths using ICMP messages from network telescopes without requiring ISP cooperation, demonstrating effectiveness in tracing spoofed attacks in 55.4% of cases for at least one router and 23.4% for 10+ routers.
"SecureAngle enhances wireless network security by using angle-of-arrival signatures from multiantenna APs to block external attackers and prevent address spoofing."
SculpTE is a self-configuring traffic engineering scheme that automatically adapts network-layer topology to dynamic traffic demands, ensuring responsiveness, stability, and optimal load balancing without manual configuration or prior traffic knowledge.
The paper proposes a unified session layer integrating security features like access control, encryption, and authentication into the network core to address current Internet security shortcomings.
The paper proposes **Accelerometer-Assisted Rate Adaptation (AARA)**, which optimizes Wi-Fi throughput in underground MRT systems by dynamically adjusting transmission rates across four distinct train motion phases, outperforming conventional methods.
The paper evaluates existing WiFi rate adaptation algorithms in vehicular environments and proposes a simple, low-overhead solution tailored for mobile clients with predictable link quality.
The paper proposes an energy-efficient opportunistic channel access scheme for cognitive radio secondary nodes that estimates transmission duration using residual idle time distribution, reducing sensing overhead and conserving energy while limiting primary user interference.
The paper proposes a GPU-based IP lookup architecture achieving O(1) time complexity and demonstrating a 6x throughput improvement in real-world experiments.
The paper demonstrates that curb-side acoustic sensing can effectively estimate road traffic conditions with 70-90% accuracy in classifying congested versus free-flowing traffic using threshold-based methods.
The paper "Dense 802.11 wireless networks present a pressing capacity challenge" introduces **Cone of Silence (CoS)**, a technique that uses software-steerable directional antennas to enhance 802.11 network capacity by adaptively nulling interference while maintaining signal strength, improving throughput in dense deployments.
The paper proposes using GPUs to offload SSL/TLS cryptographic operations, significantly boosting server throughput to 25.8K transactions per second while maintaining low latency.
The paper proposes an open router virtualization framework using OpenFlow-enabled hardware to enable flexible, high-performance virtual networks while addressing the limitations of commercial and software-based solutions.
The paper introduces *QuagFlow*, a system enabling seamless integration between the Quagga routing suite and OpenFlow to create an open, vendor-independent networking stack.
The paper demonstrates a Linux-based Multipath TCP (MPTCP) implementation that efficiently distributes a single TCP flow across multiple Internet paths without modifying applications, showcasing simultaneous multi-path usage and failover capabilities.
The paper presents a multi-hop packet tracking system designed for experimental networks like PlanetLab to monitor packet paths and QoS characteristics, detailing its implementation and prototype results.
The paper compares traditional H.264/AVC video streaming over 802.11 with SoftCast, a wireless video scheme that adapts quality dynamically based on receiver channel conditions.
The paper presents *PixNet*, a system that enables high-speed, interference-free wireless communication between LCDs and cameras by adapting OFDM to address perspective distortion and blur, achieving multi-Mbps transmission over multi-meter distances at wide viewing angles.
The paper discusses the challenges of distributing large Virtual Appliances (VAs) due to their significant download sizes and proposes solutions to mitigate delays and bandwidth bottlenecks.
The paper proposes "InfoNames" as presentation-invariant, content-based identifiers to decouple multimedia information from sources, formats, and device constraints, enabling flexible retrieval via an InfoName Resolution System (IRS).
Ripcord is a modular platform for prototyping and evaluating scale-out data center networks using commercial hardware and open-source software, demonstrated through three custom network functions—traffic isolation, energy-efficient dynamic management, and health monitoring—on a 160-node cluster with live interactive visualization.
The paper presents an in-network processing (INP) framework that efficiently orchestrates computing resources and network devices to deploy network services, exemplified by a web Ad-insertion application, as an alternative to traditional hardware middleboxes.
The paper presents an open-source system that uses frequent itemset mining to automatically identify and summarize anomalous network flows, integrated with a commercial anomaly detector and successfully deployed in the GÉANT network.
The paper introduces **Sora**, a programmable software radio platform on general-purpose PCs, demonstrating its capabilities through four applications: wireless channel capture, signal generation, real-time reception, and a software-based WiFi driver compatible with commercial cards.
The paper introduces *Collage*, a censorship-resistant system that hides messages within user-generated content on various websites, making it difficult for censors to block or monitor communications by leveraging the vast number of possible hosting sites and concealment methods.
The paper introduces *Cuckoo*, a decentralized, socio-aware microblogging system that leverages P2P techniques and social relations to enhance scalability, reliability, and performance compared to centralized architectures.
The paper introduces **Multi-Purpose Access Point (MPAP)**, a software radio-based virtualization architecture that consolidates multiple wireless standards into a single hardware platform, enabling shared spectrum use, resource efficiency, and improved coexistence through coordinated baseband processing.
"Stratus reduces smartphone radio energy consumption by up to 50% using cloud-based proxy optimizations like traffic aggregation, compression, and opportunistic scheduling."
The paper presents a simulation and experimentation framework for evaluating low-power IPv6 routing mechanisms in networked devices, enabling both detailed simulations and real-world testbed deployments.
The paper proves the decidability of protocol converter existence for certain classes of communicating finite automata, provides a construction method, and establishes an upper bound on the computational complexity of the algorithm.
The paper presents an analytic tool for evaluating fair and prioritized demand assignment protocols in LANs, accommodating heterogeneous workloads with station-dependent message sizes and arrival rates to better reflect real-world network conditions.
The paper proposes two distributed prioritized multiaccess protocols for twin-bus optical channels—one using token packets to enforce priorities and the other employing time-outs—to enable efficient, priority-based communication for time-critical applications while requiring only backlogged stations to remain active.
The paper presents a real-time, packetized voice communication system over a PC-based token-passing ring network, analyzing its capacity for concurrent two-way conversations without reconstitution delay.
The paper proposes a generalized multi-machine pipeline mechanism using DARPA protocols to enable transparent and consistent access to distributed services across heterogeneous networks, extending the UNIX pipe concept for complex computations.
The paper describes a five-layer OSI protocol simulation model predicting that OSI protocols on a LAN can achieve 1.5 Mbps throughput with 6–10 ms one-way delays and 15–25 ms response times, recommending CSMA/CD for under 40% load and token passing for 40–70% load in time-critical factory applications.
The paper presents a verified sliding window protocol with modulo-N sequence numbers for reliable, flow-controlled data transfer over unreliable channels, extending TCP with dynamic window sizing and selective acknowledgments while determining the minimum N for correctness.
The paper presents a hybrid first-order logic model for specifying and verifying a transport layer protocol using state machines and time expressions, employing decomposition and abstraction to simplify verification.
The paper introduces a fully-distributed protocol for integrated voice/data traffic in a local-area broadcast network, using a movable boundary in framed TDMA/CSMA to dynamically allocate capacity between collision-free voice and CSMA/CD data traffic, supporting variable packet sizes and multi-party calls without system-wide clock synchronization.
The paper presents an object-oriented architecture for a real-time multimedia conferencing system, dividing it into five functional areas—shared workspace, user interface, conference management, communications, and information base—with a focus on workspace modeling and uniform handling of multimedia data.
The paper presents tier automation as a universal and manipulable model for representing communication protocols, demonstrates its application in modeling distributed architectures, and illustrates its use with a sample protocol and transmission session.
This paper presents a general Petri net reduction algorithm that preserves all properties while minimizing state explosion, introducing simple well-behaved modules (SWBMs) for automated recursive reduction and demonstrating its application in protocol synthesis.
The paper introduces LOTOS, an ISO-standardized executable specification language combining CCS and ACT ONE, and describes a prototype interpreter implemented in YACC/LEX, C, and Prolog for syntax analysis, translation, and evaluation.
The paper proposes a multichannel network architecture using combined Frequency-Time Division control to improve system capacity and reduce delays in high-speed networks by introducing allocation protocols that avoid channel sensing and idle channel location penalties.
The paper proposes a CSP-based methodology to verify protocol conformity by transforming protocol and service specifications into CCS behavior expressions and proving their equivalence, demonstrated using the Alternating Bit Protocol.
The paper presents a method for synthesizing efficient, error-recoverable two-party protocols for noisy channels by leveraging local modeling and elaboration to avoid state explosion while ensuring correctness and generality.
The paper explores formal and semi-formal methods for designing conformance tests for OSI protocols, focusing on the Class 4 transport protocol in Estelle, and discusses distributed test architecture with optimized lower tester implementation.
The paper analyzes the technical and access control challenges of Inter-Organization Networks (IONs), proposing non-discretionary control schemes and alternative protocols to enforce secure cross-boundary connectivity while minimizing interference with internal operations.
The paper describes the protocols used by Mach's Network Servers to securely extend the port-based communication abstraction across a network by protecting messages and access rights to network ports.
The paper analyzes a class of slotted ALOHA dynamic control strategies, proving their stability and lossless operation for large user populations with aggregate arrival rates below \( e^{-1} \) packets/slot through ergodicity conditions and Markov chain modeling.
The paper analyzes a contention-based TDMA protocol that outperforms existing methods in light-to-moderate traffic, offering simpler implementation than contention-free alternatives, with validated transient and steady-state performance metrics.
The paper proposes an efficient reliable datagram service mechanism using semi-virtual circuits and a simple error detection/recovery algorithm for transaction-oriented distributed systems, implemented via the UDSTP protocol.
The paper describes a diagnostic message protocol for fault diagnosis in distributed systems, using a test dependency model and adaptive strategies to reduce testing costs while determining system faults.
The paper analyzes the performance of a client-server distributed file system using a queueing network model, identifying bottlenecks and evaluating design improvements such as file caching, outboarding transport functions, and multiple network interfaces.
The paper proposes and analyzes a load-balancing algorithm for optimizing process and read site placement in the LOCUS distributed file system, using centralized synchronization to manage access conflicts and improve system performance.
The paper presents a resilient distributed protocol that allows synchronous algorithms to operate on asynchronous networks by maintaining synchronization despite dynamic network changes like link/node failures and recoveries.
The paper introduces two symmetric surveillance protocols that ensure mutual detection of unavailability between distributed sites within a bounded time and allow tunable timeout thresholds to improve reliability.
The paper presents a distributed algorithm for detecting deadlocks in store-and-forward networks, first developing an efficient knot detection method for static graphs with \(O(n^2 + m)\) messages and \(O(\log n)\) memory, then extending it to dynamic environments with targeted, non-disruptive execution.
The paper describes the integration of System V UNIX interprocess communication features—messages, semaphores, and shared memory—into the Locus distributed operating system.
The paper argues that timers in distributed systems are inherently suboptimal due to their reliance on incomplete information, and recommends prioritizing external event notifications over timeout-based actions for optimal performance.
The paper describes the design of the Versatile Message Transaction Protocol (VMTP), a transport-level protocol optimized for remote procedure calls, multicast, real-time communication, and efficient network file access, while challenging traditional protocol design principles.
The paper describes the Universe Network, a UK-based system connecting Cambridge ring LANs via a 1 Mbit/s satellite channel, which operates without an internet protocol, maintaining a host's communication view as if on a single ring.
The paper describes a distributed file distribution system on the Universe Network, utilizing cooperating agents to optimize satellite channel usage, ensure reliability, and support broadcast file delivery through specialized protocols and robust fault-tolerant interactions.
The paper describes the Universe network's packet TDMA protocol, which uses a master-controlled, demand-based scheduling system over a satellite channel to connect local area networks into a high-speed wide area network, and explores its broader applications.
The paper outlines key design principles for link communication protocols—modularization, symmetry, and state-exchange models—demonstrating their benefits over command-response approaches and evaluating their application in X.25 Level 2 while proposing a compliant protocol.
The paper presents a multi-phase model for protocols, defining each phase as a network of communicating finite state machines with correctness properties, and demonstrates how to connect phases to construct a multi-function protocol while maintaining these properties, illustrated with an example from IBM's Systems Network Architecture.
The paper describes the Universe network's broadcast protocol for satellite-linked local area networks, detailing its design, implementation, and experimental use in file distribution, while suggesting potential for advanced applications.
The paper presents a multidestination routing solution for internetworks that reduces traffic, enhances failure resilience, and maintains compatibility with existing IP protocols while using shortest-path routing without altering subnetwork routing.
The paper presents Petri net formal specifications for the ISO transport protocol's timeout mechanism, data transfer, and connection phases, validates them using an extended OGIVE-based tool, and discusses the results.
The paper summarizes an automated analysis of the NBS Class 4 transport protocol, identifying completeness errors and proposing solutions to address them.
The paper presents the CIL-approach, which uses the CIL programming language, a formal execution theory, and interactive verification tools to develop and verify communication services, demonstrated through a transport service example.
The paper presents an automated proof method extending classical resolution with temporal operators to verify liveness properties of network protocols, including FIFO and LIFO queues and the alternating bit protocol.
The paper explores the use of LOTOS, a formal description technique, to specify OSI session services, including the Basic Combined Subset and Expedited Data, while testing conciseness through notational variants.
The paper argues that CPU speed is the primary performance factor in distributed applications, while also examining network protocol improvements and high-level protocols to enhance efficiency.
The paper describes the construction of an analytic performance model to predict bottlenecks in protocol execution, focusing on a transport protocol's data transfer stage, as part of a computer-aided design process for communication systems.
The paper presents a flexible routing algorithm that allows arbitrary placement of network topology information across nodes, with protocols for updating topology data between capable nodes and enabling less capable nodes to report and acquire route information.
The paper proposes an automated PROLOG-based method for validating behavioral consistency across different abstraction levels of software functionality, particularly useful for layered protocol architectures, enhancing reliability and reducing errors in test harness construction.
The paper presents a protocol development strategy using finite state machines with predicates, integrating conceptual tools (PDIL, VADILOC) for specification and validation, and experimental tools (STQ, Cerbere, Genepi) for conformity testing, linked by the GAST test sequence generator.
The paper presents simulation studies evaluating high-speed (≥100 Mbit/s) baseband local area network channel access protocols and their effectiveness for mixed traffic data transmission.
The paper proposes a dynamic integrated systems architecture that efficiently allocates bandwidth between 64 kbit/s stream traffic and bursty high-speed traffic by adaptively adjusting time slots and high-speed channel capacity, achieving near-blocking-free performance and favorable comparison to CSMA/CD.
The paper proposes a minimal duplex connection capability for the top three OSI layers (application, presentation, session) to enable small systems like home computers to interconnect with larger systems by simplifying service primitives and leveraging existing session layer protocols.
The paper describes a request manager that ensures atomicity in distributed processes through communication primitives, recoverable action state tables, and a protocol verified using evaluation nets.
The paper presents a symbolic technique for deriving performance expressions in Timed Petri Nets without requiring specific time delays, only timing constraints, and demonstrates it on a communication protocol.
The paper demonstrates that structured name spaces simplify management in distributed name server systems and provides quantitative methods to optimize performance by strategically replicating database entries based on client access patterns.
The paper presents an approximate iterative-decomposition method to analyze queueing network models (QNMs) of computer communication networks with window flow control, enabling efficient computation of user response times while accounting for network delays and processing constraints.
The paper extends slotted ALOHA for spread spectrum packet radio by separating transmissions into short preambles on public channels and packet bodies on private channels, enabling logical connectivity while utilizing all available codes efficiently.
The paper proposes an adaptive timeout algorithm using exponentially weighted moving averages of acknowledgment delays to minimize retransmission delay while limiting unnecessary retransmissions.
The paper examines the Random Drop algorithm's congestion recovery performance in high-speed internet gateways, finding it generally ineffective and worse in single-bottleneck topologies, with local traffic impacted by distant gateway events.
The paper proposes a congestion management framework using stop-and-go queueing to provide loss-free, bounded-delay transmission for real-time traffic while efficiently handling bursty traffic through an admission policy and multi-frame sizing.
The paper *VirtualClock* proposes a new algorithm for controlling data traffic in high-speed networks, ensuring reserved throughput rates while maintaining statistical multiplexing flexibility, as validated through simulations.
The paper proposes adaptive algorithms for dynamically adjusting sliding window sizes in non-stationary high-speed networks by tracking the root of a quasi-stationary design equation based on round-trip response time measurements, with simulations confirming stable and efficient performance.
The paper presents a stateless, clock-based message passing protocol that guarantees duplicate detection and enables efficient at-most-once remote procedure calls (RPCs) with performance comparable to non-guaranteed RPCs, relying on loosely synchronized system clocks for performance but not correctness.
The paper discusses the evolution from static tables to dynamic directory services like X.500 and DNS for resource discovery in networks, comparing low-level protocols (e.g., RARP, ICMP) and high-level services, highlighting their respective limitations in query flexibility and cost-effectiveness.
The paper proposes a finite pushdown automaton-based model to analyze the memory and processing time requirements of OSI Application layer data structures, particularly for high-speed networks, and evaluates its effectiveness using document transmission examples.
The paper introduces the Dual Bus Protocol (DBP), an enhanced version of QPSX adapted for the Terrestrial Wideband Network (TWBNET), designed for scalable high-speed wide-area networking with reserved bandwidth, and evaluates its performance through simulations and real-world measurements.
The paper introduces efficient high-speed access protocols for various network topologies, including LANs and satellite networks, using a tree algorithm with deferred collision detection to achieve 100% channel utilization regardless of propagation delay or network size.
The paper proposes an asymptotic approximation for computing blocking probabilities in multi-rate circuit-switched networks under complete sharing policy, reducing the problem to evaluating a single Erlang formula and solving a polynomial equation.
The paper presents an optimization method that shortens protocol conformance test sequences by overlapping test subsequences derived from UIO sequences, yielding significantly shorter sequences than other UIO-based approaches.
The paper introduces the Gauss ATM Switching Element (ASE), a high-performance, nonblocking, modular switching fabric designed to meet ATM requirements with minimal delay and delay variance, using only two types of integrated circuits.
The paper introduces *Pulsar*, a non-blocking, high-speed switch for gigabit networking that uses a shift-register ring to eliminate Head-Of-Line blocking while supporting variable packet sizes and low latency.
The paper analyzes feedback flow control techniques in datagram networks, comparing aggregate and individual feedback with FIFO and Fair Share gateways, and finds that individual feedback with Fair Share gateways achieves time-scale invariant, fair, stable, and robust performance most effectively.
The paper introduces the SPF-EE routing algorithm, which enhances traditional SPF by providing alternate emergency paths to reduce congestion and improve performance in dynamic traffic conditions.
The paper surveys dynamic shortest-path routing algorithms, proposes a new approach using a "dynamic synchronizer" to achieve linear time and polynomial communication complexity, and suggests its broader applicability in protocol design.
The paper proposes a multi-processor architecture with parallel processing techniques to achieve Gbps throughput in transport protocol processing, addressing limitations of existing protocols like TP4 and TCP.
The paper proposes new architectural principles—Application Level Framing and Integrated Layer Processing—and highlights the importance of the presentation layer to address future network requirements beyond current protocol suites like TCP/IP.
The paper examines physical and logical multiplexing in communication systems, emphasizing fine-grained physical resource sharing to meet application demands and recommending logical multiplexing be limited to layer 3 for high-speed systems.
This paper proposes an algorithm that uses static topology information to detect and resolve loops and duplications in nested group name resolution for internet communications, with a worst-case complexity of \(O(|A|)\).
The paper proposes that a source-routing architecture using link-state algorithms with policy information in link-state advertisements best meets inter-Administrative Domain routing needs but presents scalability challenges.
The paper explores the trade-off between the amount of topology information shared among interconnected autonomous networks and routing efficiency, showing that partial knowledge can achieve reasonably efficient routing without full topology disclosure.
The paper argues that modern workstations, when properly programmed, can effectively monitor LAN behavior with high-performance graphics and flexibility, offering a viable alternative to dedicated monitoring systems.
The paper analyzes the performance of the Fiber-Distributed Data Interface (FDDI) LAN standard, concluding that a Target Token Rotation Time (TTRT) of 8 ms optimizes efficiency, response time, and access delay across various configurations and workloads.
The paper reexamines connection establishment in fast packet networks with integrated traffic, critiques existing solutions as inadequate, and proposes a new protocol suitable for such networks, using the PARIS network as a model but with broader applicability.
The paper presents a simplified Reed-Solomon erasure correction coder architecture for broadband networks, capable of 1 gigabit per second encoding and decoding in a custom 1-micron CMOS VLSI chip to complement ARQ protocols.
The paper presents an inclusive session-level protocol for local networks, designed to meet distributed application needs with services like group communication, synchronization, recovery, and distributed primitives, leveraging local network strengths for high performance.
The paper proposes a novel solution to TCP's retransmission ambiguity problem, where acknowledgments for retransmitted segments cannot distinguish between original and retransmitted transmissions, improving reliability and efficiency.
The paper presents an accurate analytical model of Orwell's high-speed slotted ring protocol with destination-release, demonstrating its performance under varying parameters and showing that carried load can exceed transmission rate.
The paper analyzes the IEEE 802.5 token ring priority mechanism, showing that it generally provides minimal delay discrimination between priority levels, often slightly increasing delays for all priorities, with significant benefits only under rare, extreme conditions.
The paper describes the development and current status of JUNET, a Japanese computer network that connects research organizations, focusing on hierarchical domain naming, Japanese character handling, and efficient communication protocols like UUCP and TCP/IP to provide services such as email and network news.
The N-1 protocol was developed to connect Japanese universities through networks like the Inter-University Computer Network and Science Information Network, utilizing high-speed digital lines and local IEEE802.3 networks, including an optical fiber backbone at the University of Tokyo.
The Sigma network is a name-oriented virtual network designed to enhance software productivity by providing a logically integrated development environment across companies, supporting multiple protocols under TCP/IP, and utilizing a hierarchical name space managed by a Name Server.
The paper specifies and verifies a symmetric connection management protocol for unreliable channels, ensuring safety and progress properties through a 3-way handshake, and demonstrates its compatibility with hierarchical verification when combined with data transfer protocols.
This paper surveys and evaluates existing strategies to mitigate state space explosion in reachability analysis for protocol verification and proposes a new heuristic-based strategy, PROVAT, demonstrating its effectiveness through preliminary results.
The paper introduces the Power Tier Automaton (PTA) as a superior model to Petri nets for representing systems like computer architectures and communication networks, demonstrating its advantages through homomorphisms and applications in protocol manipulation.
The paper compares open and closed queueing network models, introduces the Proportional Approximation Method (PAM) for efficient closed network analysis, and presents a heuristic routing algorithm that maximizes throughput, validated by experimental results on 100 random networks.
The paper introduces BIAS&trade;, a decentralized, deterministic, and adaptive routing system for Burroughs Network Architecture (BNA) that automatically updates routes in response to network topology changes while minimizing recovery time and avoiding loops.
The paper presents the architecture for OSI routing protocols, outlining their design, hierarchical structure, and inter-network relationships, while identifying remaining challenges for global OSI network implementation.
The paper describes the design, implementation, robustness, and congestion-control mechanisms of the NSFNET Backbone Network, which connects supercomputer sites, regional networks, and ARPANET while supporting DARPA Internet and DCN subnet protocols.
The paper presents algorithms for reducing the size of timed finite state graphs, including a generalized vertex folding method based on transition time conservation, and demonstrates their application to a stop-and-wait protocol, alongside a software package for automated performance prediction.
The paper extends Hoare's CSP by incorporating timing information and probabilistic event sequences to enable protocol performance specification and verification.
The paper introduces a novel yellow-pages service that maps service names to server addresses using attribute-based matching, enabling clients to specify desired server attributes and integrating with internet protocols for global access.
The paper presents decentralized resource management solutions using multicasting, demonstrating their advantages in scalability, fault tolerance, and efficiency over centralized approaches.
The paper describes EIES2, a decentralized, object-oriented computer-supported cooperative work system with asynchronous and synchronous communication capabilities, designed to facilitate group networking across geographical constraints using modern standards and an intuitive interface.
The paper describes a prototype X.400 messaging system for IBM's VM/SP operating system that integrates X.400 services natively within the RSCS communication environment.
The paper introduces the Integrated Media Architecture Laboratory (IMAL), a Bell Communications Research facility that emulates diverse network and CPE environments using off-the-shelf technologies to study and demonstrate integrated multimedia communications.
The paper proposes a partially informed distributed database model that distributes directory information on a "needs-to-know" basis to address fundamental challenges in managing large-scale distributed databases for telecommunications and public services.
The paper presents a **Threaded/Flow methodology** for managing the assembly, control, and disassembly of distributed small-scale configurations in large-scale reconfigurable systems, particularly for telecommunications and multi-tasking environments, using data-flow constructs and threaded-interpretive techniques to support autonomy, concurrency, and real-time performance.
The paper presents the design, implementation, and performance evaluation of a transparent distributed shared memory system for communication and data exchange in loosely coupled distributed computing environments.
The paper proposes a resource management scheme with sophisticated semantics transparency for handling duplicated resources in distributed environments and discusses a specification language to support this transparency.
The paper proposes an efficient negative-acknowledgment-based multicast data transfer protocol for broadband broadcast networks, optimizing resource usage while matching the functionality of positive-acknowledgment protocols, and compares its performance in lecture and conference applications.
The LAN-HUB is a flexible, cost-effective local area network solution that combines collision-free arbitration for star configurations with Ethernet-like bus performance, improving reliability, capacity, and user configurability.
The paper argues that computing statistics across diverse database structures can reveal semantic similarities, enabling effective data integration and other complex data management tasks.
The paper proposes a lossless compression method for large boolean matrices by reordering columns via a high-dimensional Hamming space TSP heuristic, improving access times and compression efficiency.
The paper systematically analyzes and demonstrates the optimality of bitmap compression techniques (BBC and WAH) for high-cardinality attributes, showing their efficiency and compactness compared to traditional indices like B-trees.
The paper proposes a method to minimize communication costs in XPath query processing over networks by computing and sending a minimal, non-redundant view set from the database to the client, ensuring optimal efficiency.
The paper proposes an efficient client-based access control system for XML documents, utilizing a dedicated index and security mechanisms to dynamically regulate access while preventing data disclosure and tampering.
The paper proposes methods to securely publish XML documents by preventing sensitive information leakage through data inference using semantic constraints, ensuring maximal data disclosure while maintaining security.
The paper presents an efficient method for enforcing cell-level privacy policies in databases with minimal performance overhead, demonstrating scalability for large datasets.
The paper systematically analyzes the satisfiability of tree pattern queries (a key fragment of XPath/XQuery) with or without schemas, identifying polynomial-time solvable cases and NP-complete ones, and demonstrates through experiments that satisfiability checking is efficient and beneficial for query optimization.
The paper studies the complexity of containment for nested XML queries, showing polynomial-time containment for queries with fanout 1, coNP-hardness for arbitrary fanout, and coNP-completeness for fixed nesting depth, while also analyzing practical extensions and proposing heuristics for efficient containment checking.
The paper demonstrates that optimizing XML-to-SQL query translation by exploiting translation-time information is more effective than post-translation SQL optimization, presenting an algorithm that generates efficient SQL for path expression queries over tree schemas.
The paper presents a novel, distribution-agnostic method for detecting and quantifying changes in data streams with statistical reliability guarantees, applicable to both continuous and discrete data.
This paper proposes false-negative-oriented algorithms based on the Chernoff bound to mine frequent itemsets from high-speed transactional data streams with bounded memory, ensuring controlled recall rates while outperforming existing false-positive approaches in accuracy, memory efficiency, and speed.
The paper proposes **TempIndex**, an indexing scheme for temporal XML documents that improves query performance by indexing continuous paths valid over specific time intervals.
The paper demonstrates that relational databases can efficiently process XQuery by compiling it into SQL that operates on a relational tree encoding, maintaining XQuery semantics while achieving strong performance and leveraging SQL's OLAP features.
The paper proposes *Relational Over XML (ROX)*, a method to access natively stored XML data via SQL interfaces to avoid migrating legacy applications to XQuery, demonstrating its feasibility through performance experiments with DB2.
The paper proposes a method to translate XML view updates into relational view updates by mapping XML query trees to relational views and leveraging existing relational view update techniques to propagate changes to the underlying database.
The paper introduces space-efficient streaming algorithms for extended wavelets, including optimal dynamic programming and near-optimal greedy approaches, to improve synopsis construction for multi-measure data sets while minimizing error.
The paper proposes memory-efficient algorithms for approximate sliding-window stream joins under both frequency-based and age-based stream arrival models, addressing maximum-subset and random-sampling scenarios, and optimizes memory allocation for multiple joins.
The paper introduces WIC, a parameterized algorithm for converting pull-based web sources into push-based streams, optimizing the trade-off between timeliness and completeness while proving it is a 2-approximation and sometimes optimal.
The paper introduces AWESOME, a data warehouse-based system that dynamically selects optimal website recommenders using user feedback and machine learning for adaptive website optimization.
This paper proposes an instance-based schema-matching approach using domain-specific query probing to address intra-site and inter-site schema-matching problems in web databases, achieving high accuracy through cross-validation.
The paper proposes a distributed search engine framework where web servers compute their own PageRank locally and merge results to overcome scalability issues of centralized crawler-based systems, achieving accuracy comparable to Google's PageRank.
The paper proposes asymptotically optimal algorithms for dynamically balancing range-partitioned data across distributed nodes in peer-to-peer systems, ensuring storage balance and efficient range queries despite adversarial insertions and deletions.
The *Linear Road Benchmark* is a standardized test for Stream Data Management Systems (SDMS) that evaluates performance in real-time stream processing, specifically simulating a dynamic tolling system, and demonstrates that SDMS can outperform relational databases by at least 5x in such applications.
The paper demonstrates the limitations of relational algebra and SQL in handling sequence and stream queries, proposes extensions like user-defined aggregates and timestamp-based stream merging to enhance expressive power and efficiency, and introduces necessary data model modifications.
The paper proposes a DBMS-based mechanism using cryptographic hash functions to detect tampering in audit logs, ensuring their integrity with low overhead and efficient validation.
The paper proposes a minimal cubing approach that computes a thin layer of the data cube with value-list indices to enable efficient high-dimensional OLAP operations while maintaining manageable storage and I/O costs.
The paper proposes TrustRank, a semi-automated method to combat web spam by using expert-selected seed pages and link analysis to identify reputable pages, effectively filtering spam with a small seed set.
The paper proposes a model-driven approach to sensor data acquisition that combines statistical modeling with live sensor readings to improve query accuracy and efficiency, offering optimal and heuristic solutions for balancing confidence and resource costs.
"GridDB is a data-centric overlay for scientific grids that manages data entities, offering declarative interfaces, type-checking, interactive queries, and memoization, validated through real-world physics and astronomy use cases and prototype measurements."
The paper presents ONYX, an overlay network-based system designed to enable scalable, internet-wide XML dissemination with support for filtering and transformation.
The paper proposes efficient algorithms, based on the footrule distance, to merge and rank approximate match results from multiple relational attributes in a declarative manner, with SQL implementations and experimental validation.
The paper *Steps towards cache-resident transaction processing* proposes *Steps*, a technique that reduces instruction cache misses in OLTP workloads by multiplexing transactions to exploit common code paths, improving performance by up to 96.7% in cache miss reduction and 64% in branch misprediction elimination.
The paper introduces a write-optimized B-tree design that combines the benefits of large writes (like those in log-structured systems) with traditional B-tree operations, enabling efficient page migration, in-place updates, and high-bandwidth writes while maintaining ACID guarantees and fine-grained locking.
The paper proposes efficient algorithms for exact reverse k nearest neighbor (RkNN) queries in dynamic, multidimensional datasets, supporting arbitrary k values without pre-computation and outperforming existing methods.
The paper introduces ERP, a metric combining L1-norm and edit distance to handle local time shifting in time series, and presents efficient pruning strategies using triangle inequality and a B+-tree-indexed lower bound to enhance search performance.
The paper presents a system for evaluating complex SQL queries on probabilistic databases using a probabilistic model, focusing on efficient query evaluation with optimization algorithms, while addressing #P-complete queries through approximation and Monte-Carlo methods.
The paper proposes two efficient indexing methods—an augmented R-tree and a variance-based clustering technique—to optimize probabilistic threshold queries (PTQs) over uncertain data modeled with intervals and probability density functions.
The paper proposes a domain-independent probabilistic ranking system for database query results, leveraging data and workload statistics and correlations to efficiently and effectively prioritize relevant tuples.
The paper presents **pSQL**, an annotation management system for relational databases that extends SQL with three annotation propagation schemes (default, default-all, and custom) to track data provenance and quality, along with storage methods and query translation algorithms for efficient implementation.
The paper proposes native database support for cardinality-bounded multisets to efficiently represent and manage symmetric relations, introducing compact storage, optimized query processing, indexing, normalization, and syntactic SQL extensions for improved consistency and performance.
The paper presents an algebra of grid-fields for efficiently manipulating both regular and irregular gridded scientific datasets, offering optimization benefits over existing database and visualization tools while reducing programming effort.
The paper proposes STAIRs, a mechanism to undo past routing decisions in adaptive query processing, enabling eddies to achieve higher adaptivity and performance by addressing the long-term effects of routing history.
This paper presents a flexible, generic implementation of Mandatory Access Control (MAC) in relational databases to support diverse application domains beyond Multilevel Security (MLS), including SQL compiler extensions to enforce label-based access rules and prevent data leakage during query optimization.
P*TIME is a highly scalable, memory-centric OLTP DBMS designed for update-intensive workloads, leveraging innovative techniques like differential logging and latch-free indexing to outperform traditional RDBMS on commodity hardware, as demonstrated in a real-world stock market deployment.
The paper introduces QGEN, a flexible query generator that rapidly produces large, statistically profiled query sets for dynamic decision support system evaluation without requiring predefined queries.
The paper introduces SQL operators, an indexing scheme, and system-defined tables to enable ontology-based semantic matching directly within an RDBMS, enhancing efficiency and ease of development for ontology-driven applications.
BioPatentMiner is a Semantic Web-based information retrieval system that enhances biomedical patent searches by integrating ontology knowledge, enabling advanced querying, semantic association discovery, and ranked results to reduce information overload.
The DB2 Design Advisor is an advanced industrial tool in DB2 UDB Version 8.2 that automatically recommends optimal combinations of indexes, materialized views, partitionings, and clustering for a given workload, using a scalable hybrid approach to handle feature interactions efficiently.
The paper proposes combining slightly outdated global analysis data with a fast radix-sorting-based index construction algorithm to significantly speed up intranet search engine indexing without compromising search quality.
The paper presents novel techniques for indexing XML data stored as BLOBs in Microsoft SQL Server to improve query performance while maintaining fidelity to the XML data model.
The paper presents an automated model for selecting optimal clustering keys in single- and multidimensional relational databases using cell/block storage, leveraging query cost modeling, data sampling, and search algorithms to balance performance and sparsity, with results showing effectiveness comparable to human experts.
The paper discusses Microsoft SQL Server's transition from a complex, manually configured system to a self-tuning database that minimizes human intervention by automating configuration and workload adaptation.
The paper discusses the challenges of integrating diverse biomedical data sets at institutional and national levels, presents a data warehousing solution implemented at Washington University School of Medicine, and describes tools for storing, querying, analyzing, and visualizing these integrated data.
The paper introduces **GPX (Gene Pattern eXplorer)**, an interactive system for exploring co-expressed genes and coherent expression patterns in gene expression data, integrating novel techniques like the coherent pattern index graph and user-guided clustering to address domain knowledge integration and high data connectivity.
The paper introduces the coDB P2P database system, which interconnects heterogeneous databases using cyclic GLAV coordination rules to enable querying and data retrieval across nodes with different schemas.
The paper presents PLASTIC, an enhanced tool for reducing query optimization overheads by recycling execution plans through query clustering and classification, demonstrated on IBM DB2 and Oracle 9i.
The paper introduces **CORDS**, a tool that automatically detects and ranks column correlations in databases to improve query optimization by providing accurate statistics, significantly enhancing query performance.
The paper introduces **CHICAGO**, a test and evaluation environment for assessing coarse-grained optimization techniques in SQL statement sequences using rewrite rules and a heuristic optimizer.
The paper introduces **HiFi**, a unified architecture designed to manage data acquisition, filtering, and aggregation in high fan-in systems like sensor networks and RFID readers using stream query processing.
The paper presents a framework for unified query processing and optimization across sensor networks and data stream management systems, enabling seamless queries with combined quality-of-service metrics, demonstrated through a factory mockup application.
"QStream introduces a deterministic, real-time-capable streaming system prototype with Quality-of-Service guarantees for querying data streams, addressing limitations of best-effort approaches."
"AIDA is an adaptive immersive data analyzer that enables spatio-temporal and continuous querying of sensory data collected from kids in a virtual classroom environment to study attention disorders."
BilVideo is a versatile video database management system supporting spatio-temporal, semantic, and low-level feature queries for diverse applications, demonstrated with a news archive search example.
PLACE is a scalable, real-time spatio-temporal query processor that incrementally evaluates continuous queries over moving object data streams using pipelined operators.
The paper proposes a measurement-based admission control algorithm for predictive service in integrated networks, which allows occasional delay violations to achieve high network utilization while reliably meeting delay bounds, as demonstrated through simulations across various topologies and traffic models.
The paper proposes a hybrid "one-pass-with-advertising" approach for end-to-end resource reservation in heterogeneous networks, addressing both the local-to-end-to-end service tension and varying router capabilities.
The paper presents **RETHER**, a software-based timed-token protocol for Ethernet that provides real-time guarantees for multimedia applications without hardware modifications, supporting hybrid operation, admission control, and fault tolerance while maintaining non-real-time traffic performance.
The paper presents a simpler, smaller, and VLSI-friendly shared-buffer switch architecture using pipelined memory banks, achieving 1 Gbps/link in an 8×8 switch with a 64 Kbit buffer in 45 mm².
The paper introduces the *a I t P m* architecture, a scalable gigabit IP router with an ATM core and multi-CPU processing, enabling flexible experimentation with next-generation IP protocols while demonstrating synergy between IP and ATM technologies.
The paper proposes a two-level hierarchical routing model for the MBone to reduce overhead by partitioning it into regions, using DVMRP for inter-region routing and allowing various intra-region protocols, with flexibility for additional hierarchy and incremental deployment.
The paper demonstrates that checksum and CRC algorithms exhibit skewed distributions over real UNIX file system data, leading to uneven error detection and high failure rates for TCP and Fletcher's checksums in detecting packet splices.
The paper analyzes MD5's performance in software and hardware, concluding that its speed is insufficient for high-bandwidth networks like HiPPI and FiberChannel, and recommends reconsidering its use as IPv6's default authentication algorithm in favor of a faster alternative.
The paper describes modifications to a BSD protocol stack to leverage outboard buffering and checksumming in a network adaptor, achieving more efficient "single-copy" communication for sockets and other applications, with significant performance improvements for large data transfers.
The paper explains that self-similarity in high-speed network traffic arises from the aggregation of many ON/OFF sources with highly variable (infinite variance) periods, linking this "Noah Effect" to the observed "Joseph Effect" (long-range dependence), supported by empirical Ethernet traffic analysis.
The paper presents a unified model for self-similar VBR video traffic that accurately captures both short- and long-term autocorrelation structures and marginal distributions, along with importance sampling techniques to estimate low packet-loss probabilities in multiplexers.
The paper introduces deterministic and statistical techniques to analyze queue size and delay in variable-rate communication nodes, providing bounds for performance evaluation in heterogeneous networks.
The paper presents a modular network subsystem for constructing high-level communication protocols by composing fine-grained micro-protocol objects within a hierarchical runtime framework, enabling greater flexibility and configurability.
The paper demonstrates that Integrated Layer Processing (ILP) improves file transfer performance by reducing memory accesses in a protocol stack integrating marshalling, encryption, and TCP checksum calculation, though gains are smaller than in simpler scenarios and depend on data manipulation characteristics.
The paper proposes *source hashing*, *threaded indices*, and a *Data Manipulation Layer* to optimize packet processing by reducing lookup costs and caching flow information, achieving significant performance improvements in high-speed networks.
The paper introduces *Tango*, a tool that automatically generates backtracking trace analyzers for single-process Estelle specifications, enabling validation of execution traces with various checking options, and discusses its application to protocols like LAPD and TP0, along with performance and partial trace analysis considerations.
The paper investigates whether TCP Vegas outperforms TCP Reno in bandwidth efficiency and throughput by examining factors like link bandwidth, buffer capacity, acknowledgment algorithms, and network congestion.
This paper presents a high-performance user-space TCP implementation that achieves 80% of single-copy TCP performance, reaching 160 Mbit/s throughput, while overcoming common user-space protocol challenges.
The paper introduces *Leave-in-Time*, a non-work-conserving, rate-based service discipline for connection-oriented networks that provides sessions with deterministic delay, jitter, and buffer bounds, independent of other sessions' traffic, while supporting delay-shifting adjustments.
The paper proposes Renegotiated Constant Bit Rate Service (RCBR), a method to manage compressed video traffic by dynamically adjusting bandwidth allocation through renegotiation and buffer monitoring, effectively handling multi-scale burstiness while maintaining performance and leveraging statistical multiplexing gains.
Deficit Round Robin (DRR) is a simple, hardware-implementable fair queuing algorithm that achieves near-perfect fairness with O(1) per-packet processing overhead, addressing the inefficiencies of prior schemes.
The paper introduces a mobile user location management mechanism combining distance-based updates and delay-constrained paging, develops an analytical model to optimize the update threshold for minimal cost, and evaluates performance under varying delay requirements.
The paper proposes new authentication protocols for wireless communications to enhance security in roaming environments by protecting subscriber identities, preventing masquerading and eavesdropping, and addressing roaming billing disputes.
The paper introduces **Floor Acquisition Multiple Access (FAMA)**, a family of MAC protocols combining carrier sensing and collision-avoidance dialogue to ensure exclusive channel access, outperforming non-persistent CSMA in throughput and handling hidden terminals effectively.
The paper introduces the Conference Control Channel Protocol (CCCP), a scalable framework for managing conferences of varying sizes by integrating new and existing applications.
The paper explores various methods for selecting replicated servers to optimize client-server traffic segregation by network topology, evaluating tradeoffs in effectiveness, cost, deployment ease, and portability through simulations based on U.S. network topologies and NTP servers.
The paper demonstrates through log-driven simulations that persistent HTTP connections, which allow multiple requests per TCP connection, significantly improve efficiency and reduce latency compared to traditional HTTP.
The paper explores cost allocation methods for multicast flows, analyzing distributive principles and one-pass mechanisms to fairly assign shared network costs among receivers.
The paper introduces **Log-Based Receiver-reliable Multicast (LBRM)**, an efficient and scalable protocol for high-performance distributed applications like simulations, addressing low-latency loss recovery, wide-area distribution, and minimal overhead in fine-grained multicast groups.
The paper presents SRM (Scalable Reliable Multicast), an efficient and robust framework for reliable multicast communication, tested in large-scale applications like the distributed whiteboard tool *wb*, and introduces adaptive algorithms to optimize performance across diverse network topologies.
The paper describes software challenges and solutions in optimizing the OSIRIS network adaptor, introduces application device channels for direct user-space access, and offers lessons for future adaptor design.
The paper introduces Jetstream, a low-cost, high-speed LAN combining shared-medium and ATM technologies, enabling efficient, application-controlled data handling with high throughput for both kernel- and user-space protocols.
"Vegas, a new TCP implementation, outperforms Reno with 40-70% higher throughput and significantly fewer losses by employing three key techniques, as demonstrated through simulations and Internet experiments."
The paper presents a modular, high-performance TCP implementation in Standard ML, leveraging its higher-order functions, type safety, and synchronous event processing to achieve both structural clarity and efficiency.
The paper analyzes congestion control in networks with selfish users and centralized switch control, showing that Fair Share service disciplines ensure fairness, uniqueness, and accessibility of operating points, unlike FIFO, though no discipline guarantees optimal efficiency.
The paper presents a scalable multicast control mechanism for continuous media streams that uses probing to estimate receiver numbers and separates congestion signals from control algorithms, implemented in the IVS video conference system to optimize bandwidth usage and perceptual quality.
The paper extends Parekh and Gallager's deterministic analysis of GPS scheduling by providing statistical guarantees on backlog and delay tail distributions for single-server and network settings using the E.B.B. traffic model, particularly for CRST and RPPS GPS networks.
The paper examines TCP performance over ATM networks without congestion control, finding low throughput due to cell drops and proposing Early Packet Discard to improve efficiency by dropping entire packets before buffer overflow.
The paper proposes a resilient and memory-efficient hop-by-hop flow control scheme for fair bandwidth sharing in networks, particularly ATM, with significant buffer optimization and applicability to rate-based methods.
The paper introduces a credit-based flow control method for ATM networks, featuring a simple credit update protocol, adaptive buffer allocation for shared VCs, and enhanced statistical multiplexing efficiency, validated through analysis, simulation, and implementation.
The paper proposes the Simple Internet Protocol Plus (SIPP) as a next-generation IP solution, addressing scalability and routing limitations through larger addresses and a generalized loose source route mechanism to enable advanced features like mobility, multicast, and provider selection.
The paper introduces **Protocol Independent Multicast (PIM)**, a scalable and efficient multicast routing architecture designed for sparsely distributed groups across wide-area networks, optimizing state, control, and data processing while remaining independent of unicast routing protocols.
The paper introduces **link vector algorithms (LVA)**, a family of routing algorithms that selectively diffuse link-state information based on preferred-path computations, offering lower complexity and better performance than traditional link-state and distance-vector approaches.
This paper presents the design and implementation of operating system and signaling support for native-mode ATM applications, enabling QoS-guaranteed virtual circuits, seamless integration with IP networks, and efficient resource management in a Unix-like environment.
The paper summarizes the experiences, implementation changes, lessons learned, and conclusions from the Fairisle project, which focused on ATM in local area networks.
The paper proposes a state-dependent routing scheme that enhances any base state-independent routing by rerouting blocked flows to alternate paths, improving performance under Poisson assumptions while requiring only local link-state information.
The paper proposes a neural network-based polling policy optimized via simulated annealing to minimize mean customer delay in asymmetric polling systems with multiple queues.
The paper presents the design and implementation of a ShuffleNet-based fiber optic interconnect prototype for shared-memory multiprocessors, using deflection routing and optical payload transmission to reduce remote memory latency while detailing practical challenges and performance metrics.
The paper presents a passive optical shuffle network design using fixed-wavelength transmitters and reconfigurable channel monitoring to enable scalable, partitionable, and conflict-free interconnects for heterogeneous traffic.
The paper introduces MACAW, an improved wireless LAN media access protocol based on MACA, using an RTS-CTS-DS-DATA-ACK exchange and a refined backoff algorithm to enhance performance.
The paper analytically evaluates the efficiency gains of RSVP reservation styles for multipoint-to-multipoint real-time applications on three network topologies, showing linear scalability improvements in resource utilization compared to traditional approaches.
The paper proposes a decentralized ad-hoc network routing protocol where mobile hosts act as specialized routers, using modified Bellman-Ford mechanisms to dynamically adapt to changing topologies and improve loop-handling and MAC-layer support.
The paper proposes a compositional protocol design technique using synchronization and inhibition constraints between component protocols, enabling modular design and verification for complex systems.
The paper evaluates 21 wide-area TCP traffic traces and finds that while user-initiated TCP session arrivals (e.g., remote-login and file-transfer) can be modeled as Poisson processes, other connection arrivals deviate significantly, TELNET packet interarrivals are poorly estimated by exponential models, and FTPDATA connections arrive in bursts that dominate traffic, with preliminary insights on potential self-similarity in wide-area traffic.
The paper analyzes a 2-hour VBR video sample, finding heavy-tailed marginal bandwidth distributions and long-range dependence in autocorrelation, proposes a non-Markovian source model for synthetic traffic generation, and demonstrates improved bandwidth efficiency in multiplexing despite long-range dependence.
The paper proposes a lossless smoothing algorithm for compressed video streams, characterized by parameters *D* (delay bound), *K* (known picture sizes), and *H* (lookahead), ensuring delay-bound compliance when *K* ≥ 1, and validates its performance using MPEG video statistics.
"USC is a flexible, high-performance stub compiler that outperforms traditional marshaling schemes and manual code generation by up to 20 times in speed."
The paper presents an object-based method for implementing protocol software by modeling each state as an object, where state transitions are handled via member functions, and includes a tool for graphical FSM editing and automatic C++ code generation.
The paper describes algorithmic and engineering refinements in NTP Version 3 that enhance accuracy, stability, and reliability in clock synchronization across networks, along with Unix system enhancements for submillisecond precision.
The paper proposes a particle filter-based MCMC method for efficient Bayesian analysis of massive datasets by conditioning on a small subset of data and incorporating the remainder via importance sampling with rejuvenation, significantly reducing data access while maintaining estimation efficiency.
The paper proposes scalable, robust covariance and correlation estimators with linear complexity in observations and quadratic complexity in variables, addressing outlier sensitivity in classical methods while maintaining computational efficiency.
The paper introduces **Multiple Additive Regression Kernels (MARK)**, a boosting-based algorithm that efficiently constructs an optimal heterogeneous kernel ensemble from a large or infinite library of kernels, automating parameter selection while maintaining computational scalability and competitive performance compared to SVM and kernel ridge regression.
The paper reviews and compares 21 interestingness measures for association patterns, identifies key properties to guide measure selection for specific applications, and presents an algorithm to help experts choose the most suitable measure efficiently.
DualMiner is the first algorithm that efficiently prunes the search space for frequent itemset mining by simultaneously leveraging both monotone and antimonotone constraints, demonstrating superior performance in theoretical and experimental evaluations.
The paper presents an improved method for optimizing viral marketing strategies by mining influence networks from knowledge-sharing sites, reducing computational costs, and allocating marketing funds more effectively while accounting for partial network knowledge and data-gathering expenses.
The paper introduces **TREE MINER**, an efficient algorithm for mining frequent subtrees in a forest using a novel **scope-list** data structure, demonstrating significant performance improvements over pattern-matching approaches and showcasing its application in web log analysis.
The paper introduces ANF, a fast and scalable graph mining tool that efficiently approximates the neighborhood function to analyze large graphs like the Internet, citation networks, and social networks, significantly speeding up computations while maintaining high accuracy.
The paper proposes an efficient infinite-state automaton model to detect and hierarchically organize topic bursts in document streams like emails and research papers, based on sharp frequency increases of features signaling emerging topics.
The paper argues that many time series data mining algorithms lack meaningful utility due to overstated improvements overshadowed by dataset variability and implementation details, demonstrated through extensive experiments on 50 diverse datasets, and calls for standardized benchmarks and rigorous evaluation.
The paper presents Polaris, an interactive visual exploration tool that extends its algebraic formalism and query generation to support hierarchical structures in OLAP databases, enabling analysts to navigate and visualize data at various levels of abstraction.
The paper proposes **Hyperbolic Multi-Dimensional Scaling (H-MDS)**, a novel visualization method for high-dimensional data by projecting proximity-based datasets into hyperbolic space (H2), enabling interactive exploration while preserving context, and demonstrates its effectiveness on synthetic, real-world, and unstructured text data.
The paper proposes a method to optimize search engine rankings using clickthrough data with a Support Vector Machine approach, demonstrating improved retrieval quality over Google with minimal training data.
The paper introduces **Relational Markov Models (RMMs)**, a flexible extension of Markov models that handles heterogeneous state spaces with hierarchical variables, enabling accurate user behavior prediction in web navigation despite sparse data, as demonstrated in e-commerce and academic site experiments.
The paper develops a Bayes error framework under a Markov assumption to analyze the difficulty of discovering recurrent patterns in categorical sequences, such as DNA motifs, and uses it to explain pattern discoverability, calibrate algorithms, and validate findings empirically.
The paper proposes a wavelet-based approach for effectively classifying long biological strings by capturing both global and local patterns through multi-resolution decomposition.
The paper demonstrates that Proximal Support Vector Machines (PSVM) with a linear kernel are equivalent to ridge regression, provides Bayesian interpretations for nonlinear kernels, and introduces an improved hybrid algorithm (PSVMMIX) combining ridge regression and Gaussian processes.
The paper proposes a hierarchical model-based clustering method called Fractionation and its extension Refractionation to efficiently handle large datasets by reducing computational costs while estimating the number of groups, even when many small clusters are present.
The paper proposes a divisive word clustering algorithm that optimizes information-theoretic criteria to improve hierarchical text classification accuracy, especially with fewer features, outperforming previous agglomerative methods.
The paper introduces a parallel EM and naive Bayes algorithm for text classification to improve computational efficiency on large datasets, demonstrating effectiveness on a Linux PC cluster.
The paper proposes a refinement approach that improves text categorization accuracy by successively correcting model misfit in training data without altering the underlying classifiers, achieving a 45% average performance boost for naïve Bayesian and Rocchio classifiers.
The paper proposes a privacy-preserving framework for mining association rules from randomized transaction data, introducing non-uniform randomization to prevent privacy breaches while enabling accurate support estimation through derived formulae and validated algorithms.
The paper presents **OpportuneProject**, an efficient algorithm for mining frequent item sets by dynamically choosing between array-based or tree-based projections and heuristically selecting filtered or unfiltered projections to optimize CPU time and memory usage, outperforming existing methods on both sparse and dense datasets.
The paper introduces **Positive Example Based Learning (PEBL)**, a framework using **Mapping-Convergence (M-C)** with SVM to classify web pages accurately without requiring manually labeled negative examples, achieving performance comparable to traditional SVM.
The paper proposes a reinforcement learning-based approach for sequential cost-sensitive decision making to maximize long-term benefits, demonstrating its effectiveness in targeted marketing compared to isolated decision optimization.
The paper presents an active learning-based deduplication system that interactively identifies challenging training pairs to improve accuracy while reducing manual effort.
The paper proposes a method to preserve anonymity in shared data by optimizing generalizations and suppressions using flexible transformations and genetic algorithms, ensuring utility for data mining while meeting privacy constraints.
The paper proposes **ASSEMBLE**, an adaptive semi-supervised ensemble method that iteratively improves classification by leveraging both labeled and pseudolabeled unlabeled data to maximize classification margin, demonstrating strong performance in benchmarks and winning the NIPS 2001 Unlabeled Data Competition.
The paper demonstrates that boosting cannot guarantee high recall and precision for rare classes if the base learner performs poorly in these metrics, emphasizing the need for a strong base learner to achieve effective ensemble performance.
The paper demonstrates that randomized classifier ensembles using small random feature subsets can efficiently handle high-dimensional data while maintaining accuracy, as shown through experiments with a randomized Adaboost variant in text classification.
The paper presents **IPM2**, an interaction-pattern mining algorithm that extracts functional requirements as usage scenarios from run-time system-user interactions to aid in reengineering legacy systems into web-accessible components.
The paper presents a data-driven optimization approach for maximizing banking marketing ROI by strategically selecting cross-sell and upsell offers for customers while accounting for resource constraints, multiple campaigns, and business rules.
The paper presents a segment-based approach for modeling customer lifetime value (LTV) and estimating the impact of retention efforts in the telecommunications industry, implemented in the Amdocs BI platform.
The paper presents a framework for automatically mining and analyzing product reputations from web opinions using text mining techniques, significantly reducing costs compared to manual surveys while enabling insightful marketing analysis.
The paper presents an extension to the Active Atlas system that learns domain-specific weights for string transformations through limited user input, improving object identification accuracy while reducing manual effort across diverse domains.
The paper describes a real-time system for competitive market intelligence that analyzes news stories about companies using text categorization, rule induction, and numerical data integration to detect critical differences and present actionable insights.
The paper proposes mining historical intrusion detection alarms using episode rules and a new conceptual clustering technique to improve the efficient handling of future alarms.
The paper proposes a nonstationary learning algorithm that models normal network traffic behavior to detect novel attacks by identifying deviations from learned norms, focusing on time-based probabilities and protocol vocabularies, achieving partial success on the DARPA IDS dataset with unique detection coverage.
The paper introduces **ADMIT**, a real-time, semi-incremental anomaly-based intrusion detection system that differentiates masqueraders from legitimate users with an 80.3% detection rate and 15.3% false positive rate while addressing concept drift and interpretability.
The paper proposes a method for efficiently evaluating and exploring large numbers of association rules in microarray data analysis using rule grouping, filtering, browsing, and inspection operators to help biologists identify meaningful gene regulation patterns.
The paper explores how domain literature can enhance statistical modeling in data-scarce or noisy contexts by evaluating text-based vector representations and similarity measures for clustering genes and learning Bayesian network dependencies, comparing their performance against expert and data-derived references.
The paper proposes a Time Lagged Recurrent Neural Network with trajectory learning to identify and classify gene functional patterns from heterogeneous nonlinear time series microarray data, outperforming other classification methods.
The paper proposes *collaborative crawling*, a method for topical resource discovery that leverages user browsing patterns from proxy logs to identify relevant web pages more effectively than traditional linkage-based crawlers.
The paper introduces a depth-first search algorithm with a vertical bitmap representation for efficient mining of long sequential patterns, outperforming previous methods by up to an order of magnitude.
The paper introduces efficient text clustering algorithms (FTC and HFTC) using frequent term sets to address high dimensionality and large database challenges while providing understandable cluster descriptions.
The paper proposes a theoretical framework for learning from multiple disparate data sources without explicit translations, enabling mathematical analysis of task complexity and algorithm development.
The paper presents simple algorithms for discovering ε-separable topic models in 0–1 data, provides theoretical guarantees for their success, and demonstrates their effectiveness on real-world datasets while connecting to matrix factorization methods.
The paper introduces **DecText**, a method for extracting simple, C4.5-like decision trees from trained neural networks, along with new discretization and pruning techniques to improve fidelity and handle continuous features.
The paper introduces FAST, a two-phase sampling algorithm that efficiently discovers association rules by first estimating item supports from a large initial sample and then refining a smaller final sample for accurate rule mining, achieving 90–95% accuracy with a 10x speedup over traditional methods.
The paper proposes CVS, a Correlation-Verification based Smoothing method that improves information retrieval by leveraging term co-occurrence and statistical significance, achieving a 14.6% effectiveness boost over prior techniques.
The paper presents scalable and adaptive techniques for clustering and matching high-dimensional identifier names in data integration, showing improved performance over non-adaptive baselines in experimental evaluations.
The paper proposes **SECRET**, a scalable linear regression tree algorithm that uses EM-based clustering to transform regression into classification at each node, achieving high accuracy and efficiency for large datasets.
The paper introduces **AQSim**, a system that enables efficient ad-hoc querying of large-scale simulation data by using two statistical modeling techniques—unbiased mean estimation and Andersen-Darling normality testing—on multi-resolution data partitions, significantly reducing storage and query times while maintaining accuracy.
The paper proposes a tumor cell identification method combining robust segmentation algorithms, feature extraction, and classifier voting strategies to improve accuracy over traditional image processing techniques.
The paper introduces the FIS algorithm, which simultaneously integrates feature and instance selection for text classification, demonstrating improved accuracy, reduced data complexity, and faster performance compared to traditional methods like Mutual Information and Support Vector Machines.
The paper introduces **SyMP**, an efficient, synchronization-based clustering algorithm that identifies arbitrary-shaped clusters in large datasets by modeling data points as pulse-coupled oscillators, dynamically determining cluster numbers and shapes, and handling noise robustly.
The paper proposes a hybrid multi-class classification method that combines the speed of naïve Bayes classifiers with the accuracy of support vector machines (SVMs) by using a confusion matrix to reduce redundant SVM computations, achieving comparable or better accuracy while being 3–6 times faster than standard SVMs.
The paper presents visualization tools integrated into the D2MS system to enhance user involvement in the knowledge discovery process, demonstrating their effectiveness through medical case studies on meningitis and stomach cancer.
The paper proposes a scalable method that enables any discrete search-based induction algorithm to run in constant time independent of database size while maintaining decision quality, demonstrated by significantly speeding up Bayesian network learning without sacrificing predictive performance.
The paper proposes a probabilistic model, pricing framework, and real-time algorithm to dynamically learn customer valuations and optimize e-content pricing, maximizing revenue while adapting to changing market behavior.
The paper introduces **SimRank**, a structural-context similarity measure that assesses object similarity based on their relationships to other objects, applicable across domains and computable via efficient graph-theoretic methods.
The paper proposes a method to measure time series similarity by identifying and evaluating the most relevant portions of data, validated through clustering and comparison with standard classifications in stock market datasets.
The paper introduces an efficient linear-time-and-space method for identifying surprising patterns in time series data by defining surprise as significant deviations from expected frequencies based on prior observations.
The paper introduces a clustering method that accounts for measurement errors by using a scale-invariant, error-based distance function in hierarchical clustering, demonstrating improved performance on retail and simulated seasonality data compared to traditional methods.
The paper introduces *k-optimal* rule sets for robust classification on incomplete test data, proving they outperform traditional classifiers by maintaining accuracy even with up to *k* missing attributes, and presents optimal and heuristic methods to generate them.
The paper demonstrates that decision tree classifiers are highly sensitive to small changes in training data, provides theoretical foundations for this instability, and proposes algorithmic improvements to enhance stability, noise tolerance, and rule quality.
The paper introduces a distributed causality rule mining framework for chain store databases, proposing efficient "level matching" algorithms (LM, LMS, Distributed LM) to handle non-sequential, inter-transaction patterns while addressing computational costs and scalability.
The paper proposes a robust and efficient two-phase clustering algorithm called CSM, which uses a novel cohesion-based similarity measure to merge subclusters hierarchically, demonstrating resilience to outliers and effectiveness for arbitrary-shaped datasets.
The paper proposes *InfoDiscoverer*, a system that identifies informative content blocks in web pages by analyzing feature entropy and dynamically selecting an entropy threshold, achieving high precision (over 0.956) in separating news articles from redundant elements like ads and navigation panels.
The paper analyzes collusion in the U.S. crop insurance program using data mining to identify anomalous behavior among agents, adjusters, and producers, finding that non-recursive triplet and agent-producer doublet arrangements account for the highest fraud risk.
The paper proposes an incremental context mining technique for adaptive document classification to efficiently handle evolving document contexts and vocabularies, improving classification precision and efficiency.
The paper proposes a Linear Programming framework to extend ROCCH analysis for finding cost-minimizing classifiers under performance constraints in two-class problems.
The paper introduces CBC (Clustering By Committee), an algorithm that automatically discovers word senses from text by forming tight, scattered clusters ("committees") and iteratively assigning words to them while removing overlapping features, enabling the identification of both frequent and domain-specific senses.
The paper proposes a novel co-training method that combines clustering-derived features with original features to improve text classification accuracy using unlabeled data, particularly when labeled examples are scarce.
The paper proposes parametric mixture models (PMMs) for single-shot detection of multiple non-exclusive text categories, outperforming conventional binary classification in multitopic web page detection.
The paper demonstrates the use of machine learning, specifically SVM classifiers, to automatically and accurately classify source code archives by application topics and programming languages.
The paper presents a two-party algorithm for privacy-preserving association rule mining in vertically partitioned data, enabling collaborative discovery of frequent itemsets without revealing individual transaction values.
The paper advocates for local non-linear dimensionality reduction techniques as efficient tools for visualization and classification, demonstrating their ability to capture high-dimensional similarity perceptions and achieve competitive classification accuracy with fewer dimensions.
The paper proposes a hub-authority profit ranking method to select items that maximize profit by accounting for cross-selling effects, addressing both size-constrained and cost-constrained selection problems.
The paper proposes *Discovery Net*, a Grid-based architecture for collaborative, distributed knowledge discovery, enabling resource sharing, high-performance data mining, and reuse of algorithms and processes in a visual analytical environment.
The paper proposes an information-theoretic nonlinear axis scaling method that enhances dimensionality reduction by improving retained dimensions' value and reducing their number while revealing correlations and scaling efficiently.
The paper proposes **B-EM**, a classifier combining **bootstrap** and **Expectation-Maximization (EM)** to improve classification accuracy by leveraging both labeled and unlabeled data while minimizing computational costs.
The paper proposes a unified on-line learning framework using probabilistic models to adaptively detect outliers and change points in non-stationary time series data by incrementally updating the model while gradually discounting past data.
The paper introduces CLOPE, a fast, scalable, and effective clustering algorithm for transactional data by optimizing the height-to-width ratio of cluster histograms.
The paper proposes a two-stage topic-conditioned novelty detection method using supervised topic classification and named-entity-based event detection, demonstrating improved performance over traditional approaches in identifying first-reported events in document streams.
The paper introduces a method to transform classifier scores into accurate multiclass probability estimates by combining calibrated binary probabilities and proposes a new calibration technique for two-class problems, demonstrating effectiveness across various domains.
The paper discusses specifications for connecting the Apollo DOMAIN network with the VAX system, recommending Ethernet as the optimal LAN hardware and detailing protocol designs based on the OSI model, particularly focusing on transport, presentation, and application layers for seamless file transfer.
The paper describes the use of server-based intelligent gateways to enable efficient internetworking between standard protocols and the V-System's lightweight local network protocol, improving performance while enhancing access control, reliability, and security.
The paper describes the design and implementation of a controlled flooding technique in Syteks LocalNet™ systems for dynamic node-to-channel assignment and efficient path discovery in interconnected broadband LANs, optimizing performance and limiting route discovery overhead.
The paper examines the semantics and relationships of names, addresses, and routes in network communication, explores their implications in layered architectures and dynamic networks, and highlights their importance for network design and standardization.
The paper evaluates the feasibility of the ISO-OSI reference model in designing HMINET-2, a heterogeneous local network, discussing encountered challenges and implemented solutions.
The paper describes two distinct multi-process structures for exception handling in X.25 and X.29 protocols, using finite state machines for lower layers and independent processes with auxiliary exception handlers for higher layers to improve clarity and maintainability.
The paper proposes and analyzes a group random access protocol with time windows for time-constrained communication, develops an optimal control policy to minimize message delays, and validates its performance improvements through analytical modeling and simulations.
The paper proposes protocols and algorithms to efficiently handle multiaccess links with broadcast capabilities in general network topologies, addressing challenges not covered by conventional routing methods designed for sparsely connected networks.
The paper analyzes the performance of a sliding window protocol in a local area network, studying mean throughput and packet delay statistics through simulation.
The paper describes the development, services, and field trial configuration of SSC, a prototype message handling system that provides storage, conversion, and mailbox services to enhance public communication networks, aligning with CCITT standards.
The paper analyzes security risks in the heterogeneous Bell Labs Network (BLN) and proposes countermeasures to minimize vulnerabilities as part of a global security mechanism.
The paper proposes a linear programming model to optimize the design and resource allocation of Distributed Data Processing (DDP) networks by maximizing reliability while considering topology, resources, and channel capacity.
The paper presents a synchronization-based methodology for designing reliable communication protocols that automatically detects and resolves collision errors, reducing state explosion in validation and simplifying error handling for designers, while also exploring its verification for arbitrary protocols.
The paper examines and analyzes multiple families of reliable multicast algorithms for local networks, addressing transmission errors to ensure dependable service.
The paper analyzes the throughput and delay of a reliable broadcast protocol in a local-area network using a Send-and-Wait mechanism and provides numerical results on performance characteristics.
The paper proposes decomposing complex network protocol finite state machines into interconnected subgraphs with unique entry and exit nodes to simplify analysis and ensure correctness by verifying individual subgraphs and their connections.
The paper describes the ITT/NET corporate network, detailing its goals, architecture, operational environment, prototype development, and the challenges of integrating diverse systems, while evaluating the practicality of formal development methods for the project.
The paper presents the design and implementation of distributed filing and printing services over a token-ring network, focusing on client/server communication protocols and their integration with applications.
The paper describes an experimental security mechanism for controlling connections between components in the Cambridge Distributed System using a ring network, with applicability to other architectures.
The paper describes the implementation of a multi-layer X.25 protocol on a multiprocessor system, focusing on flow control, inter-processor communication, synchronization challenges, and performance impacts of splitting the protocol between system and peripheral processors.
The paper proposes a priority-based CSMA/CD protocol with minimal overhead, incorporating nonpreemptive and preemptive disciplines (single and batch modes), where preemptive single mode offers superior performance while ensuring fairness and hierarchical independence.
The paper analyzes performance improvements in higher-level communications protocol interface processors using a simplified model to identify market-relevant parameter combinations.
The paper proposes a Bayesian-network classifier with inverse-tree structure (BNCIT) for joint classification and variable selection in voxelwise MRI analysis, efficiently handling high-dimensional data with limited samples by embedding variable selection within classifier training and using latent variables for stability and accuracy.
The paper introduces **Variable Latent Semantic Indexing (VLSI)**, a query-dependent low-rank approximation method that outperforms traditional LSI by optimizing for specific query distributions, achieving comparable performance with significantly fewer dimensions.
The paper proposes a hierarchical statistical learning framework for semantic image classification, using a hierarchical mixture model and an adaptive EM algorithm to improve classifier training and handle outlying images, with successful results on outdoor photos.
The paper presents an algorithm for extracting interpretable, non-overlapping rules from linear support vector machines and other hyperplane-based classifiers by solving computationally efficient constrained optimization problems, demonstrating its utility on real-world datasets, including medical applications.
The paper proposes a semi-definite programming-based algorithm for star-structured high-order heterogeneous data co-clustering by treating it as consistent bipartite graph co-partitioning, demonstrating effectiveness on toy and real-world datasets.
The paper proposes a method to detect low-dimensional clusters in high-dimensional data by extending the fractal correlation dimension to estimate local intrinsic dimensionality and introduces algorithms for discovering such clusters, demonstrating effectiveness on low-dimensional manifolds and low-rank sub-matrices.
The paper presents an algorithm for mining tree-shaped patterns with constants and existential nodes in large graphs, offering provable optimality properties, a SQL-based implementation, and experimental results on food webs, protein interactions, and citation data.
The paper proposes a conditional ensemble framework to discover novel, non-redundant clusterings in data by leveraging background knowledge about undesired clusterings, treating the base clustering method as a black box.
The paper demonstrates that online chatter volume, tracked through blogs, media, and web pages, can predict book sales rank spikes and trends when analyzed with carefully crafted or automated queries.
The paper presents the first one-pass streaming algorithms for constructing wavelet synopses that minimize various non-Euclidean error metrics (e.g., weighted \( l_p \) and relative error) without restricting synopsis coefficients to wavelet coefficients, offering provable additive approximation guarantees.
The paper introduces a method to combine multiple email classifier models in the Email Mining Toolkit (EMT) to improve spam detection accuracy and reduce false positives by analyzing relative gains and maximum achievable accuracy from different classifier combinations.
The paper proposes using nomograms to visualize support vector machines (SVMs) by converting hyperplane distances into probabilities via logistic regression, enabling clear interpretation of SVM decision structures.
The paper introduces an efficient sampling-based method to identify the strongest discrepancies (unexpected patterns) between a large database and a background Bayesian network, with provable correctness and scalability.
The paper proposes local sparsity control for Naive Bayes to improve classification performance under extreme misclassification costs, demonstrating its effectiveness over global feature selection in document classification tasks.
The paper introduces a multiple tree-based algorithm to efficiently and exhaustively associate asteroid observations by addressing combinatorial challenges in track initiation and improving upon sequential methods.
The paper proposes a probabilistic label aggregation method for combining multiple clustering solutions (both hard and soft partitions) by modeling the probability that objects are grouped together, using an EM optimization strategy, and demonstrates its competitive performance compared to existing methods.
The paper proposes a feature bagging method for outlier detection that combines scores from multiple algorithms using random feature subsets, improving detection accuracy in large, high-dimensional datasets.
The paper introduces VizRank, a fast and interpretable method for identifying optimal two-dimensional visualizations (e.g., scatterplots, radviz) that effectively separate cancer diagnostic classes using small gene subsets in gene expression data.
The paper analyzes the evolution of real-world graphs, revealing that they typically densify over time with super-linear edge growth and often exhibit shrinking diameters, then proposes a "forest fire" graph generation model to explain these phenomena.
The paper presents a symmetric, general model for clustering binary data, connects it to existing methods, addresses cluster number determination, and demonstrates its effectiveness experimentally.
The paper proposes probabilistic methods for discovering and summarizing evolutionary theme patterns in temporal text data by identifying latent themes, constructing theme evolution graphs, and analyzing theme life cycles, demonstrating effectiveness in news and scientific literature domains.
The paper presents a privacy-preserving distributed learning framework for heterogeneous data sources by sharing local probabilistic models and integrating them into a global model using maximum likelihood and maximum entropy principles, with efficient algorithms and empirical validation across diverse datasets.
The paper proposes new "emerging cluster" space-time scan statistics for rapid detection of disease outbreaks by combining time series analysis with enhanced scan statistics, demonstrating improved speed and accuracy over traditional methods in simulated anthrax and fictional outbreak scenarios.
The paper introduces the problem of mining cross-graph quasi-cliques, proposes an efficient algorithm called **Crochet** to address it, and demonstrates its effectiveness and scalability through experiments on synthetic and real datasets, including bioinformatics applications.
The paper introduces *query chains*—sequences of related queries—to derive implicit feedback from clickthrough data, enabling improved learning-to-rank models for web search that outperform traditional methods.
The paper proposes a weight decay method for observation weights in boosting that robustifies the loss function, connects boosting to bagging at extreme decay, and demonstrates practical benefits for prediction performance while relating one form of decay to Huberizing.
The paper proposes an L1-norm-based linear programming algorithm for unsupervised text classification and dimensionality reduction, achieving near-supervised SVM accuracy by projecting documents into a topic-representative low-dimensional subspace.
The paper proposes a sampling-based sequential subgroup mining method that iteratively discovers diverse, interpretable rules by focusing on unexpected patterns relative to prior knowledge and introduces a connection between subgroup discovery and classifier induction via stratified resampling.
The paper introduces a probabilistic workflow model and a polynomial-time learning algorithm to mine workflow representations from activity logs, addressing both compliance checking and model discovery in business processes.
The paper proposes a method to infer a partial order from unordered 0-1 data by constructing small ordered fragments, optimizing their orientations, and combining them heuristically to minimize a problem-specific score function, demonstrating its effectiveness on paleontological data.
The paper proposes a novel web object indexing approach that leverages domain knowledge and hierarchical LSI spaces to extract and organize structure attributes from web objects for improved searchability.
The paper proposes a data-driven method to improve discriminative sequential learning models like CRFs by identifying and leveraging rare-but-important statistical associations in training data, enhancing performance in tasks like phrase chunking and named entity recognition.
The paper proposes a profile-based approach to summarize a large set of frequent itemset patterns into *K* representative patterns that cover most patterns and approximate their supports, using a generative model and efficient algorithms to optimize interpretability and accuracy.
The paper proposes two methods, **CloseCut** (pattern-growth) and **splat** (pattern-reduction), to efficiently mine closed frequent relational subgraphs with connectivity constraints in large-scale networks (e.g., biological or social networks with ~10K nodes and ~1M edges), leveraging edge connectivity and graph theory optimizations.
The paper proposes efficient cryptographic techniques for anonymous online data collection without a trusted third party, preserving privacy by preventing data linkage to respondents while maintaining data accuracy and generality.
The paper proposes **CrossClus**, a user-guided clustering method that efficiently identifies relevant features across multiple relations to generate meaningful clusters tailored to user-specified tasks.
The paper proposes an SVM selective sampling technique for learning ranking functions, reducing labeling effort by selecting informative partial orders, and demonstrates its effectiveness in data retrieval applications.
The paper introduces redescription mining as a tool for analyzing set relationships—such as overlaps, similarities, and differences—using minimal generators of closed itemsets, with applications in bioinformatics for interactive exploration and biological insight.
The paper introduces an algebraic-technique-based privacy-preserving data classification scheme that outperforms randomization methods in accuracy and privacy protection while being easily integrable with existing systems.
The paper introduces **α-investing**, an adaptive streaming feature selection method that dynamically adjusts error reduction thresholds to control overfitting while providing false discovery rate guarantees, enabling efficient feature selection even with over a million potential features.
The paper presents a scalable method for identifying similar documents in large repositories by analyzing shared byte-stream chunks and clustering related files using graph-based techniques.
The paper proposes a kernel feature space-based anomaly detection method for spacecraft by modeling normal behavior from telemetry data and identifying deviations as anomalies, validated using orbital transfer vehicle simulator data.
The paper proposes a machine learning-based price prediction system for online auctions and introduces Auction Price Insurance, a service that guarantees sellers a minimum price for their goods, demonstrating its profitability through accurate predictions.
The paper proposes a system combining web mining, text analysis, and interactive visualization to extract marketing insights from online discussions about consumer products.
The paper proposes a data-ensemble framework with sampling and voting techniques to enhance the robustness of holistic schema matching against noisy input schemas, improving matching accuracy in large-scale deep Web integration.
The paper proposes a precision-and-coverage-based similarity measure for mining evolving web clickstream clusters in a single pass, demonstrating its superiority over cosine similarity in discovering accurate user profiles under dynamic conditions.
The paper describes a relational knowledge discovery approach using statistical relational learning to predict securities fraud by brokers, outperforming existing NASD rules by incorporating organizational relationships and achieving high correlation with expert evaluations.
The paper proposes a probabilistic hit-miss model with two novel extensions—a mixture model for numerical errors and a method for handling correlated fields—to detect duplicates in the WHO drug safety database, achieving high accuracy (94.7%) and reasonable recall-precision (63%-71%).
The paper introduces TIPPPS, a system that predicts high-value upsell opportunities for corporate telecom customers by using time-interleaved data and value-weighted learning to address high dimensionality and class imbalance, achieving a 3.7% improvement in ranking accuracy over existing methods.
The paper proposes **CommunityNet**, a novel algorithm combining social network dynamics and semantic content analysis to model and predict personal information dissemination behavior, demonstrating improved accuracy in email recipient prediction compared to existing methods.
The paper proposes a cascaded SVM-based approach for email data cleaning, formalizing it as non-text filtering and text normalization to improve text mining accuracy.
The paper proposes a dynamic syslog mining methodology using adaptive Hidden Markov Models and online learning to detect network failures, identify emerging patterns, and discover alarm correlations in real-time syslog data.
The paper proposes two algorithms to enhance the lift (true positive rate) at a specified customer pull rate under budget constraints, demonstrating their effectiveness in predicting mutual fund account defection.
The paper presents a data mining methodology to predict train wheel failures using operational and maintenance data, aiming to optimize maintenance, enhance safety, and reduce costs, with large-scale experiments demonstrating its effectiveness.
The paper proposes the Subspace Decision Path (SD-Path) method, an interactive diagnostic tool for high-dimensional classification that reveals significant classification behavior for individual test instances while enhancing interpretability through hierarchical decision exploration and data visualization.
The paper extends an overlapping clustering model by generalizing Gaussian mixture models to any regular exponential family distribution and corresponding Bregman divergence, providing algorithmic modifications and demonstrating results on synthetic and real-world datasets.
The paper presents a method to integrate profile hidden Markov model outputs into association rule mining, enabling flexible parameter analysis and efficient handling of multiple model datasets to enhance pattern mining in scientific modeling.
The paper proposes a scalable framework for reconstructing hidden emails (quoted but missing in folders) from large email datasets, demonstrating its effectiveness using the Enron corpus.
The paper proposes a user-oriented text mining framework that extracts novel association rules from competitors' websites by leveraging a concept hierarchy derived from a user's background knowledge and measuring rule interestingness through semantic distance.
The paper proposes LIPED, an HMM-based method for adaptive event detection that models event activeness patterns using life profiles to improve clustering performance, achieving higher F1 scores on the TDT1 corpus.
The paper introduces **Par-CSP**, a parallel algorithm for efficiently mining closed sequential patterns on distributed memory systems, achieving high scalability and load balancing while minimizing communication overhead.
The paper proposes a novel peer-to-peer overlay network protocol using hierarchical Dirichlet processes to cluster user preferences, improving file-sharing efficiency compared to existing methods.
The paper presents an algorithmic framework for decomposing time-stamped text documents into coherent semantic threads using graph decomposition techniques, with applications in summarization, browsing, and document integration.
The paper introduces the "boasting problem" to discover meaningful trends in historical rankings by formalizing and efficiently computing maximal claims about an object's past performance under two partial orders, connecting it to optimized confidence association rules.
The paper explores kernel methods in link analysis, demonstrating that Neumann kernels unify co-citation, bibliographic coupling, and HITS importance, while Laplacian-based kernels address limitations of co-citation, with practical applications and solutions discussed for real-world data.
The paper introduces the concept of arbitrarily partitioned data and presents an efficient privacy-preserving protocol for distributed k-means clustering in this setting.
The paper proposes a knowledgeable cache system and optimization algorithms to efficiently handle sequences or simultaneous sets of frequent pattern mining queries, achieving up to 9x speedup over non-optimized systems.
The paper proposes a framework for mining frequent topological structures (large-scale patterns) from graph datasets using topological minors, constraint-based mining, and approximate matching, with applications demonstrated in protein structure analysis and other domains.
The paper proposes a maximum entropy-based web recommendation system that integrates collaborative features (navigation/rating data) and content features (using LDA) to improve recommendation accuracy and interpretability.
The paper proposes the Latent Interest Semantic Map (LISM) model, which integrates Collaborative Filtering and Probabilistic Latent Semantic Analysis to jointly model user interests and document topics, addressing the cold start problem in information retrieval.
The paper demonstrates that stylistic text features, analyzed using automatic tools and support vector machines, can accurately predict an anonymous author's native language.
This paper presents a fast multilevel kernel-based graph clustering algorithm that outperforms spectral methods in speed, memory efficiency, and objective function optimization while allowing flexible cluster sizes.
The paper introduces **block value decomposition (BVD)**, a co-clustering framework that factorizes dyadic data matrices into row-coefficient, block value, and column-coefficient matrices, with a specific algorithm for non-negative data and experimental validation of its effectiveness.
The paper introduces the adversarial classifier reverse engineering (ACRE) problem, presenting efficient algorithms to reverse-engineer linear classifiers for constructing adversarial attacks, demonstrated on spam-filtering data.
This paper proposes a heuristic algorithm to estimate missed rare-class instances (false negatives) by ensuring conditional independence between classifiers' feature sets, addressing challenges in high-data-rate domains like network intrusion detection.
The paper proposes scalable sparse heuristics for kernel partial least squares (KPLS) and kernel boosted latent features (KBLF) to improve computational efficiency while maintaining performance, validated on benchmark datasets.
The paper introduces *Persist*, an unsupervised time series discretization method that uses Kullback-Leibler divergence to preserve temporal patterns, outperforming existing static methods and Hidden Markov Models in accuracy and noise robustness.
The paper proposes a text mining system that extracts key semantic structures from documents by analyzing syntactic dependencies, reduces redundancy, and reconstructs natural language phrases or sentences, validated through experiments on real-world datasets.
The paper proposes using fuzzy distance probability functions to enhance density-based clustering (specifically DBSCAN) for uncertain data, demonstrating improved performance over single-valued distance methods in experiments.
This paper presents a large-scale empirical comparison of six similarity measures for recommending online communities in the Orkut social network, evaluating their effectiveness based on user engagement and exploring the impact of recommendation ordering and social dynamics.
The paper proposes a hybrid unsupervised document clustering method combining hierarchical clustering and Expectation Maximization, using heuristics to automatically select initial clusters and estimate model dimensions, achieving higher-quality results than its individual components on real-world datasets.
The paper proposes a method for cross-lingual information integration by mining comparable bilingual text corpora to discover word and document mappings between languages without relying on manual linguistic resources.
The paper introduces **Regression Error Characteristic (REC) surfaces**, which extend REC curves by visualizing the joint cumulative distribution of prediction errors and target values, enabling more detailed model performance analysis, especially for non-uniform error costs and rare extreme value prediction.
The paper proposes a kernel trick-based method to learn context-dependent nonlinear distance functions for improved similarity measurement in data mining and information retrieval, demonstrating theoretical soundness and empirical effectiveness.
The paper proposes *RePro*, a system that combines proactive and reactive predictions for data streams by tracking conceptual changes, anticipating future concepts, and adapting models dynamically to improve prediction accuracy and efficiency.
The paper presents a generalized framework for mining spatial associations and spatio-temporal episodes in scientific datasets by modeling features as geometric objects, introducing robust distance metrics, and demonstrating scalable algorithms on real-world molecular dynamics and fluid flow data.
The paper reviews and classifies isometric data embedding techniques, highlights the issue of disconnected neighborhood graphs in nearest-neighbor approaches, and proposes a new method alongside three existing ones to construct k-connected neighborhood graphs for robust data projection.
The paper proposes a leaping traversal approach for efficiently discovering maximal frequent patterns by selectively jumping to promising nodes in the candidate lattice, avoiding unnecessary nodes and reducing computational overhead.
CLICKS is an efficient algorithm for mining subspace clusters in categorical datasets by identifying k-partite maximal cliques, outperforming existing methods in speed and scalability.
The paper presents a method for efficiently computing sliding-window Pearson correlations on high-speed, noisy time-series data by combining sketches, convolution, structured random vectors, grid structures, and combinatorial design.
The paper presents an online failure detection and localization method for component-based systems by decomposing observation data into signal and noise subspaces, tracking their statistical distributions using SDEM, and detecting anomalies via Hotelling T² and SPE scores, demonstrating improved accuracy over independent component profiling.
The paper presents the design and architecture of an **IDAS Data Set Generator (IDSG)**, a tool for creating synthetic data sets to evaluate knowledge discovery systems by simulating diverse, rule-based, and semantically interconnected data, demonstrated through a credit card transaction example.
The paper describes the implementation of data mining in a multinational chemical company, highlighting its role in identifying value-creating opportunities across business units, leveraging Six Sigma for data-driven decision-making, and addressing challenges like system complexity and sub-optimal solutions.
The paper presents an algorithm for mining statistically significant risk patterns in medical data using relative risk as a metric and demonstrates its utility by identifying patterns associated with allergic reactions to ACE inhibitors.
The paper presents an integrated framework for automating system management by mining log files using text mining, temporal analysis, and visualization techniques to categorize messages, discover event relationships, and validate patterns.
The paper presents an automated method for detecting frontal systems from numerical model data using feature vectors derived from wind fields, K-means clustering, probability estimation, hierarchical thresholding, and post-processing, with results matching expert identifications.
The paper presents an unsupervised method using self-organizing maps (SOM) to classify and visualize frequent and rare events in multi-camera surveillance video by analyzing motion and color features from integrated event data.
The paper proposes using data mining and machine learning techniques to predict short-term performance issues in enterprise systems, demonstrating that multivariate methods outperform univariate approaches and that models trained on combined data generalize well across systems.
The paper proposes using a mixture of multinomials clustering model, instead of K-means, to identify program execution phases in computer architecture simulations, significantly reducing simulation time while maintaining accuracy for SPEC2000 benchmarks.
The paper proposes a pattern-based similarity measure and an efficient algorithm for near-neighbor search in microarray data, focusing on synchronous expression patterns rather than magnitude-based distances.
The paper discusses the development of a national packet-switching network for higher education to share computing resources, initially relying on common carriers but potentially incorporating leased circuits for high-volume traffic as needs grow.
The paper proposes two closed-loop control algorithms (CLC-CD and CLC-CND) to stabilize S-ALOHA satellite communication channels by dynamically adjusting transmission rates, demonstrating superior stability and efficiency over uncontrolled and other control schemes without requiring collision detection or parameter tuning.
The paper introduces CPODA, a contention-based demand assignment protocol designed for efficient handling of mixed packetized data and voice traffic with multiple priorities, variable message lengths, and diverse station capabilities in a satellite broadcast network.
The paper presents queueing models to analyze average and 90th percentile packet delays in Datapac's two-priority-class network, evaluating sensitivity to link capacities, occupancies, packet lengths, and traffic mix.
The paper highlights two key reliability challenges in data communications systems: modeling dependent line failures and handling highly variable reliability parameters, while discussing initial models addressing these issues.
The paper describes the design and implementation of host software extensions enabling an IBM 360/91 running an existing operating system to provide ARPANET services, operational at UCLA since 1971.
The paper describes the ARPANET Telnet protocol's purpose, principles, implementation, and influence on host operating system design, acknowledging contributions from numerous developers in the ARPANET community.
The paper proposes modularizing modern operating systems into smaller functional modules distributed across separate hardware processors and suggests adapting a standard end-to-end protocol for efficient interprocess communication.
The paper discusses recent progress in hierarchical data communications standards, outlining identified levels and their independent standardization developments.
The paper presents a formal language-based method for describing data link level protocols, specifically the High-Level Data Link Control (HDLC) procedure, using automata and formal grammars to model half-duplex and full-duplex operations, including features like time-out and sequence numbering, while highlighting the advantages of formal grammars over automata for complex systems.
The paper presents a **Dual-Mode Slotted TDMA Digital Bus** that combines dedicated slots for high-duty/synchronous users and contention-based ALOHA slots for low-duty asynchronous users, optimizing bandwidth with an adaptive retransmission algorithm, supported by simulation data.
The paper demonstrates that local congestion control with random routing outperforms deterministic routing in reducing congestion in packet-switched networks, as shown through a queueing model analysis.
The paper proposes an encryption-based virtual connection model for securing interactive terminal-host communication, outlining protection goals, protocol requirements, and system design considerations.
The paper proposes a systematic framework for automatically designing effective, domain-specific distance functions for data mining applications, reducing user effort and outperforming standard metrics like Euclidean distance.
The paper proposes a mixture of von Mises-Fisher distributions for clustering high-dimensional directional data, derives two EM-based algorithms that generalize spherical k-means, and demonstrates improved performance on text and gene-expression datasets.
The paper presents a randomized nested-loop algorithm with a pruning rule that achieves near-linear time performance for mining distance-based outliers in high-dimensional data by reducing dependency on dataset size for non-outliers.
The paper introduces a framework for adaptive duplicate detection using trainable string similarity measures, including an extended learnable edit distance and a novel SVM-based vector-space measure, which outperform traditional methods in accuracy.
The paper proposes an iterative hypothesis-testing strategy for pattern discovery in large datasets, combining statistical methodologies with outlier and residual analysis, and demonstrates its application through simulated and real-world examples.
The paper introduces **EASE**, a data-reduction algorithm that efficiently generates a representative subsample of categorical count data using epsilon-approximation methods, outperforming FAST and random sampling in tasks like association rule mining and contingency-table analysis.
The paper proposes a lattice-based approach using cube transversals and closures to extract semantics from datacubes, enhancing supervised classification, summarizing cube semantics, and visualizing multidimensional associations.
The paper introduces a translation-invariant mixture model using EM and Bayesian methods to simultaneously align and cluster multidimensional curves, improving predictive accuracy and reducing within-cluster variance in applications like gene expression and storm trajectory data.
The paper presents an information-theoretic co-clustering algorithm that maximizes mutual information between clustered row and column variables in contingency tables, demonstrating effectiveness in sparse, high-dimensional data like word-document clustering.
SEWeP enhances web personalization by integrating semantically annotated content (using a taxonomy) and usage data through C-logs to generate more relevant recommendations.
The paper introduces the **Inverted Matrix** algorithm, a disk-based association rule mining method that improves efficiency by reducing database scans, using small independent trees for frequent items, and employing a non-recursive mining process, outperforming FP-Tree in large datasets with many unique items.
The paper presents a data mining algorithm (HAMLET) that analyzes airfare price trends to advise consumers on optimal purchase timing, achieving 61.8% of potential savings in simulations and averaging 4.4% savings per ticket.
The paper introduces methods for discovering ordered fragments in high-dimensional 0–1 data where variables have an underlying but unobserved order, proposing measures, algorithms, and applications to real-world datasets.
The paper introduces a greedy algorithm with provable approximation guarantees for selecting influential nodes in social networks to maximize influence spread, achieving at least 63% of optimal performance across several models and outperforming centrality-based heuristics in experiments.
PROXIMUS is an efficient framework for error-bounded compression of high-dimensional discrete datasets, enabling scalable and accurate analysis, including association rule mining, with minimal runtime and high precision.
The paper introduces a visualization method to analyze how feature weighting and normalization impact the cluster structure of high-dimensional data, demonstrated through music-related case studies.
The paper introduces a hierarchy of relational concepts requiring specific aggregation operators, proposes new distribution-aware aggregation methods for classification, and demonstrates their effectiveness in improving generalization performance on relational data.
The paper proposes *cross-training*, a semi-supervised learning method that improves classification accuracy by leveraging probabilistic relationships between two semantically overlapping label sets and their corresponding training documents.
The paper presents a method for generating English summaries of time-series data by selecting key patterns using Gricean maxims (Quality, Quantity, Relevance, Manner) to enhance human understanding, with applications in weather forecasts, gas-turbine sensors, and hospital intensive care data.
The paper proposes methods for evaluating cluster separation in model-based clustering and introduces a hybrid algorithm that prunes hierarchical model-based clustering results by merging clusters not corresponding to distinct data density modes.
The paper presents a privacy-preserving method for *k*-means clustering on vertically partitioned data, allowing sites to determine entity clusters without sharing attribute data.
The paper proposes a versatile index structure for multi-dimensional time-series data that efficiently supports multiple distance measures—including LCSS, Euclidean distance, and DTW—without requiring index restructuring, ensuring no false dismissals while improving retrieval speed for trajectory similarity analysis.
The paper proposes a weighted ensemble classifier framework to improve accuracy and efficiency in mining concept-drifting data streams by dynamically weighting models trained on sequential data chunks based on their expected performance.
The paper *CLOSET+* systematically evaluates and integrates the best strategies for mining frequent closed itemsets, proposing an efficient algorithm that outperforms existing methods like CLOSET, CHARM, and OP in runtime, memory usage, and scalability.
The paper introduces an approach to mine unexpected rules by incorporating user dynamics, prior knowledge from the start, and iterative rule evaluation to ensure continued unexpectedness.
The paper demonstrates that contrast-set mining is a specialized case of general rule-discovery, as shown by successfully applying the commercial system Magnum Opus to perform the task.
The paper proposes algorithms for estimating the relative importance of nodes in networks relative to specific root nodes, evaluating them on synthetic and real-world datasets.
The paper proposes a loglinear modeling approach enhanced with graph-theoretical partitioning to efficiently identify significant multi-item associations in large datasets, overcoming limitations of traditional support-based and pairwise methods.
The paper proposes CloseGraph, an algorithm for mining closed frequent graph patterns to efficiently reduce the exponential number of subgraphs while maintaining the same support as their supergraphs.
The paper proposes a **Style Tree-based technique** to identify and eliminate noisy blocks (e.g., ads, navigation panels) in web pages by analyzing common content and presentation styles, improving clustering and classification accuracy in web data mining.
The paper proposes **Clustering-Based SVM (CB-SVM)**, a scalable method that uses hierarchical micro-clustering to efficiently train SVMs on large datasets by summarizing data statistics in a single scan, achieving high accuracy with reduced computational cost.
The paper proposes **XRules**, a structural classifier for XML data that uses frequent discriminatory substructures for effective and cost-sensitive classification, outperforming traditional IR-based methods.
The paper introduces **Diffset**, a novel vertical data representation that reduces memory usage by tracking only differences in transaction IDs, significantly improving the performance and scalability of vertical mining algorithms for association rule mining.
The paper presents a near-linear-time algorithm for detecting abnormal aggregates in data streams across multiple sliding window sizes, with applications in astrophysics (Gamma Ray Bursts) and finance (trading activity and stock volatility), significantly outperforming direct computation.
The paper introduces **Golden Path Analyzer (GPA)**, a system that clusters web clickstreams by identifying the shortest successful paths (golden paths) and grouping users whose paths are supersequences of these, enabling clear, actionable insights for website optimization.
The paper presents a web-based safety data mining environment using an enhanced MGPS algorithm to efficiently analyze post-marketing adverse drug reaction reports for pharmacovigilance, enabling the discovery of drug-event associations and multi-way interactions.
The paper presents a temporal abstraction approach to analyze irregular, timestamped hepatitis test data, enabling machine learning to extract medically relevant insights from short-term and long-term changes.
The paper proposes a simulation model to more accurately assess the potential accuracy and privacy impacts of data mining systems for law enforcement and security, suggesting improved technical designs that could address concerns while highlighting necessary conditions for their effectiveness and social acceptability.
The paper proposes using data mining, specifically the Info-Fuzzy Network algorithm, to automate software testing by deriving functional requirements from execution data, enabling efficient test design, regression testing, and fault detection.
The paper develops passenger-based predictive models (using individual passenger data and cabin-level aggregation) to more accurately forecast airline no-show rates, demonstrating improved accuracy and potential revenue gains of 0.4%–3.2% over conventional historical methods.
The paper presents a structured process for microarray gene expression data analysis, covering pre-processing, gene selection, testing, classification, and clustering, illustrated through case studies using Clementine Application Templates.
The paper introduces REMIND, a Bayesian framework that automatically extracts and integrates structured clinical data from unstructured patient records to enable outcomes analysis for applications like treatment effectiveness and financial impact.
The paper proposes a proactive prediction system for large computer clusters using filtered event logs and probabilistic models, demonstrating that time-series and rule-based methods can accurately predict system performance and critical events.
The paper proposes two frequent-subsequence-based methods—association rules and SVMs—to improve the prediction of outer membrane proteins (OMPs) in Gram-negative bacteria, demonstrating higher accuracy and biological interpretability compared to existing approaches.
The paper proposes a clustering-based method to discover climate indices that overcomes the limitations of eigenvalue techniques like PCA and SVD, yielding interpretable indices that often outperform traditional approaches in correlation metrics.
The paper presents a method for combining expert systems and machine learning to generate new predictive rules from expert-derived data, demonstrated by the SEAS system for sales lead discovery.
The paper introduces **MORF**, a multimodal information filter combining a confidence-based classifier, Cross-bagging ensemble, and multimodal classification to efficiently block objectionable online content, with empirical validation from deployments in the U.S. and Asia.
The paper demonstrates that computational stylistics and multiclass Winnow algorithms can effectively discriminate authorship among up to 20 authors in electronic messages by analyzing stylistic features.
The paper proposes an adaptive method for identifying recent frequent itemsets in online data streams by decaying the influence of old transactions and optimizing processing time and memory usage.
The paper introduces a probabilistic, scalable, and noise-resistant algorithm for efficiently discovering time series motifs, which are approximately repeated subsequences, with high probability and anytime capability.
The paper proposes and evaluates automated methods for extracting and classifying image pointers in biomedical figure captions, achieving over 94% precision and recall for the most important class.
The paper proposes using randomized response techniques to build decision tree classifiers from privacy-preserved disguised data while maintaining high accuracy and analyzing the impact of randomization parameters on results.
The paper compares Random Projections and PCA for dimensionality reduction in supervised learning, finding that while PCA performs better predictively, Random Projections offers computational benefits for certain applications.
The paper introduces **VFDTc**, an extension of the VFDT algorithm for mining high-speed data streams, which handles continuous data and improves classification accuracy at tree leaves while maintaining online, single-pass efficiency and competitive performance with standard decision trees.
The paper proposes efficient and accurate algorithms using Singular Value Decomposition (SVD) combined with dimensionality reduction and sampling to identify correlations in both synchronous and asynchronous data streams, demonstrating their effectiveness through experimental evaluation.
The paper proposes a click-stream tree-based model for predicting a user's next web page by clustering sessions based on page order and time spent, then using similarity measures to generate recommendations.
The paper presents a scalable agglomerative clustering algorithm to identify stable natural communities in large linked networks, such as citation graphs, enabling temporal tracking of their evolution.
The paper presents a scalable local clustering method for navigating large datasets by modeling overlapping neighborhoods, enabling efficient feature extraction, hierarchical exploration, and identification of minor clusters within intersections of larger ones.
The paper introduces *viewpoint patterns*—invariant relationships between objects in images based on relative distance and orientation—and presents a scalable algorithm to mine these meaningful patterns from image databases.
The paper presents an interactive, iterative method for high-dimensional correlation analysis that progressively reduces data size while preserving correlations, using a polynomial-time algorithm with quality guarantees.
The paper proposes an interactive framework using a **coherent pattern index graph** and **attraction tree** structure to efficiently discover and explore coherent gene expression patterns in time-series data, demonstrating superior effectiveness and scalability compared to existing methods.
This paper introduces a numerical interval pruning (NIP) method to improve efficiency in decision tree construction on streaming data, reducing execution time by 39%, and leverages entropy/gini gain properties to cut required sample size by 37% while maintaining accuracy.
The paper proposes a "bag of paths" model for efficiently measuring structural similarity in web documents by representing tree-based structures as path sets, enabling computationally simple yet meaningful similarity measures that effectively cluster structurally similar pages.
The paper proposes and compares collaborative filtering methods for recommender systems that use ranked order responses instead of traditional rating scales to better represent user preferences.
The paper presents a two-way visualization method combining dendrograms and low-dimensional embeddings to simultaneously display hierarchical clustering structure and inter-cluster relationships while ensuring clear separation between all clusters.
The paper proposes an effective technique for mining contiguous and non-contiguous data records in web pages using observations about web data structures and a string matching algorithm, significantly outperforming existing methods.
The paper proposes a disk-based CFP-tree structure to efficiently store and query frequent patterns, enabling "mining once and using many times" with algorithms for support and item-constrained queries.
The paper proposes an online novelty detection framework for temporal sequences, incorporating confidence-based detection and using online support vector regression, with experimental validation on synthetic and real-world data.
The paper proposes a statistical framework for distributed cooperative mining in information consortia, where agents collaboratively estimate their individual data distributions without sharing raw data, and derives conditions for information gain or loss.
The paper introduces two graph-based anomaly detection techniques and a new method for calculating graph regularity, demonstrating their effectiveness with real-world and synthetic data.
The paper introduces **CARPENTER**, an efficient algorithm for discovering closed frequent patterns in biological datasets with many columns and few rows, outperforming existing methods like CLOSET and CHARM.
The paper presents a fast, unsupervised clustering algorithm with linear computational complexity (~O(N)) that uses grid-based weighting and rule-based agents to efficiently cluster large datasets without requiring iterations or initial cluster representatives, while also ranking clusters by density.
The paper proposes greedy and stochastic algorithms for program-specific code restructuring to enhance spatial locality, reducing working set size and page faults more effectively than traditional methods, as demonstrated on data mining algorithms.
The paper proposes two algorithms—heuristic search and mutual reinforcing adjustment—for simultaneously mining phenotypes and informative genes from gene expression data, demonstrating their superior performance over existing methods.
The paper introduces a weighted association rule mining framework (WARM) that uses weighted support and significance to efficiently discover significant item relationships while addressing the invalidation of the downward closure property in weighted settings.
"PaintingClass is an interactive system for constructing, visualizing, and exploring decision trees, enabling intuitive navigation and data projection to support knowledge discovery beyond classification."
The paper introduces two efficient, sound local algorithms for discovering direct causal relations and Markov blankets around a target variable, significantly outperforming existing methods in scalability and sample efficiency by leveraging graph connectivity rather than local region size.
The paper proposes a distributed multivariate regression method that reduces communication costs by transmitting only influential observations from each local site, balancing accuracy and efficiency better than compression or random sampling.
The paper proposes a novel correlation-based filter method for efficiently removing redundant and irrelevant features in high-dimensional data, demonstrating its effectiveness through empirical studies.
The paper introduces an adaptive nearest-neighbor search method using parametric distance to recommend electronic part replacements by learning user-specific similarity preferences.
The paper presents a system architecture for military decision support using two data mining techniques—rule discovery and Bayesian networks—to analyze extensive simulation data from Project Albert, aiming to uncover nonlinear and emergent behaviors for actionable insights.
The paper proposes using expert system production rules to enforce business rules as data constraints, incrementally improving data quality by flagging and correcting violations while allowing valid data to pass unchecked, illustrated with a case study.
The paper proposes a framework for managing and analyzing similarities in government regulations by leveraging hierarchical and referential structures, aided by text mining and a taxonomy-based repository, to uncover non-obvious connections between provisions.
The paper presents a NIC-based intrusion detection system that implements anomaly and signature detection on NIC firmware, demonstrating promising performance and tamper-proof potential for next-generation network security.
The paper presents a visualization technique using brushed, parallel histograms to effectively analyze concept drift by comparing feature and outcome distribution changes and their impact on predictive rules, demonstrated in power demand and stock investment scenarios.
This paper proposes a method for extracting meaningful association rules from Japanese customer inquiries by transforming text into sequential word pairs (verb-noun dependencies) and applying a novel rule selection criterion to discover useful information classes not obtainable through simple keyword retrieval.
This paper proposes a new methodology (LDCA) for automatic community generation in uni-party data without explicit links, applying it to money laundering crime (MLC) investigation through a timeline-based method (CORAL), with prototype testing on real case data.
The paper describes the design and implementation of a high-concurrency TCP/IP stack for IBM OS/370, optimized for terminal traffic and mainframe resources, while addressing protocol ambiguities and challenges in replacing hardwired terminals with TCP/TELNET in a public access environment.
The paper analyzes and compares stop-and-wait, sliding window, and blast protocols for high-speed local area network data transfers, demonstrating that blast protocols perform slightly better than sliding window, with go-back-n retransmission being nearly as effective as selective retransmission while being simpler to implement.
The paper presents a novel test sequence generation method for protocol implementation conformance testing using finite state machines and Unique Input/Output (UIO) sequences to detect faults without locating them.
The paper presents a distributed protocol and algorithm for bridges in an extended LAN to compute an acyclic spanning tree efficiently, with convergence time proportional to the network diameter, minimal memory and bandwidth usage, and includes a poetic summary ("Algorhyme") of the spanning tree process.
The paper demonstrates how communicating finite state machines (CFSMs) can model physical layer protocols, such as asynchronous start-stop and synchronous modem transmission, and presents a verification methodology using network decomposition, machine equivalence, and closed covers to ensure communication boundedness and progress.
The paper introduces **Real-Time Asynchronous Grammars (RTAG)**, a formalism for specifying and implementing communication protocols with support for data-dependent behavior, real-time constraints, and concurrency, and demonstrates its viability through an automated implementation of part of the ISO Transport Protocol (TP-4) in the 4.2 BSD UNIX kernel.
The AN/1 network architecture organizes compact LANs into hierarchical, overlapping subnetworks with formal gates for interconnection, using linear programming and perturbation methods to optimize bandwidth and topology based on traffic and distance.
The paper proposes a distributed domain name system for host and mailbox naming in the DARPA Internet and beyond, addressing administrative and technical challenges for global mail integration.
The paper proposes a path-oriented routing strategy for packet-switching networks that enables stable, loop-free, multi-path routing without packet disordering by using a distributed shortest-path algorithm and tagged updates to handle network changes, significantly improving throughput compared to single-path routing.
The paper proposes a multicast service model for internetworks, discusses its applications and implementation, and integrates it into the US DoD Internet Architecture.
The paper proposes a protocol translation gateway solution for interconnecting IBM's SNA and Xerox's XNS private networks, addressing gaps in existing internetworking models for vendor-proprietary architectures.
The paper proposes an architecture for network management systems to efficiently handle large data sets and enable user-defined detail levels, exemplified by the CNMgraf tool developed to test this model.
The paper discusses the benefits of mining e-commerce data for business insights (the Good), the limitations of web server logs as a data source (the Bad), and unresolved challenges in e-commerce data mining despite proper site architecture (the Ugly), while sharing real-world lessons and metrics.
The paper introduces the Discrete Gaussian Exponential (DGX) distribution, a flexible model that outperforms the Zipf distribution in fitting skewed real-world data, provides efficient parameter estimation via MLE, and demonstrates strong performance in applications like outlier detection.
The paper proposes new tree-based regression and classification splitting criteria that prioritize identifying interesting data subsets over balanced splits, leading to simpler, more interpretable, and expressive trees by leveraging the "end-cut problem" to peel small data layers.
The paper proposes probabilistic mixture models for profiling transaction data, demonstrating improved prediction, scalability, and interpretability over histogram-based methods while enabling applications like outlier detection and visualization.
The paper introduces GESS, a scalable similarity-join algorithm that significantly reduces distance computations, memory usage, and I/O costs compared to MSJ while incorporating controlled data replication for improved efficiency in high-dimensional spaces.
The paper proposes modeling customers' network value—accounting for their influence on others in a social network—using a Markov random field approach to enhance viral marketing strategies beyond traditional intrinsic value assessments.
The paper proposes an empirical Bayes method to identify unusually frequent item sets in transaction databases by estimating their interestingness relative to independent occurrence baselines, enabling detection of rare but significant associations (e.g., adverse medical events) without requiring high minimal support thresholds, and distinguishes between pairwise-explainable and synergistic multi-item associations.
The paper introduces the proximal support vector machine (PSVM), a fast and simple classifier that assigns points to the closest of two parallel planes by solving a single linear system, achieving comparable accuracy to standard SVMs with significantly faster computational performance, including handling large datasets efficiently.
The paper presents a data mining classification method using sparse grids with simplicial basis functions to efficiently handle high-dimensional feature spaces, achieving competitive accuracy with linear scalability relative to data points.
The paper proposes CVFDT, an efficient algorithm for mining decision trees from time-changing data streams by dynamically updating subtrees while maintaining accuracy comparable to a sliding-window approach but with lower computational complexity.
The paper introduces **Star Coordinates**, an interactive multi-dimensional visualization technique that arranges axes radially on a 2D plane to reveal clusters, trends, and outliers through dynamic transformations, aiding exploratory data analysis in fields like telecommunications.
The paper proposes *Ensemble-index*, a novel indexing framework that improves query efficiency by using multiple dimensionality reduction techniques tailored to different data subsets, rather than a single technique for the entire database.
The paper proposes using the robust Donoho-Stahel estimator to transform data spaces for distance-based KDD operations, demonstrating its stability and introducing a fast Hybrid-random algorithm for high-dimensional computation.
The paper applies feature mining techniques to identify HIV-active molecular substructures in a skewed dataset using the MOLFEA system, aiming to aid pharmaceutical development.
The paper proposes automated methods to discover unanticipated, useful information from competitors' websites, enhancing competitive analysis through efficient and practical techniques.
The paper demonstrates that user-centric clickstream data yields more accurate purchase prediction models than site-centric data, highlighting the limitations of incomplete data in personalization tasks.
The paper explores using Bayesian networks, Markov random fields, and mixture models to estimate query selectivity and generalization for transaction data, leveraging frequent itemsets and ADTrees for efficient compressed representation and query answering.
The paper demonstrates that collective predictions from three online games—Hollywood Stock Exchange, Foresight Exchange, and Formula One Pick Six—produce accurate probabilistic forecasts of real-world events, often outperforming expert opinions.
The paper introduces **tri-plots** and **pq-plots** as scalable visualization tools for analyzing separability, distance patterns, and classification in multidimensional datasets, demonstrating their effectiveness on synthetic and real-world data.
The paper introduces an efficient algorithm for discovering error-tolerant frequent itemsets in high-dimensional sparse data, demonstrating its effectiveness in clustering, query selectivity estimation, and collaborative filtering compared to traditional methods.
The paper proposes methods for learning unbiased cost and probability estimators in decision-making tasks where both are example-dependent and unknown, addressing sample selection bias using Heckman's approach, and demonstrates superior performance over existing methods on the KDD98 dataset.
The paper demonstrates that Self-Organizing Maps (SOMs) can efficiently cluster and profile serious sexual assault offenses, achieving comparable results to conventional methods in significantly less time (10 weeks vs. 2 years).
The paper proposes a human-computer cooperative system that combines human intuition with computational support to effectively cluster high-dimensional data by identifying meaningful clusters in hidden subspaces.
The paper introduces *conceptual reconstruction*, a method to mine massively incomplete data sets by transforming them into effective conceptual representations that leverage data correlations, avoiding extrapolation errors.
The paper presents a method to evaluate the novelty of text-mined rules using WordNet's semantic distance, showing that automated novelty scores correlate with human judgments as strongly as human ratings correlate with each other.
The paper proposes an efficient method for ordering large categorical datasets to enhance visualization quality without sacrificing computational speed.
The paper demonstrates that random projection is an efficient and effective dimensionality reduction method for image and text data, preserving data similarity comparably to PCA while being computationally cheaper, especially with sparse random matrices.
The paper introduces projection-based tour methods to visually explore and interpret support vector machine (SVM) classifiers with linear kernels in high-dimensional spaces.
The paper introduces PVA, a self-adaptive personal view agent system that dynamically adjusts both content and structure of user profiles to track evolving interests, improving personalization accuracy.
The paper proposes a scalable and memory-efficient clustering algorithm for mixed-type data by introducing a probabilistic distance measure and adapting the BIRCH framework, automatically determining cluster numbers and handling noise effectively.
The paper proposes a spectral co-clustering algorithm that partitions documents and words simultaneously by modeling their relationships as a bipartite graph and using singular vectors of a scaled word-document matrix to achieve optimal bipartitioning.
The paper proposes a spectral method using Laplacian matrix eigenvectors to efficiently separate disconnected and nearly-disconnected web graph components, offering comparable complexity to BFS/DFS but with easier implementation and additional capabilities like identifying articulation points and bridges.
The paper proposes a novel clustering method based on analyzing random walks on a weighted graph derived from spatial data, enabling efficient detection of arbitrarily shaped clusters with varying densities while handling noise and outliers in \(O(n \log n)\) time.
The paper proposes a rule-based ensemble classifier for regression problems by discretizing the output variable via k-means clustering, solving it as a classification task, and averaging predictions from top-voted classes, demonstrating competitive performance against regression trees and bagged ensembles.
The paper proposes an efficient micro-cluster-based algorithm to identify the top-n local outliers in large databases while minimizing computational costs by reducing unnecessary k-nearest neighbor searches and addressing micro-cluster overlaps.
The paper presents a generalized framework for clustering that unifies diverse algorithms through iterative optimization, incorporating supervised learning and data assignment, and experimentally evaluates novel methods derived from this framework.
The paper proposes a statistically principled method for creating and maintaining real-time, concise customer behavior summaries ("signatures") from high-speed transaction streams, enabling efficient data mining and querying, as demonstrated in a study of 96,000 wireless customers' calling patterns.
The paper proposes a distributed boosting algorithm that efficiently integrates classifiers from large, distributed datasets by exchanging and combining them into weighted ensembles, achieving comparable or better accuracy than standard boosting with reduced memory, computation, and communication costs.
The paper presents UNICON, an unsupervised algorithm for inducing semantic classes from text, capable of handling low-frequency words, large-scale clustering, and classifying new words into existing clusters without manual intervention.
The paper "DIRT @SBT@discovery of inference rules from text" presents an unsupervised method for deriving inference rules (e.g., "X wrote Y" implies "X is author of Y") by applying an extended Harris Distributional Hypothesis to dependency tree paths in parsed text.
The paper proposes an efficient algorithm to identify non-actionable association rules, reducing redundancy and helping users focus on potentially useful rules for decision-making.
The paper proposes a method to identify a small subset of fundamental association rule changes that cannot be explained by other changes, reducing the overwhelming number of reported changes and revealing meaningful shifts in real-world datasets.
The paper proposes efficient dynamic programming methods with pruning heuristics to estimate piecewise constant intensity functions from event sequence data, offering results comparable to Bayesian MCMC approaches but with significantly lower computational cost.
The paper proposes and evaluates data-driven techniques, including genetic algorithms and entropy-based heuristics, to identify optimal spectral wavelength ranges for automated classification of minerals from reflectance spectra, showing improved accuracy for some mineral classes.
The paper presents an algorithm to efficiently identify frequently co-occurring service requests (e.g., "ticket" and "timetable") that are spatially close in large spatial databases, aiding location-based service optimization.
The paper demonstrates that online versions of bagging and boosting achieve comparable classification accuracy to their batch counterparts while significantly reducing runtime by requiring only one pass through the training data.
**Summary:** TreeDT is a novel gene mapping method that identifies disease-associated genomic regions by analyzing tree-like patterns in genetic marker data, demonstrating competitive performance compared to existing methods like TDT and HPM.
The paper proposes algorithms and statistical tests for detecting spatial outliers in graph-structured data, demonstrates their effectiveness on a traffic dataset, and provides a cost model for outlier detection.
The paper introduces the Streaming Ensemble Algorithm (SEA), which builds and combines classifiers from sequential data chunks using a heuristic replacement strategy, enabling efficient large-scale or streaming classification with constant memory usage and adaptability to concept drift.
This paper extends Aumann and Lindell's method for association rules with numeric consequents, proposing alternative interestingness measures and efficient algorithms to handle dense datasets where their original approach is impractical.
This paper proposes a hybrid outlier detection framework that iteratively combines unsupervised learning (SmartSifter) and supervised learning to improve accuracy and interpretability by generating filtering rules from scored unlabeled data.
The paper *Infominer: mining surprising periodic patterns* introduces *information gain* as a novel metric to identify surprising periodic patterns by measuring the deviation of observed frequencies from expected probabilities, addressing limitations of traditional support-based methods and enabling efficient mining through bounded information gain.
The study compares five association rule algorithms, finding that their performance differences are only significant at impractical support levels generating excessive rules, with Apriori being sufficiently fast for practical use under reasonable rule limits.
The paper evaluates IBM's segmentation-based modeling solution (ATM-SE) for targeted marketing, developed with Fingerhut BI to improve predictive accuracy by optimizing customer segmentation.
The paper introduces an interactive path analysis framework for understanding web visitor navigation by examining elements, paths, and couples through configurable extraction, filtering, and visualization tools.
The paper proposes a novel unsupervised method combining nearest neighbor and classical statistical techniques for frontier analysis to estimate optimal business targets, demonstrating its effectiveness in Verizon’s print yellow page division for upselling and regional performance benchmarking, with potential revenue gains of millions.
The paper critiques data mining practices in the CoIL 2000 challenge, highlighting the effectiveness of naive Bayesian classifiers, the importance of feature interactions, and widespread overfitting due to insufficient attention to statistical significance.
The paper develops predictive models using classification trees and logistic regression to estimate individual passenger no-show probabilities, improving forecast accuracy and revenue optimization in airline overbooking compared to traditional historical averaging methods.
The paper presents a text mining system that uses rule analysis and Correspondence Analysis to accurately extract characteristics and their relationships from open-ended survey responses, aiding business decision-making.
The paper introduces *funnel report mining*, a novel web usage mining technique for analyzing user retention across page sequences on the MSN network, proposing a tree-based framework to efficiently extract meaningful funnels in a single data scan.
The paper presents an algorithm and methodology for analyzing knowledge gaps in customer support knowledge bases by comparing problem ticket clusters to existing solutions using cosine distance metrics, thereby identifying underaddressed categories to improve efficiency and reduce maintenance labor.
The paper proposes analyzing user session data in RightNow Web to infer FAQ utility ("solved count") and navigation patterns (link matrix), dynamically improving information retrieval through swarm intelligence and aging mechanisms.
The paper proposes using web log mining to enhance GDSF caching and prefetching policies, significantly improving web-access performance by leveraging access patterns.
The paper proposes a specification language (SPEX) for communication protocols, verifies a connection establishment protocol using SPEX and the AFFIRM system, uncovers errors, and confirms most of the protocol's correctness.
The paper outlines key design principles for international computer mail protocols, proposing a simplified functional model and aligning with the ISO Open System Architecture framework to address non-interactive communication between users.
The paper presents four new digital signature schemes using arbitrators to verify and authenticate messages and signatures without accessing message content.
The paper *At the present manufacturers network architectures do not provide co-operation between systems exceeding the boundaries of their own homogeneous networks* introduces Project SNATCH, which demonstrates how a gateway system can bridge closed network architectures of different manufacturers by translating their protocols into neutral ones, paving the way for open systems.
The paper explores the challenges and potential solutions for creating an ideal heterogeneous local area network that supports high-speed, error-free communication between diverse computational entities, discussing existing LANs, protocols, standards, and implementation approaches.
The paper surveys various perspectives on how telematics impacts employment and concludes with open questions relevant to Mexico.
The paper proposes a cryptographic system for secure Electronic Funds Transfer (EFT) networks using PINs, secret keys, and smart cards to ensure separate and secure personal verification and message authentication across institutions.
The paper presents a heuristic method for solving large-scale minimum-cost multicommodity flow problems in intercity data networks, offering suboptimal solutions with linear memory requirements and faster execution compared to traditional linear programming approaches.
The paper models flow-controlled virtual channel networks using closed multi-chain queueing networks, analyzes them with the efficient tree convolution algorithm, evaluates an open-chain approximation, and explores optimal routing for adding virtual channels.
The paper analyzes a queueing model for packet-switched networks with non-Poissonian ("peaked") traffic, showing that excessive delays arise from short interarrival times and long printer traffic, and recommends priority queuing over message switching for mixed traffic types.
The paper analyzes the maximum throughput of a full-duplex link under three protocols with varying retransmission and packet retention policies, showing that retaining all correctly received packets improves throughput, especially in error-prone conditions.
The paper introduces BID, a bidirectional token-passing local area network that supports both packet and circuit-switching modes, reduces walk time via preemption, and demonstrates uniform delay performance through simulation and analytical modeling.
The paper presents a communication system using a packet-switched virtual circuit switch (Datakit) to connect UNIX-based operating systems, focusing on interface performance, switch reliability, and distributed OS design.
"TORNET is a low-cost, experimental slotted-ring LAN with local and central rings, using limited insertion for efficient character traffic and two packet formats, designed for small devices and terminals."
The paper argues that the high costs of maintaining replicated databases across network nodes make widespread replication impractical, suggesting alternative approaches like single-node replication or shadow copies instead.
The paper defines a service class as a combination of handling directives and routing metrics, explains how packet-switched networks can support a fully general service class structure, and discusses the challenges of implementing this in hierarchical networks.
The paper examines the engineering challenges of designing a decentralized ring network and compares its potential advantages to Ethernet in terms of operational and technical performance.
The paper describes an optimal double loop network topology with forward and backward loops, proving its superiority in hop distance, delay, throughput, and reliability, while analyzing performance metrics and failure impacts.
The paper describes the Bell Laboratories Network (BLN), a 7-layer host-to-host networking service designed for heterogeneous environments, emphasizing portability and operational deployment since 1979.
The paper describes the design rationale behind Xerox's 48-bit host numbering scheme for Ethernet and internetworks, contrasting it with existing practices.
The paper presents a framework and model for analyzing the impact of satellite communications on high-level protocols, deriving data transfer bounds and illustrating them using an early NBS transport protocol's flow control mechanism.
The paper presents an isolated-word recognition system using LPC-based speech compression, where sampled speech is mapped to a finite codebook via a minimum distortion rule without requiring real-time LPC analysis.
The paper presents a single-channel full-duplex wireless transceiver design using RF and baseband techniques, including novel antenna cancellation, achieving near-ideal performance and enabling solutions to hidden terminals, congestion, and latency in wireless networks.
The paper proposes DC-MAC, a protocol that leverages intentional interference patterns to create a free coordination channel in wireless networks, improving channel utilization efficiency by up to 250% compared to CSMA.
The paper proposes CSMA/CN, a wireless protocol where the receiver detects collisions and notifies the transmitter via a unique signature, enabling early transmission abortion to improve channel efficiency.
The paper proposes *Ganache*, a dynamic guardband configuration system that adapts guardband sizes based on network conditions, improving throughput by 150% compared to fixed guardband allocations by reducing spectrum waste from 40% to 10%.
ZiFi detects nearby WiFi networks using ZigBee radios by analyzing WiFi beacon interference with a novel Common Multiple Folding algorithm and CFAR detector, achieving high accuracy, low delay, and minimal energy overhead.
The paper proposes **CTRL**, a self-organizing femtocell management architecture with three complementary control loops to mitigate uplink interference in co-channel deployments while maintaining macrocell service quality and achieving efficient femtocell coordination.
The paper proposes *Intentional Networking*, a system that leverages application-provided declarative labels and constraints to optimally match network transmissions to diverse mobile networks, improving interactive message latency by 48% to 13x with minimal throughput overhead.
"Bartendr is an energy-aware cellular data scheduling system that predicts signal strength using location tracks, enabling applications to defer or advance communication for optimal energy efficiency, achieving up to 60% energy savings across diverse workloads and networks."
The paper proposes a distributed RFID sensing approach using separate transmitter and listener devices, implemented via GNU Radio, to enable flexible, scalable tag detection and protocol analysis.
The paper proposes *visual-MIMO*, a mobile optical communication system using LED arrays and camera sensors to overcome the short-range limitations of directional optical transmissions, discussing its feasibility, applications, and research challenges across PHY, MAC, and network layers.
**Summary:** *Hermes* is a system that transmits data over cellular voice calls by modulating acoustic signals to be voice-like, achieving 1.2 kbps throughput while overcoming codec distortions and channel errors.
The paper presents SMSFind, an SMS-based search system that uses information retrieval techniques to deliver concise, relevant 140-byte responses from a conventional search engine backend, achieving 57.3% accuracy on ChaCha queries and demonstrating feasibility through a pilot deployment in Kenya.
PixNet enables high-speed, interference-free wireless communication using LCD-camera pairs by addressing channel distortions like perspective and blur, achieving up to 12 Mb/s at 10 meters with wide viewing angles.
The paper proposes *Escort*, a mobile phone-based system that uses sensor data and opportunistic user encounters to guide a person to within 8 meters of a desired individual in public spaces without relying on GPS or WiFi.
The paper proposes three novel localization schemes (LRL, TSL, TSLRL) for mobile networks by exploiting temporal stability and low-rank structure in mobility traces, demonstrating superior accuracy and robustness compared to existing methods.
The paper presents **EZ Localization**, a WiFi-based indoor localization algorithm that eliminates pre-deployment effort by leveraging sporadic location fixes and wireless propagation constraints, achieving median errors of 2m and 7m in small and large buildings, respectively.
The paper demonstrates that anonymous mobility traces can be re-identified with high probability using minimal side information, revealing significant privacy vulnerabilities in published datasets.
The paper presents the first multi-user beamforming system for wireless LANs, experimentally demonstrating that optimal performance for two receivers requires a quarter-wavelength separation, channel updates every 100 ms (static) or 10 ms (mobile at 3 mph), and spatial reuse trade-offs between interference elimination and user quality degradation.
The paper *Pushing the envelope of indoor wireless spatial reuse using directional access points and clients* demonstrates that directional antennas on both APs and clients enhance indoor wireless performance with minimal hardware complexity, proposing *Speed*, a practical distributed control system to optimize antenna orientation, MAC protocols, and client-AP association for improved spatial reuse.
The paper presents MiDAS, a directional antenna system for mobile devices using a single RF chain, which improves link gain by 3dB median, enhances goodput by 85% in low SNR, and reduces transmit power by 51% in high SNR without requiring infrastructure changes.
The paper presents **NVS**, a network virtualization substrate for WiMAX networks that enables efficient resource slicing with isolation, customization, and optimal scheduling while supporting diverse reservation types and per-slice flow scheduling.
"Stix is a scalable, goal-oriented distributed management system for large-scale broadband wireless access networks that reduces management traffic and simplifies network reconfiguration and performance tasks through graphical workflows executed by distributed agents."
The paper proposes MiRA, a MIMO-aware rate adaptation algorithm for 802.11n networks that outperforms existing MIMO-oblivious schemes by dynamically switching between single- and double-stream modes.
The paper presents THEMIS, a fair and distributed single-radio WLAN backhaul aggregation system that ensures equitable throughput distribution without requiring network modifications, validated through experiments and real-world deployment.
The paper introduces *Remap*, a novel retransmission permutation technique for OFDM networks that improves collision resilience by dynamically reassigning bits to subcarriers, enhancing decoding efficiency and throughput without hardware changes.
The paper presents *Sybot*, an adaptive, mobile spectrum-survey system for WiFi networks that uses three monitoring techniques to efficiently generate accurate spectrum-condition maps while reducing measurement effort by over 56%.
The paper presents **MIDAS**, a framework that infers **Activity Share** (time nodes spend transmitting simultaneously) from passive measurements to identify and mitigate conflicting transmissions in managed 802.11 networks, improving under-served links with high accuracy (error <12%).
The paper introduces the metric **κ** to measure inter-link packet reception correlation, demonstrating its effectiveness in predicting the performance of opportunistic routing and network coding protocols, such as Deluge and Rateless Deluge, across different network environments.
The paper empirically analyzes DSRC communication characteristics, focusing on Packet Delivery Ratio (PDR), by examining environmental factors, radio parameters, and correlation properties in real-world vehicular environments.
The paper proposes *Phantom*, a scalable hourglass co-clustering method, to identify distinct and persistent mobile user browsing behavior patterns in 3G networks, revealing that most users exhibit either homogeneous or heterogeneous browsing interests over time.
The paper demonstrates that UHF RFID tags can be physically identified using timing and spectral features, achieving ~71% classification accuracy and unique identification of up to 2^6 tags, with implications for privacy and cloning detection in supply chains.
The paper proposes and implements a modified back-pressure routing and rate control algorithm for intermittently connected wireless networks (ICNs), demonstrating its performance on a 16-node testbed.
The paper proposes a distributed algorithm using differential one-forms to efficiently track mobile targets and answer aggregate range queries in dynamic sensor networks, with update costs proportional to target movement and query costs scaling with target distance.
The paper develops a multi-layered model of Computer-Based Messaging Systems (CBMS), detailing functions at each layer, discussing tradeoffs, and exploring distribution options from centralized to decentralized architectures.
The paper describes MMDF, a system designed to enable flexible, multi-network memo distribution by supporting variable network structures, emulation of network attachment, and inter-network forwarding, reducing reliance on specific communication environments.
The paper proposes a methodology for integrating a DBMS into a distributed network by defining interfaces and communication protocols, enabling flexible distribution of schema, subschema, applications, and data across nodes, with feasibility demonstrated for certain topologies.
The paper proposes a hierarchical processing structure for distributed database concurrency control, defining two types of consistency to decentralize processing load, clarify update mechanisms, and establish a clear control philosophy.
The paper presents a distributed control algorithm for updating replicated databases using local locking and timestamps to detect conflicts, resolving them via rollbacks, and demonstrates its performance as comparable to centralized locking and superior to voting algorithms.
The paper describes the ARPANET's new distributed, adaptive routing algorithm, which replaces the original by using measured line delays to compute shortest paths and periodically broadcasting updates for consistent network-wide routing decisions.
The paper proposes that virtual circuits are well-suited for certain transaction-oriented applications, demonstrating their effectiveness in providing centralized control over distributed terminals on the Datapac network.
The paper proposes extending protocol multiplexing to enable message and packet sharing among multiple communicating processes, thereby improving communication efficiency by reducing transmitted messages.
The paper proposes and evaluates a linear incremental backoff algorithm for Ethernet, comparing its performance to binary exponential backoff under different traffic conditions and prioritization schemes (equal treatment or priority for acknowledgments) through theoretical analysis and simulation.
The paper analyzes *concentrated ALOHA* satellite systems, where multiple slotted ALOHA up-channels are statistically multiplexed onto a single down-channel to improve bandwidth efficiency, showing that optimal performance depends on buffer size, relative bandwidth costs, and a newly proposed protocol that enhances throughput and delay.
The paper develops a mathematical model to analyze packet transmission success probability over fading channels, evaluates throughput degradation in three random access schemes for packet radio, and derives a relationship between throughput degradation and distance from the base station.
The paper presents an algorithm for optimizing flow-control window settings in message-switched computer networks to maximize the throughput-delay ratio, using numerical heuristics and demonstrating results with an example network.
The paper proposes design principles—such as multiplexing, switching, cascading, wrapping, and layering—for interconnecting heterogeneous computer networks and applies them to addressing, routing, and other key challenges, advocating for standardized international solutions.
The paper presents a protocol architecture for MITRE's Cablenet, a coaxial cable-bus network, introducing a Flexible Transport Protocol to enable adaptable network transparency for diverse computing devices by leveraging advanced microprocessor capabilities.
The paper describes the design and implementation of the Data Access and Transfer System (DATS) in HMINET, a local heterogeneous network, to address incompatibilities between data management systems.
The paper describes the integration of the Bay Area Packet Radio Network into an existing network architecture, enabling radio-based communication between local networks at 12–20 kilobits/second with minimal software modification.
The paper reviews network delays in voice transmission, explores switching methods and delay-based contention resolution, presents experimental work on vocoder interfaces and variable delay thresholds, and suggests future research directions.
The paper presents an iterative method for multi-class cost-sensitive learning that uses binary classifiers, combining iterative weighting, expanded data space, and gradient boosting with stochastic ensembles, demonstrating theoretical guarantees and empirical superiority in cost minimization and computational efficiency.
The paper introduces and studies the problem of approximating a large collection of frequent item sets with a smaller set of *k* representative sets, aiming to maximize coverage while bounding extra covered sets, and provides theoretical hardness results, approximation algorithms, and empirical validation.
The paper proposes an automatic text segmentation system that mines reference tables from databases to segment unstructured text into structured records without manual training data, achieving higher accuracy than supervised methods.
The paper proposes **i-FILTER**, a particle-filter-based method for estimating latent origin-destination traffic flows from observed link loads in network tomography, improving accuracy by incorporating time dependence and non-Gaussian models, achieving error reductions of 15–46% on real-world data.
The paper demonstrates that clustering time series from ARMA models using clipped (binary) data maintains accuracy for long series, improves robustness to outliers, and reduces computational costs compared to unclipped data.
The paper proposes a probabilistic Hidden Markov Random Field (HMRF) framework for semi-supervised clustering that incorporates pairwise constraints and supports various distortion measures, improving clustering performance on text datasets.
The paper empirically analyzes nine supervised learning performance metrics, revealing their low-dimensional structure via multidimensional scaling, introduces a new combined metric (SAR) that correlates well with others, and shows that maximum margin methods excel in accuracy and ordering metrics but perform poorly in probability-based metrics.
The paper introduces a **parameter-free, fully automatic algorithm** for cross-associations in large sparse binary matrices, optimizing row and column groupings using an information-theoretic criterion without requiring user-specified parameters.
The paper proposes a semi-Markov extraction process that integrates external dictionaries into named entity recognition (NER) by classifying word segments rather than individual words, improving performance by leveraging entity-level similarity measures and features.
The paper proposes a game-theoretic framework for adversarial classification, where the classifier is optimized against an adversary's best strategy, demonstrating improved performance and adaptability in spam detection compared to standard methods.
The paper introduces a regularized multi-task learning method using a novel task-coupled kernel, demonstrating superior performance over existing multi-task and single-task SVM approaches.
The paper presents a method for efficiently discovering high-quality connection subgraphs—small subgraphs that best capture relationships between two nodes—in large social networks using electricity analogues and accelerated computations, demonstrating real-time performance on a 15-million-node graph.
The paper proposes an efficient cross-validation decision tree ensemble method to systematically select useful old data in concept-drifting streams, avoiding the ad hoc and unreliable use of outdated information.
The paper introduces **CBMiner**, an efficient algorithm for mining closed itemsets under tough block constraints, which are neither anti-monotone, monotone, nor convertible, by employing novel pruning techniques to enhance scalability.
The paper proposes a correlation mining approach called DCM, using co-occurrence patterns and a new $H$-measure to discover complex schema matchings (e.g., group or synonym attributes) across web query interfaces by analyzing both positive and negative correlations.
The paper introduces cyclic pattern kernels for predictive graph mining, which utilize cyclic and tree patterns regardless of frequency, demonstrating faster computation and superior predictive performance compared to frequent-pattern-based graph kernels on the NCI-HIV dataset.
The paper proposes techniques to automatically mine and summarize customer reviews by extracting product features, identifying sentiment (positive/negative), and aggregating opinions to aid decision-making.
The paper proposes a method to prune frequent itemsets by measuring their interestingness as the absolute difference between empirical support and Bayesian network-derived support, with efficient algorithms for computation and experimental validation.
The paper introduces *F-Miner*, a framework for systematically exploring and mining graph properties rather than predefined node characteristics, addressing limitations of traditional graph analysis by enabling automated discovery of relevant structural and relational patterns.
The paper proposes a probabilistic latent semantic analysis (PLSA) framework for discovering hidden semantic relationships in web usage data to improve user segmentation, page classification, and collaborative recommendations.
The paper advocates for parameter-free data mining to avoid spurious results and biases, proposing a method based on Kolmogorov complexity and compression algorithms that outperforms existing approaches in anomaly detection, classification, and clustering.
The paper presents a graph-theoretic method for extracting storylines—concise thematic summaries—from search results by generalizing maximum induced matching on bipartite graphs and using a fast local search algorithm.
The paper proposes efficient algorithms using *addset data structure* and *sliding window* techniques to incrementally maintain a quotient cube for holistic aggregate functions like MEDIAN while minimizing storage overhead.
The paper proposes a framework for mining, indexing, and querying historical spatiotemporal data by discovering periodic movement patterns to enhance data management and query efficiency.
The paper introduces **LOQR**, a fast, data-driven machine learning algorithm that relaxes failing queries in disjunctive normal form by learning attribute relationships from a small database subset and using nearest-neighbor techniques to adjust constraints, achieving high success rates (over 95%) and quick processing times (under a second for queries with up to 20 attributes).
The paper presents a multiresolution algorithm using an overlap-kd tree to efficiently detect the most significant spatial cluster in an N x N grid, reducing the time complexity from O(N⁴) to O((N log N)²) with substantial practical speedups.
The paper introduces **CARTwheels**, an alternating classification tree algorithm for **redescription mining**, which identifies data subsets that can be described in multiple ways by growing two trees in opposite directions and joining them at their leaves.
The paper presents a Winnow-based machine-learning algorithm that effectively detects computer intrusions by analyzing hundreds of system and user behavior metrics in real-time, achieving high detection rates (95%) with low false alarms (<1/day) and minimal CPU overhead (<1%).
The paper proposes a Bayesian network framework to systematically analyze and address reject inference in biased datasets, identifying eight cases (including MCAR, MAR, and MNAR) and unifying learning algorithms from diverse fields under a common structure.
The paper introduces *support envelopes*, a tool for analyzing association patterns by identifying items and transactions needed to find patterns involving at least *m* transactions and *n* items, providing a complete view of association structures without a support threshold and enabling efficient exploratory analysis.
The paper introduces an unsupervised probabilistic author-topic model that extracts information from text collections by representing authors as topic distributions and topics as word distributions, applied to CiteSeer data for analysis and discovery.
The paper introduces **ADI (Adjacency Index)**, a scalable disk-based index structure that enables efficient mining of frequent graph patterns in large databases, significantly outperforming **gSpan** in both scalability and speed.
The paper proposes Weighted Margin SVM (WMSVM), a generalization of SVM that incorporates prior knowledge to improve performance, demonstrating its effectiveness through experiments and showing compatibility with Sequential Minimal Optimization for training.
The paper proposes the TAPER algorithm, which efficiently identifies strongly correlated item pairs in large datasets by exploiting a computationally cheaper upper bound of Pearson's correlation coefficient with pruning properties, significantly reducing computation time compared to brute-force methods.
The paper proves that counting the number of distinct maximal frequent itemsets is #P-complete and extends this result to show that counting maximal frequent patterns in sequences, trees, and graphs is also #P-complete or #P-hard.
The paper introduces **Generalized Principal Component Analysis (GPCA)**, a dimension reduction method for image compression and retrieval that outperforms PCA by preserving spatial locality and improving efficiency in storage, query precision, and computational cost.
The paper proposes IDR/QR, an incremental dimension reduction algorithm using QR decomposition instead of SVD for LDA, offering lower computational cost and memory efficiency while maintaining competitive classification accuracy.
The paper presents a statistical method for discovering significant statistical quantitative rules (SQ rules) using resampling techniques, addresses false discovery rates, and demonstrates its application on consumer purchase data.
The paper proposes an efficient spatial join-based method for mining spatial collocation patterns by integrating neighborhood discovery with pattern mining, achieving significant performance gains over previous transactional data approaches.
The paper describes TiVo's scalable, distributed item-item collaborative filtering system for TV show recommendations, which leverages client-side processing to handle millions of user ratings efficiently while maintaining server scalability and flexibility.
The paper presents a prototype that uses individual customer classifiers trained on historical transaction data to accurately predict shopping lists, improving retail revenues by up to 11% and enhancing customer experience.
The paper proposes a rank sum test method for informative gene discovery that avoids the normality assumption of t-tests, demonstrates superior accuracy and robustness in theory, and achieves higher prediction accuracy (96.2% on colon data, 100% on leukemia data) compared to traditional methods.
The paper proposes using data mining techniques like decision trees, logistic regression, and neural networks to detect insider trading in option markets before the related news becomes public, comparing their performance against an expert model.
The paper proposes two efficient algorithms, **Sample-Gene Search** and **Gene-Sample Search**, to mine coherent gene clusters from gene-sample-time microarray data, identifying phenotype-related samples and correlated genes.
The paper presents an eigenspace-based method for automated anomaly detection in multi-tier web systems by modeling them as weighted graphs, extracting feature vectors from adjacency matrices, and using principal eigenvectors to derive an adaptive online threshold for fault detection.
The paper proposes a localized clustering-regression method for efficient and accurate damage detection in large mechanical structures by building local clusters and regression models for specific structure elements, outperforming global models and previous hierarchical approaches in both accuracy and computational efficiency.
The paper introduces **VizTree**, a novel visualization tool for mining and monitoring time-series data, particularly for aerospace telemetry, by transforming data into symbolic representations and displaying patterns in a modified suffix tree with visual encodings.
The paper presents a machine learning-based approach for detecting malicious executables using n-gram bytecode features, with boosted decision trees achieving high accuracy (AUC 0.996) on a dataset of 1,971 benign and 1,651 malicious samples.
The paper proposes a novel algorithm that directly optimizes the concordance index (CI) via a differentiable approximation to improve prostate cancer recurrence prediction, outperforming traditional Cox models and other machine learning methods.
The paper proposes a density-based spam detection method using document space density information, achieving 98% recall and 100% precision with unsupervised learning and handling over 13,000 emails per second, as validated by experiments on 50 million real emails.
The paper introduces *V-Miner*, a tool that enhances parallel coordinate visualization by automatically rearranging variables using edit-distance techniques to reveal correlations and patterns in product design and test data, with case studies from Motorola demonstrating its practical utility.
The paper proposes an on-demand classification model for evolving data streams that dynamically selects training windows to adapt to changing data patterns, maintaining high accuracy efficiently.
The paper presents a generalized Bregman divergence-based co-clustering framework that extends information-theoretic co-clustering to arbitrary matrices, introduces the minimum Bregman information principle, and provides a meta-algorithm guaranteeing local optimality, unifying several existing clustering methods.
The paper proposes an objective clustering evaluation criterion based on how well clustering improves classification performance in a semi-supervised setting, quantified using the PAC-MDL bound, and demonstrates its effectiveness on text datasets.
The paper proposes a column-generation boosting method for classification and regression that automatically selects and combines kernels from a library using 1-norm or 2-norm regularization, producing sparse, efficient models with improved testing speed.
The paper introduces **IncSpan**, an efficient algorithm for incremental mining of sequential patterns in large, dynamically updated databases, which outperforms existing methods by leveraging key properties to avoid reprocessing the entire dataset.
This paper presents parallel algorithms for computing high-dimensional robust correlation and covariance matrices (QC and Maronna methods), demonstrating their scalability and performance on large datasets, with QC being faster for small parallel systems and Maronna preferred for high robustness or large-scale parallel platforms.
The paper proposes a POMDP-based approach for signaling alarms in surveillance systems, demonstrating its effectiveness over thresholding and CUSUM methods in handling single-output and spatio-temporal data using scan statistics.
The paper proposes a steganalysis method to locate hidden messages in images by identifying outlier pixels based on energy measures, achieving 87% accuracy in color images and 61% in grayscale images among the top 0.33% of detected outliers.
The paper establishes a theoretical connection between kernel \( k \)-means and spectral clustering, showing that normalized cut objectives can be optimized via weighted kernel \( k \)-means, enabling more efficient and flexible clustering algorithms.
The paper introduces **Customer-Oriented Catalog Segmentation**, a problem where catalogs are designed to maximize the number of customers with at least a minimum interest threshold, and proposes efficient greedy and randomized algorithms that outperform traditional methods.
The paper introduces *k-privacy*, a novel privacy model for large-scale distributed environments that generalizes the trusted third party approach to enable efficient and cryptographically secure multiparty computation, demonstrated through an association-rule mining application in Data Grids.
The paper introduces a tree-based statistical method to detect extrapolation by distinguishing whether a point originates from the data distribution or a uniform null distribution, using modified CART for improved classification.
The paper proposes a method to quantify the importance of non-additive interactions in black box functions by projecting them onto additive models, enabling interpretability through graphical displays and parametric modeling of low-dimensional components.
The paper proposes **SPIN**, an efficient algorithm for mining **maximal frequent subgraphs** from graph databases by first extracting frequent trees and then reconstructing maximal subgraphs, significantly reducing output size and improving speed compared to existing methods.
The paper extends spatial scan statistics by introducing flexible square pyramid shapes to detect and model growing and shifting space-time clusters, validated on brain cancer data.
The paper demonstrates that collective inference reduces classification error in relational data by leveraging interdependencies among related instances, with effectiveness depending on specific model conditions.
The paper proposes a framework and metrics to assess whether data mining results violate privacy, addressing concerns beyond input data confidentiality.
The paper proposes a lexicon randomization technique to enhance the robustness of signature-based near-replica detection, significantly improving recall by 40-60% with minimal computational overhead compared to traditional I-Match.
The paper proposes algorithms to learn spatially variant dissimilarity measures from data, improving clustering stability and accuracy on textual datasets compared to existing methods.
The paper proposes an efficient micro-clustering approach to dynamically track and analyze moving objects, identifying spatial-temporal patterns and events like collisions, significantly outperforming traditional K-Means in runtime.
The paper proposes an ontology-driven subspace clustering framework that integrates hierarchical domain knowledge to improve clustering efficiency and interpretability while maintaining cluster quality, as demonstrated on gene expression data using gene ontology.
The paper introduces the IOC algorithm, an efficient variant of k-nearest neighbor classification for high-dimensional, many-class datasets, which prunes non-majority classes early to achieve significant speedups over traditional methods.
The paper proposes a polynomial-time Monte-Carlo algorithm for identifying biologically meaningful coClusters in gene expression data, where genes and samples follow a regulatory model with bounded relative error (sleeve-width), ensuring near-optimal results with high probability.
The paper proposes using semantic concept detection to map video clips into semantic spaces via model vectors, enabling improved retrieval, classification, visualization, and data mining of multimedia content.
The paper introduces the **Gaston** algorithm, which implements the "quickstart principle" to efficiently mine frequent substructures (paths, trees, and cyclic graphs) by progressively increasing complexity, and evaluates two frequency computation methods.
The paper proposes "MMG," a scalable, graph-based method for automatically discovering cross-modal correlations in multimedia collections, which outperforms existing methods in auto-captioning accuracy on the Corel image database by up to 10 percentage points.
The paper presents a Bayesian mark-recapture model to estimate the number of active telephone lines in the USA by analyzing calling patterns from long-distance network data, yielding state-level estimates consistent with published reports but with higher uncertainty for unclassified lines.
The paper introduces a cluster-based method to invent new relational concepts that enhance feature generation in statistical relational learning, improving predictive accuracy in tasks like venue and link prediction using CiteSeer data.
The paper presents a non-parametric method using a weighted directed graph and semidefinite programming to identify early buyers from purchase data, validated on Amazon.com datasets.
The paper presents a distributed computation algorithm for exact linear regression analysis on vertically partitioned data without disclosing individual attribute values, enabling privacy-preserving collaborative modeling.
The paper introduces *dense itemsets*, a generalization of frequent itemsets that allows for noise by requiring only a sufficiently dense submatrix of attributes, presents efficient algorithms for mining them (including top-*k* variants), and demonstrates their utility in data summarization and exploration.
The paper proposes a framework for generalizing the notion of support in association analysis by decomposing it into two functions—one evaluating pattern strength per object and another summarizing these evaluations—and demonstrates its applicability to non-binary data and diverse pattern types.
The paper proposes an unsupervised adaptation of the Hedge algorithm to combine rankings from multiple sources for pattern ordering, addressing challenges like the lack of labeled data and non-i.i.d. pattern dependencies, and shows superior performance compared to alternative methods.
The paper introduces a generative probabilistic model based on constrained mixtures of hidden Markov models to visualize sets of discrete symbolic sequences, demonstrated on web-log data and Bach chorals.
The paper introduces a rotation, translation, and scale-invariant distance measure for multidimensional trajectories, enabling efficient and accurate pattern matching in applications like handwriting recognition and motion-capture analysis.
The paper presents a privacy-preserving protocol for learning Bayesian network structures from distributed heterogeneous data using a secure version of the K2 algorithm, enabling collaborative analysis without sharing sensitive data.
The paper introduces a multilevel geodesic clustering model using multilevel meshes to analyze and reduce large scale-free networks, demonstrating its effectiveness on social and biological networks while enabling efficient distance and shortest path queries.
The paper proposes **Incremental Maximum Margin Criterion (IMMC)**, a supervised subspace learning algorithm for streaming or large-scale data, which optimizes the Maximum Margin Criterion and converges to results comparable to batch methods.
"2PXMiner is an efficient two-pass algorithm for mining frequent XML query patterns, utilizing three data structures to enhance performance and scalability."
The paper proposes an efficient redundancy-based feature selection method for microarray data that improves classification accuracy by removing redundant genes while maintaining discriminative power.
The paper proposes a cross-collection mixture model for comparative text mining (CTM) to discover latent common themes and analyze similarities and differences across comparable text collections, demonstrating its effectiveness over a baseline mixture model in experiments on news articles and product reviews.
The paper proposes a data mining approach using visual dictionaries, classification trees, and an α-Semantics Graph to model semantic relationships among image categories for improved retrieval accuracy and efficiency.
The paper proposes a Data Envelopment Analysis (DEA)-based method for combining classifiers, demonstrating its equivalence to ROC analysis for 2-class problems and superior performance for general k-class problems.
The paper proposes a framework for constructing optimal randomization schemes in privacy-preserving data mining using mixture models, balancing performance degradation and mutual information loss against privacy protection metrics.
The paper proposes a reinforcement learning-based Markov Decision Process framework to optimize cross-channel marketing by linking actions in one channel (e.g., direct mail) to customer responses in another (e.g., in-store purchases), demonstrating a 7-8% profit increase in real-world testing with Saks Fifth Avenue.
The paper presents an interactive system using decision tree classifiers to efficiently model land cover and mine remote sensing image archives by integrating spectral, DEM, and GIS data while handling missing values and mixed data types.
The paper proposes using cross-post networks to analyze Usenet newsgroups' community structure, revealing small-world properties and presenting a spectral clustering algorithm for topical classification.
The paper discusses the application of efficient feature selection methods, including filter and wrapper approaches, to high-dimensional scientific datasets from astronomy, plasma physics, and remote sensing, highlighting challenges, successful strategies, and key lessons learned.
The paper proposes a general ensemble method that incorporates data quality matrices into data mining algorithms to improve predictive accuracy by reducing variance, addressing inherent flaws in data manufacturing processes.
The paper presents an automated two-step system that uses classification models and parameter matching to accurately map supplier part numbers in Bills of Materials (BOMs) to internal part numbers, replacing manual processes and improving efficiency.
The paper proposes a real-time topic trend analysis framework using a dynamically learned finite mixture model to identify topic structures, detect emerging topics, and characterize topics, demonstrated with help desk data.
The paper proposes a travel time prediction method using probe-car data, employing AR models with seasonal adjustment or state space models to handle periodicities and MDL criterion for selecting effective explanatory variables, and demonstrates its effectiveness with real-world data from Nagoya.
The paper demonstrates that SQL can efficiently implement the K-means clustering algorithm with correctness and linear scalability by optimizing Euclidean distance calculations, nearest-cluster queries, and indexing strategies for large datasets.
The paper justifies and evaluates heuristic document preprocessing techniques like IDF, normalization, and log transformations to improve naive Bayes classification and multinomial mixture clustering for text categorization and clustering tasks.
The paper explores the use of random forests and support vector machines for classifying individuals into three disease and treatment groups based on metabolomic data, addressing challenges like non-normality, missing data, and high correlations while linking selected predictors to disease biochemistry.
The paper introduces **Analytical View (AV)**, an extension to Microsoft Business Framework (MBF) that enables seamless integration of OLTP and OLAP data through automated model transformation, BI Entities for programming, and IntelliDrill for runtime navigation, eliminating traditional ETL complexity.
